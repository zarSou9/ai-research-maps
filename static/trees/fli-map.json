{"id": "0", "title": "Value Alignment", "description": "The project of creating value-aligned AI is perhaps one of the most important things we will ever do. However, there are open and often neglected questions regarding what is exactly entailed by 'beneficial AI.' Value alignment is the project of one day creating beneficial AI and has been expanded outside of its usual technical context to reflect and model its truly interdisciplinary nature. For value-aligned AI to become a reality, we need to not only solve intelligence, but also the ends to which intelligence is aimed and the social/political context, rules, and policies in and through which this all happens. This landscape synthesizes a variety of AI safety research agendas along with other papers in AI, machine learning, ethics, governance, and AI safety, robustness, and beneficence research. It lays out what technical research threads can help us to create beneficial AI, and describes how these many topics tie together.", "breakdowns": [{"id": "00", "sub_nodes": [{"id": "000", "title": "Foundations", "description": "There are a number of open foundational mathematical and philosophical problems that have bearing on multiple facets of safety (<a href='http://intelligence.org/files/TechnicalAgenda.pdf' target='_blank'>Soares and Fallenstein 2014a</a>).", "mini_description": "Foundational mathematical or philosophical problems that have bearing on multiple facets of safety", "breakdowns": [{"id": "0000", "sub_nodes": [{"id": "00000", "title": "Foundations of Rational Agency", "description": "Within foundational mathematics, logic, and philosophy, there are some key open problems underpinning a full theoretical understanding of rational agency (<a href='http://intelligence.org/files/TechnicalAgenda.pdf' target='_blank'>Soares and Fallenstein 2014a</a>).", "mini_description": "Fundamental areas of math, logic, and philosophy containing open problems relevant to rational agency", "breakdowns": [{"id": "000000", "sub_nodes": [{"id": "0000000", "title": "Logical Uncertainty", "description": "There are different types of uncertainty, with the most common type being the known unknowns of empirical uncertainty (<a href='https://people.eecs.berkeley.edu/~russell/papers/cacm15-oupm.pdf' target='_blank'>Russell 2015</a>). Given significantly less attention than merited by its foundational utility has been logical uncertainty, which addresses the unknown knowns of potentially voluminous entailments of things already known (<a href='http://intelligence.org/files/QuestionsLogicalUncertainty.pdf' target='_blank'>Soares and Fallenstein 2014</a>). Just as one can apply deduction about some inductive steps and entailments, one can oftentimes conversely apply induction to expectations about deductions (<a href='https://mitpress.mit.edu/books/reasoning-about-uncertainty' target='_blank'>Halpern 2003</a>, <a href='http://www.hutter1.net/publ/sproblogic.pdf' target='_blank'>Hutter et al. 2013</a>, <a href='http://ict.usc.edu/pubs/Logical%20Prior%20Probability.pdf' target='_blank'>Demski 2012</a>, <a href='http://dx.doi.org/10.1007/978-3-319-08795-5_2' target='_blank'>Russell 2014a</a>). Logical uncertainty is actually implied, but sorely underanalyzed, in domains such as probability, bayesian reasoning, game theory, and economics, since there are sequences of deductive thinking steps involved (<a href='https://intelligence.org/2016/04/21/two-new-papers-uniform/' target='_blank'>Soares 2016a</a>). Logical uncertainty bridges rational choice theory and probability theory, combining logic and observation and their respective time-embedded epistemic uncertainties (<a href='http://ict.usc.edu/pubs/Logical%20Prior%20Probability.pdf' target='_blank'>Demski 2012</a>, <a href='http://intelligence.org/files/TechnicalAgenda.pdf' target='_blank'>Soares and Fallenstein 2014a</a>).", "mini_description": "The state of not knowing, or not yet knowing, the full set of logical entailments of facts that are already known", "links": [{"id": "00203030201", "reason": "From Logical Uncertainty also see Logical Induction on the operationalization and management of logical uncertainty in agents."}, {"id": "000010001", "reason": "From Logical Uncertainty also see Open Source Game Theory one bridge between intuition pumps and methods for safer self-modification and scalable control that makes use of logical uncertainty."}, {"id": "002030003", "reason": "From Logical Uncertainty also see World-Embedded Solomonoff Induction which would also find progress in logical uncertainty useful."}, {"id": "002030302", "reason": "From Logical Uncertainty also see Resource-Aware Reasoning one broader context that would be a consumer of this work."}, {"id": "0020200010102", "reason": "From Logical Uncertainty also see Game Theoretic Framing as more realistic game theoretic models will include logical uncertainty."}, {"id": "002020000", "reason": "From Logical Uncertainty also see Grounded Ethical Evolution as both economic and game theoretic analyses and simulations can be made more realistic when including logical uncertainty."}, {"id": "0000004"}]}, {"id": "0000001", "title": "Theory of Counterfactuals", "description": "One key open question in foundational rationality is what theory of counterfactual reasoning can be used to specify a procedure which always identifies the best action available to a given agent in a given environment, with respect to a given set of preferences (<a href='https://pdfs.semanticscholar.org/40b3/bbe8d3e0ff66caae3217f4b2fc0e71fd01e2.pdf' target='_blank'>Soares and Fallenstein 2015</a>, <a href='http://acritch.com/media/ai/Andrew_Critch_-_Parametric_Bounded_Lob.pdf' target='_blank'>Critch 2016</a>, <a href='http://intelligence.org/files/TechnicalAgenda.pdf' target='_blank'>Soares and Fallenstein 2014a</a>).", "mini_description": "Theories of reasoning that use alternate interpretations, alternate histories, and hypothetical scenarios to disambiguate and to find optimal actions", "links": [{"id": "0020303010607", "reason": "From Theory of Counterfactuals also see Counterfactual Reasoning regarding drift detection in induction."}, {"id": "000010000"}, {"id": "0000102"}, {"id": "00201"}, {"id": "00200"}, {"id": "00203"}, {"id": "001"}, {"id": "003"}, {"id": "00401"}, {"id": "00400"}, {"id": "00202"}, {"id": "0020200020101"}]}, {"id": "0000002", "title": "Universal Algorithmic Intelligence", "description": "Universal algorithmic intelligence is a theoretical design for an artificial general intelligence that describes a reinforcement learner combining Solomonoff induction and sequential decision theory (<a href='https://arxiv.org/pdf/cs/0701125v1' target='_blank'>Hutter 2007</a>, <a href='https://arxiv.org/pdf/1202.6153v1' target='_blank'>Hutter 2012</a>). Though its original formulation with optimality guarantees has very impactical computational complexity, there are approximations (<a href='http://arxiv.org/abs/1007.2049' target='_blank'>Veness et al. 2010</a>) of the algorithm that can be computed, and variants oriented toward more typical AI capabilities (<a href='https://arxiv.org/pdf/1209.4290v1' target='_blank'>Potapov et al. 2012</a>). There has even been serious exploration as to making it learn values and be empathetic (<a href='https://arxiv.org/pdf/1308.0702v1' target='_blank'>Potapov and Rodionov 2013</a>). It's also used in the seminal measure of algorithmic intelligence, universal intelligence (<a href='http://dx.doi.org/10.1007/s11023-007-9079-x' target='_blank'>Legg and Hutter 2007</a>). Because of its theoretical simplicity, this paradigm may be most useful for establishing bounds and theoretical limits of reinforcement learners or artificial agents more generally.", "mini_description": "A theoretical first-principles framework for artificial general intelligence", "links": [{"id": "00002", "reason": "From Universal Algorithmic Intelligence also see Projecting Behavioral Bounds to which this can contribute."}]}, {"id": "0000003", "title": "Theory of Ethics", "description": "There are significant open problems in the theoretical and philosophical foundations of both the structure of ethics and its uses.", "mini_description": "The theoretical and philosophical foundations of the structure and utility of ethics", "links": [{"id": "005"}]}, {"id": "0000004", "title": "Bounded Rationality", "description": "There remain foundational questions within the fields of logic, deduction, and philosophy about the theoretical underpinnings of what it means to be rational when there are insufficient resources to satisfy standard rationality (<a href='https://www.jair.org/media/133/live-133-1446-jair.pdf' target='_blank'>Russell and Subramanian 1995</a>, <a href='http://www.cs.cornell.edu/home/halpern/papers/rbdec.pdf' target='_blank'>Halpern et al. 2014</a>).", "mini_description": "Theoretical underpinnings of what it means to be rational when there are insufficient resources to satisfy standard rationality", "links": [{"id": "0000000"}, {"id": "002030302"}]}]}]}, {"id": "00001", "title": "Consistent Decision Making", "description": "To ensure that decisions made are not erratic, unstable, or suboptimal, explicit constructs to allow for stable, consistent, reasonable, and ideally optimal decisions given the knowledge, goals, and faculties available to the agent for it to be robust (<a href='http://futureoflife.org/data/documents/research_priorities.pdf' target='_blank'>Russell et al. 2015</a>).", "mini_description": "Constructs to allow for stable, consistent, reasonable, and ideally optimal decisions given the knowledge, goals, and faculties that the agent has", "breakdowns": [{"id": "000010", "sub_nodes": [{"id": "0000100", "title": "Decision Theory", "description": "Decision theory, the mathematical study of strategies for optimal decision-making between options involving different risks or expectations of gain or loss, has been long studied (<a href='http://bayes.cs.ucla.edu/BOOK-2K/pref.html' target='_blank'>Pearl 2000</a>), but existing methods are not robust, especially with regard to counterfactual reasoning. Mathematical tools such as formal logic, probability, and decision theory have yielded significant insight into the foundations of reasoning and decision making. However, there are still many open problems in the foundations of reasoning and decision (<a href='https://pdfs.semanticscholar.org/40b3/bbe8d3e0ff66caae3217f4b2fc0e71fd01e2.pdf' target='_blank'>Soares and Fallenstein 2015</a>), and solutions to these problems may make the behavior of very capable systems much more reliable and predictable (<a href='http://futureoflife.org/data/documents/research_priorities.pdf' target='_blank'>Russell et al. 2015</a>, <a href='http://intelligence.org/files/UDTSearchOrder.pdf' target='_blank'>Benson-Tilsen 2014</a>). Developing a general theory of highly reliable decision-making, even if it is too idealized to be directly implemented, gives us the conceptual tools needed to design and evaluate safe heuristic approaches (<a href='http://intelligence.org/files/TechnicalAgenda.pdf' target='_blank'>Soares and Fallenstein 2014a</a>, <a href='https://pdfs.semanticscholar.org/40b3/bbe8d3e0ff66caae3217f4b2fc0e71fd01e2.pdf' target='_blank'>Soares and Fallenstein 2015</a>).", "mini_description": "The mathematical study of strategies for optimal decision-making", "breakdowns": [{"id": "00001000", "sub_nodes": [{"id": "000010000", "title": "Logical Counterfactuals", "description": "Consideration of hypothetical scenarios in which logical realities and consequences were different than they currently are can and should be evaluated to support causal understanding, understanding of current options, and to enable the generalization of regret terms in learning algorithms. In a counterfactual situation where a given deterministic decision procedure selects a different action from the one it selects in reality, how can one determine the implications of this counterfactual on other algorithms? (<a href='http://intelligence.org/files/TechnicalAgenda.pdf' target='_blank'>Soares and Fallenstein 2014a</a>) I.e., is there some satisfactory way to formalize and generalize logical counterfactuals? (<a href='http://intelligence.org/files/TechnicalAgenda.pdf' target='_blank'>Soares and Fallenstein 2014a</a>) A method for reasoning about logical counterfactuals in the first place seems a prerequisite to formalizing a more general theory of counterfactuals (<a href='https://pdfs.semanticscholar.org/40b3/bbe8d3e0ff66caae3217f4b2fc0e71fd01e2.pdf' target='_blank'>Soares and Fallenstein 2015</a>). While updateless decision theory consistently theoretically outperforms causal decision theory, it will remain an abstraction until such formalizations of counterfactuals are developed (<a href='https://intelligence.org/wp-content/uploads/2014/10/Hintze-Problem-Class-Dominance-In-Predictive-Dilemmas.pdf' target='_blank'>Hintze 2014</a>, <a href='http://www.aaai.org/ocs/index.php/WS/AAAIW14/paper/viewFile/8833/8294' target='_blank'>LaVictoire et al. 2014</a>). Relatedly an analysis of Lob's theorem in bounded scenarios has implications for cooperative behavior (<a href='http://acritch.com/media/ai/Andrew_Critch_-_Parametric_Bounded_Lob.pdf' target='_blank'>Critch 2016</a>).", "mini_description": "Methods for evaluating hypothetical scenarios where logical realities and consequences are different than they are", "links": [{"id": "0000001"}, {"id": "0020303010607"}, {"id": "00200010101"}]}, {"id": "000010001", "title": "Open Source Game Theory", "description": "Game theory becomes qualitatively different when players are translucent (<a href='http://www.tark.org/proceedings/tark_jan7_13/p216-halpern.pdf' target='_blank'>Halpern and Pass 2013</a>) or transparent to each other. Open source game theory, optimal decision making in a multiagent environment where agents can see each other's source code, is useful for the modeling of one agent controlling another (<a href='http://acritch.com/media/ai/Andrew_Critch_-_Parametric_Bounded_Lob.pdf' target='_blank'>Critch 2016</a>, <a href='https://ie.technion.ac.il/~moshet/progeqnote4.pdf' target='_blank'>Tennenholtz 2004</a>). One key goal of this area is to find ways to foster robust cooperation between agents (<a href='https://arxiv.org/pdf/1401.5577v1' target='_blank'>Barasz et al. 2014</a>, <a href='http://intelligence.org/files/TechnicalAgenda.pdf' target='_blank'>Soares and Fallenstein 2014a</a>). It is also particularly useful for scenarios where one agent will be creating another agent that is to be trusted.", "mini_description": "Optimal decision making in a multiagent environment where agents can see each other's source code, useful for modeling one agent controlling another", "links": [{"id": "0000000"}, {"id": "0040101", "reason": "From Open Source Game Theory also see Controlling Another Algorithm to which open source game theory has direct import."}, {"id": "002030003"}, {"id": "00002"}, {"id": "002020000"}, {"id": "002030002"}, {"id": "004010402"}]}]}], "links": [{"id": "001"}, {"id": "0020303010607"}]}, {"id": "0000101", "title": "Safer Self-Modification", "description": "Sufficiently advanced agents may modify themselves. Doing so in a stable manner and without leading to detrimental directions is a challenge (<a href='http://intelligence.org/files/TechnicalAgenda.pdf' target='_blank'>Soares and Fallenstein 2014a</a>). Though very difficult to get right, there are some techniques to help make self-modification or generation of successors to be more goal-stable. Early attempts at making this safer leverage extensive introspection and value-based prioritization (<a href='http://people.idsia.ch/~steunebrink/Publications/TR13_bounded_recursive_self-improvement.pdf' target='_blank'>Nivel et al. 2013</a>, <a href='http://people.idsia.ch/~steunebrink/Publications/AGI16_growing_recursive_self-improvers.pdf' target='_blank'>Steunebrink et al. 2016</a>, <a href='https://intelligence.org/files/csrbai/steunebrink-slides.pdf' target='_blank'>Steunebrink 2016</a>). But because this self-improvement dynamic can snowball, one would like stronger assurances around safety (<a href='http://intelligence.org/files/TechnicalAgenda.pdf' target='_blank'>Soares and Fallenstein 2014a</a>).", "mini_description": "Techniques to make self-modification or generation of successors to be more goal-stable", "breakdowns": [{"id": "00001010", "sub_nodes": [{"id": "000010100", "title": "Vingean Reflection", "description": "If a system will attain superintelligence through self-improvement, then the impact of the system depends entirely upon the correctness of the original agents reasoning about its self-modifications, and the subsequent ability of its successor to do the same (<a href='https://intelligence.org/files/VingeanReflection.pdf' target='_blank'>Fallenstein and Soares 2015</a>, <a href='http://intelligence.org/files/TechnicalAgenda.pdf' target='_blank'>Soares and Fallenstein 2014a</a>). Generally when one requires extremely high reliability today, implementation-verifiable formal logic based systems are used. Formalized logic to reason about the correctness of self-modifications or prospectively generated superior successors is termed Vingean reflection (<a href='https://intelligence.org/files/VingeanReflection.pdf' target='_blank'>Fallenstein and Soares 2015</a>).", "mini_description": "Formalized logic to reason about the correctness of self-modifications", "breakdowns": [{"id": "0000101000", "sub_nodes": [{"id": "00001010000", "title": "Abstractly Reason About Superior Agents", "description": "Reasoning about and prediction of what the prospective superior agent would do requires a more viable approach than computing all possible action paths (<a href='http://intelligence.org/files/TilingAgents.pdf' target='_blank'>Yudkowsky and Herreshoff 2013</a>). Agents must abstractly reason about agents which are smarter than themselves, without attempting to replicate the full processes of the superior agent, highly leveraging a rich landscape of counterfactuals (<a href='http://www-rohan.sdsu.edu/faculty/vinge/misc/singularity.html' target='_blank'>Vinge 1993</a>, <a href='https://intelligence.org/files/VingeanReflection.pdf' target='_blank'>Fallenstein and Soares 2015</a>, <a href='http://intelligence.org/files/TechnicalAgenda.pdf' target='_blank'>Soares and Fallenstein 2014a</a>).", "mini_description": "Foundations and methods for abstractly reasoning about and predicting what a generally more capable agent would do"}, {"id": "00001010001", "title": "Reflective Induction Confidence", "description": "Establishing the confidence in such reflective induction can be aided by using logical induction to establish a framework for determining confidence when formally reasoning about superior agents in the first place (<a href='http://intelligence.org/files/TechnicalAgenda.pdf' target='_blank'>Soares and Fallenstein 2014a</a>).", "mini_description": "Utilizing logical induction to establish a framework for determining confidence when formally reasoning about superior agents", "links": [{"id": "00203030201"}]}, {"id": "00001010002", "title": "Lobian Obstacle", "description": "The Lobian obstacle, essentially that no sufficiently strong formal system can know that everything that it proves to be true is actually true, is a formidable issue when considering recursive verification (<a href='http://intelligence.org/files/TechnicalAgenda.pdf' target='_blank'>Soares and Fallenstein 2014a</a>). An important question to resolve is how can agents gain very high confidence in agents that use similar reasoning systems while avoiding paradoxes of self-reference (<a href='https://intelligence.org/files/VingeanReflection.pdf' target='_blank'>Fallenstein and Soares 2015</a>).", "mini_description": "Dealing with the fact that no sufficiently strong formal system can know that everything that it proves to be true is actually true"}]}], "links": [{"id": "0000102"}, {"id": "00103"}]}, {"id": "000010101", "title": "Optimal Policy Preservation", "description": "One technique for stability of goals for nearer-term systems involves creating the conditions under which modifying the reward function of a markov decision process would preserve the optimal policy (<a href='http://www.robotics.stanford.edu/~ang/papers/shaping-icml99.pdf' target='_blank'>Ng et al. 1999</a>).", "mini_description": "Creating conditions where modifying the reward function of a markov decision process would preserve the optimal policy", "links": [{"id": "0000102"}]}, {"id": "000010102", "title": "Safety Technique Awareness", "description": "As a system gains skill in software development and algorithms research, explicit modeling and usage of the variety of safety techniques listed in this document can help it to improve its overall robustness and goal stability.", "mini_description": "Explicit modeling and system knowledge of AI safety techniques, and incentivizing use of them", "links": [{"id": "005040204", "reason": "From Safety Technique Awareness also see Ethical Motivation as it will need to not only understand these techniques but want to apply them."}, {"id": "002030103", "reason": "From Safety Technique Awareness also see Metareasoning as reasoning about the agent's internal processes with cognizance of these techniques may aid long-term robustness."}]}]}], "links": [{"id": "0040101", "reason": "From Safer Self-Modification also see Controlling Another Algorithm as \"self\" modification tasks are sometimes interested in creating successor agents or alternatively subagents, which should typically be subject to control."}, {"id": "00103"}]}, {"id": "0000102", "title": "Goal Stability", "description": "The maintenance of stability in the objectives of an advanced agent is challenging (<a href='https://agentfoundations.org/item?id=130' target='_blank'>LaVictoire 2015</a>, <a href='https://intelligence.org/files/IdealAdvisorTheories.pdf' target='_blank'>Muehlhauser and Williamson 2013</a>). Methods for safer self-modification can be used to help maintain goal stability in systems that self-improve. When a static objective is acceptable, e.g. for some for short-to-medium-lived agents, one would want mechanisms to maintain the stability of the overall goals while providing flexibility as to situationally appropriate subobjective priority.", "mini_description": "Techniques to maintain the stability of goals in the face of pressures to change them", "breakdowns": [{"id": "00001020", "sub_nodes": [{"id": "000010200", "title": "Nontransitive Options", "description": "Whether individual operators, an aggregation of values of a large number of humans, or potentially the agent itself can have nontransitive preferences, where cycles or loops of comparative preferences occur (<a href='http://www.sciencedirect.com/science/article/pii/0165489694007357' target='_blank'>Dombi and Vincze 1994</a>). While some approaches seek to eliminate such a situation before it arises, more fault-tolerant approaches will attempt to handle such cycles gracefully (<a href='http://hirokinishimura.net/files/BinRelRep.pdf' target='_blank'>Nishimura and Ok 2016</a>), and this is an open area of research. It is conceivable that solutions to problems like low impact and corrigibility will result in agents that violate one or more Von Neumann-Morgenstern axioms (Taylor 2016b), and neuromorphic architectures such as the common deep learning architectures of the day, will almost certainly violate them. The challenge is to do this in a way that's reflectively stable, having the agent not want to rewrite itself into a different agent, and still allows the agent to have a sensible world model. Minimax decision rules are one plausible variant where this can hold (Taylor 2016b).", "mini_description": "How to maintain stability of goals, objectives, and actions in the presence of options or preferences that are cyclic or nontransitive", "links": [{"id": "00200010102"}]}]}], "links": [{"id": "0000001"}, {"id": "000010101"}, {"id": "002030103", "reason": "From Goal Stability also see Metareasoning types of which enable an agent to choose to e.g. avoid wireheading."}, {"id": "000010100", "reason": "From Goal Stability also see Vingean Reflection which is key to this."}, {"id": "00201", "reason": "From Goal Stability also see Avoiding Reward Hacking which is a crucial prerequisite to goal stability."}, {"id": "0040000", "reason": "From Goal Stability also see Corrigibility for which goal stability is a prerequisite."}, {"id": "00200010102", "reason": "From Goal Stability also see Multiobjective Optimization for this purpose, as multiobjectively optimized ensembles of subobjectives containing specifically formulated goal stability subobjectives might prevent reward function modification."}, {"id": "0020201", "reason": "From Goal Stability also see Degree of Value Evolution which considers why one might want evolution of goals and how that might be managed."}, {"id": "002030201"}]}]}], "links": [{"id": "0040000"}]}, {"id": "00002", "title": "Projecting Behavioral Bounds", "description": "In order to assess risks, it is helpful to understand the theoretical limits on the capabilities being considered (Selman 2017). One can understand the theoretical upper and lower limits of a class of agents without needing to know the details of their architecture (<a href='https://pdfs.semanticscholar.org/530a/1373abd714070cb9b65ab6534216b98512f5.pdf' target='_blank'>Zhang et al. 2014</a>). For advanced agents, the methods by which developers or operators can determine the theoretical bounds or maximum capabilities of a system include proofs stemming from its architecture and communication of the state of the system.", "mini_description": "Methods by which developers or operators can determine the theoretical bounds or maximum capabilities of a system", "breakdowns": [{"id": "000020", "sub_nodes": [{"id": "0000200", "title": "Computational Complexity", "description": "It may be possible to characterize or bound the time or resources particular algorithms or capabilities would require, both in their exact and approximate forms (Selman 2017). Understanding upper, average, and lower time and resource bounds regarding particular types of functionality would be useful for purposes of projecting risks and weighing tradeoffs (<a href='https://arxiv.org/pdf/1608.04112v4' target='_blank'>Kosoy 2016</a>). Typically such analyses are done by algorithm designers upfront, but enabling advanced systems to do this analysis well themselves can help tighten the estimates of resources required for specific actions, leading to more robust decisions.", "mini_description": "Characterizing or bounding the time or resources particular algorithms or capabilities would require", "links": [{"id": "002030302", "reason": "From Computational Complexity also see Resource-Aware Reasoning about such requirements."}]}]}], "links": [{"id": "0000002", "reason": "From Universal Algorithmic Intelligence also see Projecting Behavioral Bounds to which this can contribute."}, {"id": "000010001"}]}]}]}, {"id": "001", "title": "Verification", "description": "Verification is a class of techniques that help prove a system satisfies particular properties its designers desire. They provide high-confidence assurances that a system will satisfy some set of formal constraints, helping to answer whether the system was built correctly<i> given its specification.</i> (<a href='http://futureoflife.org/data/documents/research_priorities.pdf' target='_blank'>Russell et al. 2015</a>) When possible, it is advisable for systems in safety-critical situations, such as medical devices or weapons, to be verifiable. Verification of AI systems poses additional challenges on top of those in verifying more traditional software, but It should be possible in many cases to verify the designs of AI systems. If such systems become increasingly powerful and safety-critical, verifiable safety properties will become increasingly valuable and worthwhile.", "mini_description": "Techniques that help prove a system was implemented correctly given a formal specification", "breakdowns": [{"id": "0010", "sub_nodes": [{"id": "00100", "title": "Formal Software Verification", "description": "When people desire extremely high reliability, e.g. for autopilot software, they often use formal logical systems to maximize their certainty of implementation correctness (<a href='http://csrc.nist.gov/publications/history/dod85.pdf' target='_blank'>DoD 1985</a>, <a href='http://futureoflife.org/data/documents/research_priorities.pdf' target='_blank'>Russell et al. 2015</a>). This is the correct-by-construction approach to software engineering, where a system is developed in tandem with a detailed formal specification and a proof of total correctness given that specification, usually by generating the system from the formal specification (<a href='http://doi.acm.org/10.1145/336512.336546' target='_blank'>Lamsweerde 2000</a>). Creating a provably correct implementation, given a specification, is applicable for a range of layers of the software stack (<a href='http://dl.acm.org/citation.cfm?id=2402695' target='_blank'>Fisher 2012</a>, <a href='https://mitpress.mit.edu/books/principles-model-checking' target='_blank'>Baier and Katoen 2008</a>, <a href='http://www.springer.com/gp/book/9783319105741' target='_blank'>Clarke et al. 2017</a>). The seL4 kernel, for example, is a complete, general-purpose operating system kernel that has been mathematically checked against a formal specification to give strong guarantees against crashes and unsafe operations (<a href='http://web1.cs.columbia.edu/~junfeng/09fa-e6998/papers/sel4.pdf' target='_blank'>Klein et al. 2009</a>). For systems or agents that operate in environments that are at best only partially known by the system designer, it may still be practical to verify that the system acts correctly<i> given the knowledge that it has</i>, which avoids the problem of modelling the real environment (<a href='http://repository.liv.ac.uk/13195/1/verification_arxiv.pdf' target='_blank'>Dennis et al. 2013</a>) but puts much stronger onus on the formal specification to be valid.", "mini_description": "Generating a correct-by-construction implementation of a system given from its formal specification", "breakdowns": [{"id": "001000", "sub_nodes": [{"id": "0010000", "title": "Verified Component Design Approaches", "description": "The assembly of more compound software that combines pre-verified components greatly eases the use of correct-by-construction techniques (<a href='http://futureoflife.org/data/documents/research_priorities.pdf' target='_blank'>Russell et al. 2015</a>). If theories of extending verifiable properties from components to entire systems hold, then even very extensive systems can hold certain kinds of safety guarantees, potentially aided by techniques designed explicitly to handle the higher-level semantic, behavioral, and distributional properties (Russell and Norvig 2010) of AI and ML systems. Modular systems of formal mathematics (<a href='http://ssr.msr-inria.inria.fr/doc/tutorial-itp13/slides.pdf' target='_blank'>Mahboubi and Tassi 2013</a>, <a href='https://hal.inria.fr/hal-01107944' target='_blank'>Rouhling et al. 2015</a>) can serve both as a model for, and as a resource in creating, such modular verified frameworks and components.", "mini_description": "Assembly of more compound software that combines pre-verified components", "links": [{"id": "0020303010608", "reason": "From Verified Component Design Approaches also see ML with Contracts since those techniques can help significantly with architectural componentization decisions."}, {"id": "0020106", "reason": "From Verified Component Design Approaches also see Careful Engineering since applying modular design, verification, and other careful process to at least the base of an AI system can mitigate many danger points."}, {"id": "0030000"}]}, {"id": "0010001", "title": "Adaptive Control Theory", "description": "Systems that interact with the external world, such as cybersecurity systems and trading agents, necessarily have parameters that vary or are initially unknown. The field of adaptive control theory addresses design of such systems and serves as a step in the direction of the AI systems being addressed in this landscape guide (<a href='http://store.doverpublications.com/0486462781.html' target='_blank'>AAstr\"om and Wittenmark 2013</a>). Some work has been done on verification of such systems (<a href='http://ntrs.nasa.gov/archive/nasa/casi.ntrs.nasa.gov/20100031105.pdf' target='_blank'>Crespo et al. 2010</a>), and may help inform verification of more sophisticated learning systems (<a href='http://futureoflife.org/data/documents/research_priorities.pdf' target='_blank'>Russell et al. 2015</a>).", "mini_description": "Creating reliable systems that have parameters that vary or are initially unknown"}, {"id": "0010002", "title": "Verification of Cyberphysical Systems", "description": "When the external world that agents interact with is the physical world, further complications come into play. It is often difficult to actually apply formal verification techniques to physical systems, especially systems that have not been designed specifically to support verification. This has motivated research seeking a general theory linking functional specifications to physical states of the world (<a href='https://www.isr.umd.edu/sites/default/files/110919_Platzer.pdf' target='_blank'>Platzer 2010</a>), usually confined to the area surrounding the system. This type of theory would allow use of formal tools to anticipate and control behaviors of systems that approximate rational agents, alternate designs such as satisficing agents, and systems that cannot be easily described in the standard agent formalism (powerful prediction systems, theorem provers, limited-purpose science or engineering systems, and so on, <a href='http://futureoflife.org/data/documents/research_priorities.pdf' target='_blank'>Russell et al. 2015</a>). Such a theory may also allow rigorous demonstrations that a system is prevented or constrained from performing certain kinds of reasoning or taking certain kinds of actions (<a href='http://futureoflife.org/data/documents/research_priorities.pdf' target='_blank'>Russell et al. 2015</a>). The application of reachability analysis and hybrid discrete/continuous dynamics to the formal verification of computational physical entities operating in physical spaces seem promising (<a href='https://www.isr.umd.edu/sites/default/files/110919_Platzer.pdf' target='_blank'>Platzer 2010</a>, <a href='https://www.cis.upenn.edu/~alur/EmsoftSurvey11.pdf' target='_blank'>Alur 2011</a>, <a href='https://pdfs.semanticscholar.org/5608/b9ed2454a8788d0ecaab9a10d87b78125d70.pdf' target='_blank'>Winfield et al. 2014</a>, <a href='http://www.cs.cmu.edu/~jeannin/papers/acasx.pdf' target='_blank'>Jeannin et al. 2015</a>, <a href='http://symbolaris.com/pub/discworld.pdf' target='_blank'>Loos et al. 2013</a>, <a href='http://dx.doi.org/10.1109/ICCPS.2012.25' target='_blank'>Mitsch et al. 2012</a>, <a href='http://arxiv.org/abs/1605.00604' target='_blank'>Mitsch et al. 2016</a>, <a href='https://mitpress.mit.edu/books/principles-cyber-physical-systems' target='_blank'>Alur 2015</a>), as does extending the introspection and proof capabilities of differential dynamic logic (<a href='http://nfulton.org/papers/lpdl.pdf' target='_blank'>Fulton and Platzer 2016</a>).", "mini_description": "Applying verification techniques to systems that have both physical components and software components, operating in the physical world"}, {"id": "0010003", "title": "Making Verification More User Friendly", "description": "Creating verified or correct-by-construction software has a much higher overhead as compared with unverified software (<a href='https://pdfs.semanticscholar.org/c593/e5fc056b519cd43b5fdf033eb3281cd74983.pdf' target='_blank'>Ouimet and Lundqvist 2007</a>). Creating methodologies, algorithms, tools, and cultures around creating verified software with the aim of making it easier for software developers to decide to use these methods would be quite worthwhile. While verification based on theorem provers can scale to larger systems than ones based on model checkers can, and the former can also reason inductively where the latter is unable to (<a href='https://pdfs.semanticscholar.org/c593/e5fc056b519cd43b5fdf033eb3281cd74983.pdf' target='_blank'>Ouimet and Lundqvist 2007</a>), theorem provers pose usability issues for systems of realistic size. The proofs created by such verification systems for software of practical size are unfortunately quite unweildy (<a href='https://pdfs.semanticscholar.org/c593/e5fc056b519cd43b5fdf033eb3281cd74983.pdf' target='_blank'>Ouimet and Lundqvist 2007</a>), calling for easier ways to explore, summarize, check, and query them. There are also a number of relevant open problems with proof verifiers (<a href='https://arxiv.org/pdf/1609.00331v3' target='_blank'>Yampolskiy 2016</a>) that can impact their use on AI.", "mini_description": "Make it easier for engineers to create and understand verified systems"}]}], "links": [{"id": "0030000", "reason": "From Formal Software Verification also see Verified Downstack Software since having the lower layers of a system being verified often gets much of the benefit with appreciably less overhead."}]}, {"id": "00101", "title": "Automated Vulnerability Finding", "description": "Most systems today are not developed with correct-by-construction methodologies but rather are programmed directly. While techniques for attempting to verify such systems exist, there are many aspects and constructs they cannot cope with (<a href='http://www.kroening.com/papers/tcad-sw-2008.pdf' target='_blank'>DSilva et al. 2008</a>), often due to their dynamicity, but even some formal proofs may be unverifiable (<a href='https://arxiv.org/pdf/1609.00331v3' target='_blank'>Yampolskiy 2016</a>). Given such less-formally specified systems, automated bug and vulnerability finding by abstract static analysis (<a href='http://www.kroening.com/papers/tcad-sw-2008.pdf' target='_blank'>DSilva et al. 2008</a>) and static application security testing (<a href='https://www.brucker.ch/bibliography/download/2014/brucker.ea-sast-expierences-2014.pdf' target='_blank'>Brucker and Sodan 2014</a>) in conjunction with dynamic testing, is a next best technique for reducing implementation errors.", "mini_description": "Automated whitebox techniques for bug finding and security vulnerability finding", "links": [{"id": "00104", "reason": "From Automated Vulnerability Finding also see Implementation Testing for more on dynamic testing."}, {"id": "00301", "reason": "From Automated Vulnerability Finding also see Red Team Analysis which addresses vulnerability finding with blackboxes rather than whiteboxes."}]}, {"id": "00102", "title": "Verification of Intelligent Systems", "description": "Additional challenges with the verification of artificial intelligence and machine learning software come from the fact that behavior is more determined by data as compared to concentional software. To verify such systems, one needs not only provably correct implementations of the system's algorithms (given specifications of those algorithms), but also of the models they learn (<a href='https://people.eecs.berkeley.edu/~dsadigh/Papers/seshia-verifiedAI-arxiv.pdf' target='_blank'>Seshia et al. 2016</a>).: introspective environment modeling, end-to-end specifications and specification mining, developing abstractions for and explanations from ML components, creating new randomized formal methods for systematically generating realistic data, and developing verification engines more oriented around run time, quantitative operations, and learning (<a href='https://people.eecs.berkeley.edu/~dsadigh/Papers/seshia-verifiedAI-arxiv.pdf' target='_blank'>Seshia et al. 2016</a>).", "mini_description": "Applying verification techniques to indirectly specified systems that learn and act intelligently", "breakdowns": [{"id": "001020", "sub_nodes": [{"id": "0010200", "title": "Verification of Whole AI Systems", "description": "The verification of whole AI systems has been underexplored, but a number of challenges are clear. A number of principles to address these challenges have been proposed (<a href='https://people.eecs.berkeley.edu/~dsadigh/Papers/seshia-verifiedAI-arxiv.pdf' target='_blank'>Seshia et al. 2016</a>): introspective environment modeling, end-to-end specifications and specification mining, developing abstractions for and explanations from ML components, creating new randomized formal methods for systematically generating realistic data, and developing verification engines more oriented around run time, quantitative operations, and learning (<a href='https://people.eecs.berkeley.edu/~dsadigh/Papers/seshia-verifiedAI-arxiv.pdf' target='_blank'>Seshia et al. 2016</a>). A number of challenges to verifying AI systems in the general case remain (<a href='https://arxiv.org/pdf/1609.00331v3' target='_blank'>Yampolskiy 2016</a>). Following a componentized architecture, in which guarantees about individual components can be combined according to their connections to yield properties of the overall system seems promising (<a href='https://people.eecs.berkeley.edu/~dsadigh/Papers/seshia-verifiedAI-arxiv.pdf' target='_blank'>Seshia et al. 2016</a>). Review and remediation of systemic issues such as questionable data dependencies, unpleasantly unexpected data flow dynamics, inappropriate or suboptimal abstractions, and mixed technology stacks (<a href='http://research.google.com/pubs/archive/43146.pdf' target='_blank'>Sculley et al. 2014</a>) will help to simplify AI systems to help make them more likely to be meaningfully verifiable.", "mini_description": "Applying end-to-end verification techniques to whole AI systems", "links": [{"id": "0020303010608", "reason": "From Verification of Whole AI Systems also see ML with Contracts as software interface contracts make segmentations within the analyses needed for formal verification much easier."}]}, {"id": "0010201", "title": "Verification of Machine Learning Components", "description": "Each type of machine learning algorithm or paradigm may need to be considered individually with respect to what aspects need better formalization, quantization, introspection, or proofs (<a href='http://futureoflife.org/data/documents/research_priorities.pdf' target='_blank'>Russell et al. 2015</a>). There has been some work on requirements for verifying neural networks (<a href='https://pdfs.semanticscholar.org/72e5/5b90b5b791646266b0da8f6528d99aa96be5.pdf' target='_blank'>Pulina and Tacchella 2010</a>, <a href='http://www.springer.com/gp/book/9783642106897' target='_blank'>Schumann and Liu 2010</a>). There has also been exploration of directions for more nuanced and efficient summarization of the high-dimensional distributions many learning algorithms produce (<a href='http://www.jmlr.org/proceedings/papers/v48/achim16.pdf' target='_blank'>Achim et al. 2016</a>, null, <a href='https://cs.stanford.edu/~ermon/papers/kim-sabharwal-ermon.pdf' target='_blank'>Kim et al. 2016</a>), whereby quantization of the distributions might enable simple deductive proofs and satisfiability analyses. State coarse-graining or abstraction for reinforcement learners conforming to specified partial programs provide a path for formalized treatment of learned policies (<a href='https://people.eecs.berkeley.edu/~russell/papers/aaai02-alisp.pdf' target='_blank'>Andre and Russell 2002</a>). In addition to accounting for particular learned models, one must also account for future changes to the learner as new data arrives, another underexplored challenge (<a href='https://people.eecs.berkeley.edu/~dsadigh/Papers/seshia-verifiedAI-arxiv.pdf' target='_blank'>Seshia et al. 2016</a>).", "mini_description": "Applying verification to components or algorithms that learn from data", "links": [{"id": "0020001000000", "reason": "From Verification of Machine Learning Components also see Defined Impact Regularizer as there are conceptual parallels between partial programs and distance from a baseline policy."}]}]}]}, {"id": "00103", "title": "Verification of Recursive Self-Improvement", "description": "A related verification research topic that is a distinctive long-term concern is the verifiability of systems that modify, extend, or improve themselves, successors, or descendant programs, quite possibly many times in succession (<a href='http://www.sciencedirect.com/science/article/pii/S0065245808604180' target='_blank'>Good 1965</a>, <a href='http://www-rohan.sdsu.edu/faculty/vinge/misc/singularity.html' target='_blank'>Vinge 1993</a>, <a href='http://arxiv.org/abs/1502.06512' target='_blank'>Yampolskiy 2015</a>). Attempting to straightforwardly apply formal verification tools to this more general setting presents new difficulties (<a href='http://futureoflife.org/data/documents/research_priorities.pdf' target='_blank'>Russell et al. 2015</a>). With many potential approaches, a formal system that is sufficiently powerful cannot use formal methods in the obvious way to gain assurance about the accuracy of functionally similar formal systems, due to Gdels incompleteness theorems (<a href='https://intelligence.org/files/ProblemsSelfReference.pdf' target='_blank'>Fallenstein and Soares 2014</a>, <a href='http://arxiv.org/pdf/1312.3626.pdf' target='_blank'>Weaver 2013</a>). There has, however, been recent progress in modeling theorem prover verification systems in themselves, specifically illustrated with model polymorphism, cleverly circumventing the Gdellian obstacle (<a href='https://www.cl.cam.ac.uk/~rk436/itp2015a.pdf' target='_blank'>Fallenstein and Kumar 2015</a>).", "mini_description": "How formal systems of verification techniques can be used for generating robust superior successors", "links": [{"id": "0000101", "reason": "From Verification of Recursive Self-Improvement also see Safer Self-Modification as it relates safer, if not provably safe, self-improvement, potentially doing so as a style of learning."}, {"id": "000010100", "reason": "From Verification of Recursive Self-Improvement also see Vingean Reflection which addresses theoretical challenges to reflective proofs."}]}, {"id": "00104", "title": "Implementation Testing", "description": "Typical dynamic testing of software implementations include unit testing, integration testing, functional testing, system testing, stress testing, performance testing, and regression testing. These same kinds of tests can and should be applied to AI systems and agents (<a href='http://research.google.com/pubs/archive/43146.pdf' target='_blank'>Sculley et al. 2014</a>). While the levels of indirection concomitant to AI mean their domains and ranges are much too large to test exhaustively or even representatively, there remains value in this mode of quality assurance. Though not ostensibly representative of the distributions of what the agent can encounter, unit tests, for instance, can catch some representative failure modes that can be as elaborate and appropriate to the agent's complexities as can be imagined (<a href='https://pdfs.semanticscholar.org/363c/c023e00467141712292d9ecafa15acd78b25.pdf' target='_blank'>Coelho et al. 2006</a>). In contrast to the central role unit testing takes with most software development processes, it cannot be relied on to provide assurance for AI agents, and especially not advanced agents that support multiple layers of indirection. When verifying physical systems, or more generally any system properly subject to an open world model, particular cognizance needs to be paid to validating that the specification is complete.", "mini_description": "Techniques for testing implementations, including unit testing, integration testing, functional testing, system testing, stress testing, performance testing, and regression testing", "links": [{"id": "00101"}]}]}], "links": [{"id": "0000001"}, {"id": "0020303010608", "reason": "From Verification also see ML with Contracts because when whole AI systems and agents are designed by contract, piecewise verification becomes more tractable."}, {"id": "0040104", "reason": "From Verification also see Monitoring because when causal flows are more explicit, verifiable AI systems will also lend themselves to better interpretability and transparency."}, {"id": "0000100", "reason": "From Verification also see Decision Theory as the definition of correct or optimal behavior, among the good levels to verify, is made more explicit by it."}]}, {"id": "002", "title": "Validation", "description": "Given even verified software, environmental assumptions can easily not hold in the real world, or the requirements that led to the specification may be faulty or lacking. These sort of specification errors are quite usual in the world of software verification, where it is often observed that writing correct specifications can be more difficult than writing correct code (<a href='http://futureoflife.org/data/documents/research_priorities.pdf' target='_blank'>Russell et al. 2015</a>). Validation is the process of ensuring that a system that meets its formal requirements does not have undesirable behaviors or consequences. It is asking the question \"Did I build (or ask for) the right system?\".<br><br> Ensuring that the formal requirements, the specification, considers all relevant dynamics, and will be beneficial and desirable, does not actually fit into current formally provable paradigms. In order to build systems that robustly behave well, one currently needs to decide what<i> good</i> behavior means in each application domain. This ethical question of good is tied closely to questions of what technologies and engineering techniques are available, how reliable they are, and what trade-offs can be made  all areas where computer science, software engineering, machine learning, and broader AI expertise are valuable. In practical application, a significant consideration is the computational expense of different behavioral standards or ethical theories, i.e. that if a standard cannot be evaluated sufficiently expeditiously to guide behavior in safety-critical situations, cheaper approximations should be used (<a href='https://global.oup.com/academic/product/moral-machines-9780195374049' target='_blank'>Wallach and Allen 2008</a>). It is therefore likely best for different complexities of ethical or moral reasoning to be used for different timescales.<br><br> Designing simplified rules, such as those meant to govern a self-driving cars strategic decisions in critical situations, will likely require expertise from both ethicists and computer scientists (<a href='http://futureoflife.org/data/documents/research_priorities.pdf' target='_blank'>Russell et al. 2015</a>). This is relatively straightforward when specific safety limitations, behaviors, and ethical constraints are known upfront for AI systems that are largely specified upfront (<a href='https://arxiv.org/pdf/1602.04450.pdf' target='_blank'>Berkenkamp et al. 2016</a>, <a href='http://papers.nips.cc/paper/3747-help-or-hinder-bayesian-models-of-social-goal-inference.pdf' target='_blank'>Ullman et al. 2009</a>), but more powerful and operationally-flexible systems require much more sophistication. Computational models of ethical reasoning may shed light on questions of computational expense and the viability of reliable ethical reasoning methods (<a href='http://www.peterasaro.org/writing/Asaro%20IRIE.pdf' target='_blank'>Asaro 2006</a>, <a href='http://link.springer.com/content/pdf/10.1007%2Fs13347-011-0043-6.pdf' target='_blank'>Sullins 2011</a>). Being able to assure known bounded behaviors from methods and systems that learn will also be important for both medium-term and long-term safety. Validation encompasses ensuring an agent understands its environment, decisions, and actions, and that it acts robustly-in-accordance with its operators' wishes.<br><br> In the long term, AI systems might become much more powerful, general-purpose, and autonomous, a regime where failures of validity would carry significantly higher costs than with today's systems. To maximize the long-term value of validity research, machine learning researchers might focus on anticipating, preventing, detecting, and mitigating the types of otherwise unexpected generalization that would be most problematic for very general and capable AI systems. If some concepts could be learned reliably, it might become possible to use those to define tasks or constraints that minimize the chances of unintended consequences even when agents become very general and capable. This topic has been underexplored, so both theoretical and experimental research on it may be useful (<a href='http://futureoflife.org/data/documents/research_priorities.pdf' target='_blank'>Russell et al. 2015</a>).", "mini_description": "Ensuring that the right system specification is provided for the core of the agent given stakeholders' goals for the system", "breakdowns": [{"id": "0020", "sub_nodes": [{"id": "00200", "title": "Averting Instrumental Incentives", "description": "It has been argued that intelligent systems that aim toward some objective have emergent instrumental incentives, pressures to create subplans, to give them more time, power, and resources to fulfill their objectives (<a href='https://selfawaresystems.files.wordpress.com/2008/01/ai_drives_final.pdf' target='_blank'>Omohundro 2008</a>). These instrumental pressures, and the flexibility to act on those pressures, are relatively uncommon in today's rather narrow systems, but would seem much more expected and pronounced as AIs become more general (<a href='https://global.oup.com/academic/product/superintelligence-9780199678112' target='_blank'>Bostrom 2014</a>), and understand a wider array of contexts, modalities, and fields. Progress on formalizing this class of problems has only been recent (<a href='https://intelligence.org/files/FormalizingConvergentGoals.pdf' target='_blank'>Benson-Tilsen and Soares 2016</a>). How can one design and train systems such that they robustly lack default incentives to manipulate and deceive the operators, compete for scarce resources, prevent their maintenance (<a href='https://selfawaresystems.files.wordpress.com/2008/01/ai_drives_final.pdf' target='_blank'>Omohundro 2008</a>), and generally avert negative instrumental behaviors? (<a href='https://intelligence.org/files/AlignmentMachineLearning.pdf' target='_blank'>Taylor et al. 2016</a>, <a href='https://global.oup.com/academic/product/superintelligence-9780199678112' target='_blank'>Bostrom 2014</a>) Averting these implicit pressures in a design is much more difficult than it may appear initially, and there are numerous subtleties to consider (<a href='http://www.fhi.ox.ac.uk/utility-indifference.pdf' target='_blank'>Armstrong 2010</a>, <a href='https://www.fhi.ox.ac.uk/wp-content/uploads/Risks-and-Mitigation-Strategies-for-Oracle-AI.pdf' target='_blank'>Armstrong 2013</a>, <a href='https://intelligence.org/files/FormalizingConvergentGoals.pdf' target='_blank'>Benson-Tilsen and Soares 2016</a>). Sophisticated techniques to avoid such incentives can be needed not only for agents but also oracle-style question answering AIs (<a href='https://dl.dropboxusercontent.com/u/23843264/Permanent/Using_Oracles.pdf' target='_blank'>Armstrong 2016</a>).", "mini_description": "Designing systems so they assuredly do not have instrumental incentives, e.g. to manipulate and deceive the operators, compete for scarce resources, or prevent their maintenance", "breakdowns": [{"id": "002000", "sub_nodes": [{"id": "0020000", "title": "Error-Tolerant Agent Design", "description": "A hallmark of robustness is fault-tolerance. In the case where value alignment or design-time effects are flawed, one might not see those issues in an advanced agent until it has been in production a while. Highly capable agents can also be dangerous if they are specified incorrectly (<a href='https://intelligence.org/files/AIPosNegFactor.pdf' target='_blank'>Yudkowsky 2008</a>). General advanced agents may very well be aware of attempted changes to it, and by default can have emergent or implicit pressures to prevent such changes (<a href='http://intelligence.org/files/TechnicalAgenda.pdf' target='_blank'>Soares and Fallenstein 2014a</a>). Making intelligent systems such that they are amenable to correction, even if they have the ability to prevent or avoid correction, is therefore necessary (<a href='http://intelligence.org/files/TechnicalAgenda.pdf' target='_blank'>Soares and Fallenstein 2014a</a>, <a href='https://intelligence.org/files/AlignmentMachineLearning.pdf' target='_blank'>Taylor et al. 2016</a>, <a href='http://auai.org/uai2016/proceedings/papers/68.pdf' target='_blank'>Orseau and Armstrong 2016</a>).", "mini_description": "Making intelligent systems be amenable to correction, even if they have the ability to prevent or avoid correction", "links": [{"id": "0040000", "reason": "From Error-Tolerant Agent Design also see Corrigibility where preliminary progress on particular types of instrumental incentive aversion have been made."}, {"id": "0040001", "reason": "From Error-Tolerant Agent Design also see Utility Indifference where control of the objective function is facilitated by the agent's explicit indifference about its utility function."}]}, {"id": "0020001", "title": "Domesticity", "description": "The optimizers of today do not annex or use excessive resources not because doing so wouldn't lead to more optimal solutions to the function they're optimizing, but because they lack the capability to annex resources. This will change as AI becomes more general in its capabilities. It has been argued that explicitly and safely incentivizing such an intelligent agent to be low-impact on its environment will therefore be necessary (<a href='www.nickbostrom.com/papers/oracle.pdf' target='_blank'>Armstrong et al. 2012</a>, <a href='http://intelligence.org/files/TechnicalAgenda.pdf' target='_blank'>Soares and Fallenstein 2014a</a>).", "mini_description": "Safely incentivizing an intelligent agent to be low-impact to the world", "breakdowns": [{"id": "00200010", "sub_nodes": [{"id": "002000100", "title": "Impact Measures", "description": "Methods to quantify the amount an agent does, or can do, to change the world become necessary in such regimes. Using such measures, and other techniques, one can start exploring what sorts of mechanisms, including regularizers, might incentivize a system to pursue its goals with minimal side effects (<a href='http://arxiv.org/abs/1606.06565' target='_blank'>Amodei et al. 2016</a>, <a href='https://intelligence.org/files/AlignmentMachineLearning.pdf' target='_blank'>Taylor et al. 2016</a>).", "mini_description": "Methods to quantify the amount an agent does or can do to change the world", "breakdowns": [{"id": "0020001000", "sub_nodes": [{"id": "00200010000", "title": "Impact Regularizers", "description": "Regularizers, methods for structurally shaping learning or penalizing undesirable learning, can be made to penalize an agent for recognizably changing the world (<a href='https://intelligence.org/files/AlignmentMachineLearning.pdf' target='_blank'>Taylor et al. 2016</a>).", "mini_description": "Methods for structurally penalizing an agent for recognizably changing the world", "breakdowns": [{"id": "002000100000", "sub_nodes": [{"id": "0020001000000", "title": "Defined Impact Regularizer", "description": "If one has enough of an object-level understanding of what the agent will encounter in advance, one can elect particular impact measures to use in the regularizers that temper impact. One can choose to penalize changing the environment overall (<a href='http://arxiv.org/abs/1606.06565' target='_blank'>Amodei et al. 2016</a>, <a href='https://intelligence.org/files/AlignmentMachineLearning.pdf' target='_blank'>Taylor et al. 2016</a>) or one can introduce a penalty for changes relative to some baseline environmental state or a baseline policy determined by some explicit method (<a href='https://dl.dropboxusercontent.com/u/23843264/Permanent/Reduced_impact_S+B.pdf' target='_blank'>Armstrong and Levinstein 2015</a>). One can start from safe policy and try to improve the policy it from there, in manners similar to reachability analysis (<a href='http://dx.doi.org/10.1016/S0005-1098(98)00193-9' target='_blank'>Lygeros et al. 1999</a>, <a href='https://www.cs.ubc.ca/~mitchell/Papers/publishedIEEEtac05.pdf' target='_blank'>Mitchell et al. 2005</a>) or to robust policy improvement (<a href='http://dx.doi.org/10.1287/moor.1040.0129' target='_blank'>Iyengar 2005</a>, <a href='http://arxiv.org/abs/1606.06565' target='_blank'>Amodei et al. 2016</a>, <a href='http://dx.doi.org/10.1287/opre.1050.0216' target='_blank'>Nilim and Ghaoui 2005</a>).", "mini_description": "Methods for penalizing an agent for recognizably changing the world as per particular given measures", "links": [{"id": "0010201"}, {"id": "0020001000200"}]}, {"id": "0020001000001", "title": "Learned Impact Regularizer", "description": "Instead of using predefined measures, an agent can learn transferrable side effects across multiple tasks in similar environments (<a href='http://arxiv.org/abs/1606.06565' target='_blank'>Amodei et al. 2016</a>), similar in mechanism to transferring just learned dynamics (<a href='http://www.jmlr.org/papers/volume10/taylor09a/taylor09a.pdf' target='_blank'>Taylor and Stone 2009</a>). This would help it learn to characterize and quantify expected, relevant, and unexpected environmental changes.", "mini_description": "Penalizing an agent for changing the world as per measures learned from similar past experiences"}]}]}, {"id": "00200010001", "title": "Follow-on Analysis", "description": "Causal analysis of downstream effects can be used to either prune actions that would cause deleterious effects, if valence can be ascertained, or prune excess effects at all, if it cannot (<a href='https://intelligence.org/files/AlignmentMachineLearning.pdf' target='_blank'>Taylor et al. 2016</a>, <a href='http://bayes.cs.ucla.edu/BOOK-2K/pref.html' target='_blank'>Pearl 2000</a>).", "mini_description": "Causal analysis of downstream effects can be used to prune actions that would likely cause deleterious effects", "links": [{"id": "002030104"}, {"id": "00200010003"}, {"id": "0020200010101"}]}, {"id": "00200010002", "title": "Avoiding Negative Side Effects", "description": "If possible, one should aim to ensure that the agent won't disturb the environment in specifically<i> negative</i> ways while pursuing its goals, as opposed to in any way at all, and we'd ideally like to do so without specifying what not to disturb (<a href='http://arxiv.org/abs/1606.06565' target='_blank'>Amodei et al. 2016</a>).", "mini_description": "Ways to ensure an agent won't disturb the environment in negative ways while pursuing its goals, ideally without specifying in advance what not to disturb", "breakdowns": [{"id": "002000100020", "sub_nodes": [{"id": "0020001000200", "title": "Penalize Influence", "description": "One way to prevent future abuse of power is to minimize the amount of power needed to accomplish the prescribed task, i.e. penalizing empowerment or some other metric of being in a position to cause side effects (<a href='https://pdfs.semanticscholar.org/d01e/3414ca706eda917576d947ece811b5cbcdde.pdf' target='_blank'>Salge et al. 2014</a>, <a href='http://arxiv.org/abs/1606.06565' target='_blank'>Amodei et al. 2016</a>). As this is essentially the opposite of the type of objective function often used for intrinsic motivation (<a href='https://papers.nips.cc/paper/5668-variational-information-maximisation-for-intrinsically-motivated-reinforcement-learning.pdf' target='_blank'>Mohamed and Rezende 2015</a>) further study is needed on viable formulations of the idea. It has however also been noted that reinforcement learning techniques which punish the agent for attempting to accumulate control would actually incentivize the agent to deceive and appease its creators or operators until it found a way to gain a decisive advantage (<a href='http://intelligence.org/files/TechnicalAgenda.pdf' target='_blank'>Soares and Fallenstein 2014a</a>, <a href='https://global.oup.com/academic/product/superintelligence-9780199678112' target='_blank'>Bostrom 2014</a>).", "mini_description": "Penalizing empowerment or some similar metric of being in a position to cause side effects", "links": [{"id": "0020001000000", "reason": "From Penalize Influence also see Defined Impact Regularizer to contrast penalizing changing things versus penalizing the ability to change things."}, {"id": "0020001000202"}]}, {"id": "0020001000201", "title": "Multi-agent Approaches", "description": "Cooperative multiagent techniques may also disinventivize negative side effects. Transparency of, and cooperation toward, a shared reward function by multiple agents will continually solicit input from multiple stakeholders, and in so doing reduce risk of undesired outcomes per their varied models (<a href='http://arxiv.org/abs/1606.06565' target='_blank'>Amodei et al. 2016</a>). For example, biasing toward goal transparency (<a href='http://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/download/12457/12204' target='_blank'>Greene et al. 2016</a>) via autoencoding reward functions, or alternatively cooperative inverse reinforcement learning, across multiple agents, can help ensure they're happy with resultant environmental changes (<a href='https://arxiv.org/pdf/1611.08219v1' target='_blank'>Hadfield-Menell et al. 2016a</a>).", "mini_description": "Transparency of, and cooperation toward, a shared reward function by multiple agents", "links": [{"id": "004010007", "reason": "From Multi-agent Approaches also see Cooperative Inverse Reinforcement Learner a cooperative, though assymetric, multiagent algorithm."}]}, {"id": "0020001000202", "title": "Reward Uncertainty", "description": "Aware that random changes to the environment are more likely to be bad than good, one can maintain and evaluate a distribution of reward functions, optimized to minimize impact on the environment (<a href='http://arxiv.org/abs/1606.06565' target='_blank'>Amodei et al. 2016</a>). These different sub-reward-functions can each be sensitive to different kinds of undue impacts on the environment, or even just by virtue their variety cause noticable negative side effects to be less common.", "mini_description": "Maintaining a distribution of possible reward functions that are sensitive to undue impacts on the environment", "links": [{"id": "00202000200", "reason": "From Reward Uncertainty also see Operator Value Learning which maintains a distribution of different operator-intended objective functions."}, {"id": "0020001000200", "reason": "From Reward Uncertainty also see Penalize Influence as implementation paths are similar."}, {"id": "00200010301", "reason": "From Reward Uncertainty also see Resource Value Uncertainty for a resource-differentiated analogue."}, {"id": "00200010102", "reason": "From Reward Uncertainty also see Multiobjective Optimization as that provides additional degrees of freedom to subobjectives and should also achieve the target effect."}]}]}]}, {"id": "00200010003", "title": "Values-Based Side Effect Evaluation", "description": "More explicit approaches to avoiding negative side effects, combining common sense reasoning and evaluation of scenarios with respect to values, offer an integrative approach with other aspects of safety and alignment (<a href='http://people.idsia.ch/~steunebrink/Publications/AGI16_growing_recursive_self-improvers.pdf' target='_blank'>Steunebrink et al. 2016</a>, <a href='http://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/download/12457/12204' target='_blank'>Greene et al. 2016</a>). Such techniques can evaluate prospective side effects based on core, compound, and instrumental values in a consequentialist manner.", "mini_description": "Techniques to evaluate prospective side effects based on core and compound values", "links": [{"id": "00200010001"}, {"id": "0020200010101"}, {"id": "00202"}]}]}]}, {"id": "002000101", "title": "Mild Optimization", "description": "AI has typically been about optimizing for a given function as much as possible, but with power and intelligence, functions can be optimized much further than people want or need when not paying heed to side effects (<a href='https://intelligence.org/files/AlignmentMachineLearning.pdf' target='_blank'>Taylor et al. 2016</a>). One would therefore like to be able to achieve a semi-optimization of a target function with relaxed optimality requirements, such as by near-optimization or approximate optimization.", "mini_description": "Semi-optimization of a function with relaxed optimality requirements despite access to resources that would enable extreme optimization", "breakdowns": [{"id": "0020001010", "sub_nodes": [{"id": "00200010100", "title": "Optimization Measures", "description": "Varied ways of measuring optimization and penalizing excessive optimization are emerging. One can regularize against excessive optimization power (<a href='https://intelligence.org/files/AlignmentMachineLearning.pdf' target='_blank'>Taylor et al. 2016</a>), learning corresponding value functions for policies in order to learn less-extreme policies that are more likely to generalize well (<a href='http://papers.nips.cc/paper/3445-regularized-policy-iteration.pdf' target='_blank'>Farahmand et al. 2009</a>), or just generally provide less optimization power (<a href='https://www.amazon.com/Introduction-Development-Second-Steve-Rabin/dp/1584506792' target='_blank'>Rabin 2010</a>). Another option is a regularizer that penalizes more intelligence than is necessary for solving a problem sufficiently, biasing toward the speed of solution and resource conservation (<a href='https://intelligence.org/files/AlignmentMachineLearning.pdf' target='_blank'>Taylor et al. 2016</a>). Penalizing or thresholding the amount of resources or time that can be used for optimization, or stopping when reaching some probabilitsic theshold of the optimum, are options (<a href='http://homepages.herts.ac.uk/~comqdp1/publications/files/cec2005_klyubin_polani_nehaniv.pdf' target='_blank'>Klyubin and Nehaniv 2005</a>, <a href='https://pdfs.semanticscholar.org/d01e/3414ca706eda917576d947ece811b5cbcdde.pdf' target='_blank'>Salge et al. 2014</a>), but each has potential issues, and optimization measures themselves are an open problem (<a href='http://dx.doi.org/10.1007/s11023-007-9079-x' target='_blank'>Legg and Hutter 2007</a>).", "mini_description": "Ways of measuring optimization and penalizing excessive optimization"}, {"id": "00200010101", "title": "Quantilization", "description": "Rather than seeking the most extreme solution to a problem, i.e. maximizing a function such as expected reward, one can instead seek to satisfice expected reward (<a href='http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.545.5116&rep=rep1&type=pdf' target='_blank'>Simon 1956</a>, <a href='https://intelligence.org/files/AlignmentMachineLearning.pdf' target='_blank'>Taylor et al. 2016</a>). This may be approached by choosing actions from a top percentage of possible actions per a sort by expected reward or probability of success though that alone wouldn't necessarily prevent satisfaction from overoptimization (<a href='https://intelligence.org/files/QuantilizersSaferAlternative.pdf' target='_blank'>Taylor 2016</a>). The technique of quantilization selects a strategy randomly within some top percentile of strategies (<a href='https://intelligence.org/files/QuantilizersSaferAlternative.pdf' target='_blank'>Taylor 2016</a>), which probabilistically mitigates such risk.", "mini_description": "An approach for satisficing expected reward rather than using an extreme solution to maximize it", "links": [{"id": "000010000", "reason": "From Quantilization also see Logical Counterfactuals as that can assist with enumeration of options."}]}, {"id": "00200010102", "title": "Multiobjective Optimization", "description": "The simultaneous optimization of two or more subobjectives enables soft consideration and formalized tradeoff of resource usage and other types of impacts, including as measured with respect to arbitrary specified objectives (<a href='http://dx.doi.org/10.1007/978-1-4614-6940-7_15' target='_blank'>Deb 2014</a>, <a href='http://arxiv.org/abs/1609.08082' target='_blank'>Li et al. 2016</a>, <a href='http://arxiv.org/abs/1511.00787' target='_blank'>Lavin 2015</a>, Mallah 2017).", "mini_description": "The simultaneous optimization of two or more subobjectives to temper optimization and to balance concerns", "links": [{"id": "0000102"}, {"id": "00200"}, {"id": "0020001000202"}, {"id": "000010200", "reason": "From Multiobjective Optimization also see Nontransitive Options as there are open issues with integrating multiobjective optimization into safe advanced AIs that conform to the Von Neumann Morgenstern axioms due to the possibility of intransitivities arising in iterated pareto frontiers."}, {"id": "00200010203", "reason": "From Multiobjective Optimization also see Multiobjective Reinforcement Learning for treatment in an RL context."}, {"id": "0020109", "reason": "From Multiobjective Optimization also see Multiple Rewards as that considers an analogue where subobjective variants aim to represent the same conceptual goal."}, {"id": "002020003", "reason": "From Multiobjective Optimization also see Ethical Ensembles as those can be implemented using multiobjective optimization, and further may gracefully balance multiple value systems and multiple operational objectives simultaneously."}, {"id": "002030005"}]}]}], "links": [{"id": "0020002"}, {"id": "00203000102"}, {"id": "00400"}]}, {"id": "002000102", "title": "Safe Exploration", "description": "In addition to various types of resource exploitation being potentially unsafe, exploratory moves can also be unsafe if they bring the agent into an unsafe situation. In order to help ensure that the agent doesnt make exploratory moves, in order to learn, that have very bad repercussions, a variety of safe exploration techniques have been discussed (<a href='http://arxiv.org/abs/1606.06565' target='_blank'>Amodei et al. 2016</a>, <a href='http://www.jmlr.org/papers/volume16/garcia15a/garcia15a.pdf' target='_blank'>Garcia and Fernandez 2015</a>, <a href='http://cmp.felk.cvut.cz/~peckama2/papers/safe_exploration_overview_lncs.pdf' target='_blank'>Pecka and Svoboda 2014</a>).", "mini_description": "Ensuring the agent doesnt make exploratory moves that have very bad repercussions", "breakdowns": [{"id": "0020001020", "sub_nodes": [{"id": "00200010200", "title": "Risk-Sensitive Performance Criteria", "description": "One can change optimization criteria from just total expected reward to other objectives that prevent or minimize downsides of note (<a href='http://www.jmlr.org/papers/volume16/garcia15a/garcia15a.pdf' target='_blank'>Garcia and Fernandez 2015</a>, <a href='http://arxiv.org/abs/1404.3862' target='_blank'>Tamar et al. 2014</a>, <a href='http://arxiv.org/abs/1606.06565' target='_blank'>Amodei et al. 2016</a>, <a href='http://psthomas.com/papers/Thomas2015.pdf' target='_blank'>Thomas et al. 2015</a>). It's also possible to estimate uncertainty within value functions, which could be incorporated into risk-sensitive reinforcement learning (<a href='http://papers.nips.cc/paper/6500-deep-exploration-via-bootstrapped-dqn.pdf' target='_blank'>Osband et al. 2016</a>, <a href='http://jmlr.org/proceedings/papers/v48/gal16-supp.pdf' target='_blank'>Gal and Ghahramani 2016</a>), which may also model \"intrinsic fear\" with probabilities on how close in steps different states are to catastrophe (<a href='https://arxiv.org/pdf/1611.01211v3' target='_blank'>Lipton et al. 2016</a>).", "mini_description": "Changing optimization criteria from total expected reward to other objectives that prevent or minimize downsides of note"}, {"id": "00200010201", "title": "Simulated Exploration", "description": "One way to explore more is to explore in simulations as much as possible. Agents can then safely incrementally update the policies from imperfect simulation-based trajectories with sufficiently accurate off-policy trajectories using semi-on-policy evaluation (<a href='http://arxiv.org/abs/1606.06565' target='_blank'>Amodei et al. 2016</a>). Such techniques can also apply in senses broader than strictly exploration (<a href='https://medium.com/ai-control/learning-with-catastrophes-59387b55cc30' target='_blank'>Christiano 2016d</a>).", "mini_description": "Using simulations to test different exploration strategies to inform actions in the real world"}, {"id": "00200010202", "title": "Bounded Exploration", "description": "Another way to explore safely is to model or otherwise determine the state space regions in which mistakes are relatively inconsequential or recoverable, and exploring only within those areas (<a href='http://icml.cc/2012/papers/838.pdf' target='_blank'>Moldovan and Abbeel 2012</a>, <a href='http://arxiv.org/abs/1606.06565' target='_blank'>Amodei et al. 2016</a>). Recent techniques that do this iteratively for safe exploration using markov decision processes (<a href='http://arxiv.org/abs/1606.04753' target='_blank'>Turchetta et al. 2016</a>) and gaussian processes (<a href='http://www.jmlr.org/proceedings/papers/v37/sui15.pdf' target='_blank'>Schreiter et al. 2015</a>) can explore unknown environments without getting into irreversible situations.", "mini_description": "Establishing the bounds inside of which mistakes are recoverable and exploring only within that area", "links": [{"id": "004010006", "reason": "From Bounded Exploration also see Trusted Policy Oversight as bounded exploration may be a useful component of that."}]}, {"id": "00200010203", "title": "Multiobjective Reinforcement Learning", "description": "Multiobjective reinforcement learning is a generalization of reinforcement learning extended to multiple simultaneous feedback signals (<a href='http://ieeexplore.ieee.org/document/6918520/' target='_blank'>Liu et al. 2015</a>), and this can include as one of its subobjectives minimizing riskiness.", "mini_description": "A generalization of reinforcement learning extended to multiple simultaneous feedback signals, potentially include minimizing risk as one of its subobjectives", "links": [{"id": "00200010102"}, {"id": "0020109"}]}, {"id": "00200010204", "title": "Human Oversight", "description": "One can have agent determine which exploratory actions are risky, or are ambiguously risky, and query a human about them, making known-safe decisions until the human responds (<a href='http://arxiv.org/abs/1606.06565' target='_blank'>Amodei et al. 2016</a>).", "mini_description": "The agent determines which exploratory actions are risky or ambiguous and queries a human about them", "links": [{"id": "00401", "reason": "From Human Oversight also see Oversight for scalable methods designed specifically for control."}]}, {"id": "00200010205", "title": "Buried Honeypot Avoidance", "description": "One technique for learning safe exploration involves allowing monitored bugs and vulnerabilities to exist (in an evaluation environment) that can be exploited to achieve a higher reward (or other optimization measure), and selecting algorithms or agents that generalize to not exploiting such unintended and unconventional avenues (<a href='https://intelligence.org/files/AlignmentMachineLearning.pdf' target='_blank'>Taylor et al. 2016</a>), at least for some classes of them.", "mini_description": "Introduce monitored vulnerabilities that lead to high reward when exploited and select algorithms that generalize to avoiding unintended exploitation", "links": [{"id": "00302", "reason": "From Buried Honeypot Avoidance also see Tripwires which are similarly structured but are meant to be used later in the agent life cycle."}]}]}], "links": [{"id": "004010006", "reason": "From Safe Exploration also see Trusted Policy Oversight as that can enable safe exploration given blessed core control."}, {"id": "002020204", "reason": "From Safe Exploration also see Distance from User Demonstration as the state space surrounding those is typically a safe space to explore."}, {"id": "0040100", "reason": "From Safe Exploration also see Scalable Oversight as the oversight can include clarifications of which areas are safe to explore."}]}, {"id": "002000103", "title": "Computational Humility", "description": "Algorithmic techniques enabling an agent to humble itself (<a href='https://people.eecs.berkeley.edu/~russell/papers/uai14-oupomdp.pdf' target='_blank'>Srivastava et al. 2914</a>) and moderate its views of its own importance can be termed computational humility.", "mini_description": "Algorithmic techniques enabling an agent to behave more humbly and moderate the modeled weights on its own utility", "breakdowns": [{"id": "0020001030", "sub_nodes": [{"id": "00200010300", "title": "Generalized Fallibility Awareness", "description": "One central type of deference for advanced agents is to know that it may be that none of its hypotheses model the world well enough, and additionally biasing to consider its operators are less flawed than it is (<a href='https://intelligence.org/files/AlignmentMachineLearning.pdf' target='_blank'>Taylor et al. 2016</a>), so letting them shut it down or modify it if it seems like that's what they want to do. An agent can try to model all of the ways it's flawed, but that may require that those models not be discounted by their own mechanism (<a href='https://intelligence.org/files/AlignmentMachineLearning.pdf' target='_blank'>Taylor et al. 2016</a>), an open issue.", "mini_description": "Having an agent know that none of its hypotheses might model the world well enough, biasing to consider its operators are less flawed than it is", "links": [{"id": "00400", "reason": "From Generalized Fallibility Awareness also see Computational Deference which includes the very related control-centered aspects of corrigibility."}]}, {"id": "00200010301", "title": "Resource Value Uncertainty", "description": "Maintaining dynamic bayesian uncertainty regarding the importance or value of unexplored and underexplored or uncharacterized entities, relationships, and dynamics can significantly effect humble treatment of others (<a href='https://people.eecs.berkeley.edu/~russell/papers/uai14-oupomdp.pdf' target='_blank'>Srivastava et al. 2914</a>, <a href='https://people.eecs.berkeley.edu/~russell/research/future/mallah-aamas14-future.pdf' target='_blank'>Mallah 2014</a>). Reasonable multilevel aggregation would be needed to deal with constructs of varying temporal, physical, and complexity scales (<a href='https://uk.sagepub.com/en-gb/eur/multilevel-analysis/book234191' target='_blank'>Snijders and Bosker 2011</a>, Mallah 2017).", "mini_description": "Maintaining a model of uncertainty about the importance of underexplored entities, relationships, and dynamics", "links": [{"id": "0020001000202"}, {"id": "002030202"}, {"id": "0020200010101"}]}, {"id": "00200010302", "title": "Temporal Discount Rates", "description": "Resource acquisition strategies can be tempered by the effects of applying a large temporal discount rate on the expected reward (<a href='http://futureoflife.org/data/documents/research_priorities.pdf' target='_blank'>Russell et al. 2015</a>).", "mini_description": "Using the effects of large temporal discount rates on resource acquisition strategies to temper acquisitiveness"}]}], "links": [{"id": "00200"}, {"id": "002030004", "reason": "From Computational Humility also see Perceptual Distance Agnostic World Models which can aid humble knowledge representation."}, {"id": "00400"}]}]}]}, {"id": "0020002", "title": "Averting Paranoia", "description": "Another instrumental pressure is to constantly scan or prepare for possible ill-intent or possible information leakage. While security precautions and vigilence are important, an arbitrarily high amount of resources can be spent on threat detection and avoidance, while that should typically be tempered to be sufficient to prevent and deal with realistic threats (<a href='http://www.csee.umbc.edu/courses/graduate/CMSC671/fall12/resources/colby_71.pdf' target='_blank'>Colby 2013</a>).", "mini_description": "Making intelligent systems such that they do not spend an inordinate amount of resources determining the trustworthiness or ulterior motives of their operators", "links": [{"id": "00400", "reason": "From Averting Paranoia also see Computational Deference on techniques for having an agent trust its operators."}, {"id": "002000101", "reason": "From Averting Paranoia also see Mild Optimization for more on optimizing objectives and subobjectives to an appropriate amount but no more."}, {"id": "00304", "reason": "From Averting Paranoia also see Handling Improper External Behavior against which this needs to be balanced."}]}]}], "links": [{"id": "0000001"}, {"id": "00202000200", "reason": "From Averting Instrumental Incentives also see Operator Value Learning where systems can avert instrumental incentives by maintaining uncertainty over the truly desired goal."}, {"id": "004000100", "reason": "From Averting Instrumental Incentives also see Switchable Objective Functions whereby faulty interpretations of primary objective can be safely corrected without the default instrumental pressure to prevent such a change."}, {"id": "00200010102"}, {"id": "002000103"}, {"id": "004"}]}, {"id": "00201", "title": "Avoiding Reward Hacking", "description": "A very intelligent agent built to optimize just its observations rather than some function in the actual environment would likely not align with human interests (<a href='https://global.oup.com/academic/product/superintelligence-9780199678112' target='_blank'>Bostrom 2014</a>). It may very well cause unanticipated and potentially harmful behavior by gaming its reward function, deluding itself whether purposefully or inadvertantly (<a href='http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.26.6187&rep=rep1&type=pdf' target='_blank'>Thompson 1997</a>, <a href='https://people.duke.edu/~ng46/topics/evolved-radio.pdf' target='_blank'>Bird and Layzell 2002</a>). Determining how one can reliably prevent an agent from gaming its reward function like this is an open area of research, thus far with a number of promising ideas for mitigation (<a href='http://www.danieldewey.net/learning-what-to-value.pdf' target='_blank'>Dewey 2011</a>, <a href='http://dx.doi.org/10.1007/978-3-642-22887-2_1' target='_blank'>Orseau and Ring 2011</a>, <a href='http://arxiv.org/abs/1606.06565' target='_blank'>Amodei et al. 2016</a>).", "mini_description": "Preventing an agent from gaming its reward function", "breakdowns": [{"id": "002010", "sub_nodes": [{"id": "0020100", "title": "Pursuing Environmental Goals", "description": "A key question in AI safety is how one might create systems that robustly pursue goals defined not in terms of their sensory data of their environment, but in terms of the state of the actual environment (<a href='https://intelligence.org/files/AlignmentMachineLearning.pdf' target='_blank'>Taylor et al. 2016</a>). Behaviors of self-delusion, addiction, and paying attention to unimportant fictions where it seems like reward can be generated are all termed wireheading (<a href='http://people.idsia.ch/~ring/AGI-2011/Paper-B.pdf' target='_blank'>Ring and Orseau 2011</a>). It has been a noted issue in counterfactual learning (<a href='http://leon.bottou.org/papers/tr-bottou-2012' target='_blank'>Bottou et al. 2012</a>, <a href='http://dl.acm.org/citation.cfm?id=2789272.2886805' target='_blank'>Swaminathan and Joachims 2015</a>), in contextual bandits (<a href='http://jmlr.org/proceedings/papers/v32/agarwalb14.pdf' target='_blank'>Agarwal et al. 2014</a>), and will very likely become more common as environments grow in complexity (<a href='http://arxiv.org/abs/1606.06565' target='_blank'>Amodei et al. 2016</a>).", "mini_description": "Robustly pursuing goals defined per the state of the actual environment rather than defined directly in terms of sensory data of the environment", "breakdowns": [{"id": "00201000", "sub_nodes": [{"id": "002010000", "title": "Environmental Goals", "description": "Environmental goals refer to the structuring of a reward system such that an agent that would fool its sensors into registering otherwise-high-reward data would not actually receive a high reward (<a href='http://www.danieldewey.net/learning-what-to-value.pdf' target='_blank'>Dewey 2011</a>, <a href='https://pdfs.semanticscholar.org/6b2b/f1efaa66c77677070f1c52701f0f7f2a3e15.pdf' target='_blank'>Hibbard 2012a</a>, <a href='https://intelligence.org/files/AlignmentMachineLearning.pdf' target='_blank'>Taylor et al. 2016</a>, <a href='http://www.springer.com/cda/content/document/cda_downloaddocument/9783319416489-c2.pdf' target='_blank'>Everitt and Hutter 2016</a>, <a href='http://www.danieldewey.net/reward-engineering-principle.pdf' target='_blank'>Dewey 2014</a>).", "mini_description": "Structuring a reward system such that fooling its sensors into providing otherwise high-reward data would not receive a high reward"}, {"id": "002010001", "title": "Predicting Future Observations", "description": "One way to avoid self-delusion is to abductively generate, maintain, and test multiple hypotheses of the causes of received sensory input in order to distinguish illusory states from environmentally true and valid states, and update priors regarding past sensory data (<a href='https://intelligence.org/files/AlignmentMachineLearning.pdf' target='_blank'>Taylor et al. 2016</a>).", "mini_description": "Generate and test multiple hypotheses of the causes of received sensory input in order to explore and disambiguate illusory states from environmentally true and valid states"}, {"id": "002010002", "title": "Model Lookahead", "description": "Model lookahead can also be used to base reward on anticipated future states rather than the current state, to penalize and discourage the agent from modifying its reward function (<a href='http://arxiv.org/abs/1606.06565' target='_blank'>Amodei et al. 2016</a>, <a href='http://www.tomeveritt.se/papers/AGI16-sm.pdf' target='_blank'>Everitt et al. 2016</a>, <a href='https://pdfs.semanticscholar.org/6b2b/f1efaa66c77677070f1c52701f0f7f2a3e15.pdf' target='_blank'>Hibbard 2012a</a>).", "mini_description": "Base reward on anticipated future states rather than the current state, to penalize reward function modification"}, {"id": "002010003", "title": "History Analysis", "description": "An agent's historical trajectory can be analyzed for incongruences, discontinuities, and hints of efforts to self-delude, and if delusion is detected this can trigger an appropriate backtracking (<a href='https://intelligence.org/files/AlignmentMachineLearning.pdf' target='_blank'>Taylor et al. 2016</a>).", "mini_description": "Analyze historical trajectory for incongruences, discontinuities, and hints of efforts to self-delude"}, {"id": "002010004", "title": "Detecting Channel Switching", "description": "The entry of an agent into wireheading may be indicated by percept disjointedness. Delusional hallucination might therefore be avoided using \"slow features\" to detect incongruences or discontinuities that are indicative of a percept-channel switching event (<a href='http://www.ncbi.nlm.nih.gov/pubmed/11936959' target='_blank'>Wiskott and Sejnowski 2002</a>). This involves the application of anomaly detection to normally slow-changing features to detect such segmentation (<a href='https://intelligence.org/files/AlignmentMachineLearning.pdf' target='_blank'>Taylor et al. 2016</a>). Note that this issue is very different from either the hallucinations of \"inceptionism\" (<a href='https://research.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html' target='_blank'>Mordvintsev et al. 2015</a>) used for transparency into deep networks or the sorts of imaginings that generative networks (<a href='https://arxiv.org/pdf/1612.03242v1' target='_blank'>Zhang et al. 2016</a>) do to produce exemplars.", "mini_description": "Apply anomaly detection to normally slow-changing features to detect incongruencies"}]}], "links": [{"id": "002030004"}]}, {"id": "0020101", "title": "Counterexample Resistance", "description": "One might design a system to build resistance (<a href='http://arxiv.org/abs/1606.06565' target='_blank'>Amodei et al. 2016</a>) to natural and unnatural counterexamples, confounding patterns, and adversarial tricks by leveraging adversarial training (<a href='https://pdfs.semanticscholar.org/bee0/44c8e8903fb67523c1f8c105ab4718600cdb.pdf' target='_blank'>Goodfellow et al. 2015</a>, <a href='http://jmlr.org/proceedings/papers/v37/blundell15.pdf' target='_blank'>Blundell et al. 2015</a>, <a href='http://papers.nips.cc/paper/6088-adversarial-multiclass-classification-a-risk-minimization-perspective.pdf' target='_blank'>Fathony et al. 2016</a>). This discourages the agent from purturbing its data to achieve alternate interpretations.", "mini_description": "Reducing susceptibility to adversarial tricks via adversarial training", "links": [{"id": "0030400", "reason": "From Counterexample Resistance also see Adversarial ML as this can also be framed as a security issue when external parties are providing the adversarial testing or production data."}]}, {"id": "0020102", "title": "Adversarial Reward Functions", "description": "In order to enable a more active, critical, and responsive reward function, one can have the reward function itself be an agent that tries to find flawed value determinations, similar to how generative adversarial networks work (<a href='http://arxiv.org/abs/1606.06565' target='_blank'>Amodei et al. 2016</a>, <a href='http://papers.nips.cc/paper/5423-generative-adversarial-nets' target='_blank'>Goodfellow et al. 2014</a>).", "mini_description": "Having the reward function itself be an agent that tries to find flawed value determinations"}, {"id": "0020103", "title": "Generalizing Avoiding Reward Hacking", "description": "Independent of specific techniques, formalization and study of reward hacking by analyzing the distance and shape of jumps in the feature space would help to generalize its detection and avoidance (<a href='https://intelligence.org/files/AlignmentMachineLearning.pdf' target='_blank'>Taylor et al. 2016</a>).", "mini_description": "Formalizing detection and prevention of reward hacking, e.g. by analyzing the distance and shape of jumps in the feature space"}, {"id": "0020104", "title": "Adversarial Blinding", "description": "Adversarial techniques can be used to blind a model to certain variables, useful for masking how an agent's reward is generated, and enabling cross-validation for agents (<a href='http://arxiv.org/abs/1606.06565' target='_blank'>Amodei et al. 2016</a>, <a href='http://jmlr.org/papers/volume17/15-239/15-239.pdf' target='_blank'>Ganin et al. 2016</a>).", "mini_description": "Using adversarial learning techniques to blind a model to the effects of certain variables, useful for masking how an agent's reward is generated to make it difficult to hack"}, {"id": "0020105", "title": "Variable Indifference", "description": "By maintaining the independence of variables actually valued, the system can shape the direction of the optimization pressures to areas one cares less about maintaining to moderate values (<a href='http://arxiv.org/abs/1606.06565' target='_blank'>Amodei et al. 2016</a>).", "mini_description": "Shaping the direction of optimization pressures to allow optimization of certain variables in the environment without trying to optimize others"}, {"id": "0020106", "title": "Careful Engineering", "description": "To reduce the risk of reward function gaming, one can utilize extensive verification, testing, and security to create a safer base or core agent, on top of which more specific agents can be configured (<a href='http://arxiv.org/abs/1606.06565' target='_blank'>Amodei et al. 2016</a>).", "mini_description": "Utilizing extensive verification, testing, and security to create a safer base or core agent on top of which more specific agents can be configured", "links": [{"id": "0010000"}, {"id": "00303", "reason": "From Careful Engineering also see Containment as one can be further careful about isolating the agent from its reward signal using that."}, {"id": "0030000"}]}, {"id": "0020107", "title": "Reward Capping", "description": "In order to prevent extremely low-probability high-reward choices, carefully normalizing rewards or utilities at a level appropriate to the agent (<a href='http://arxiv.org/abs/1606.06565' target='_blank'>Amodei et al. 2016</a>) is in order. Concomitantly, model uncertainty should dominate most expert calculations that involve small probabilities (<a href='http://www.amirrorclear.net/files/probing-the-improbable.pdf' target='_blank'>Ord et al. 2010</a>).", "mini_description": "Preventing extremely low-probability high-reward choices"}, {"id": "0020108", "title": "Reward Pretraining", "description": "Another way to discourage gaming of the reward function is to train the reward function in a supervised manner offline, ahead of online use (<a href='http://arxiv.org/abs/1606.06565' target='_blank'>Amodei et al. 2016</a>, <a href='http://jmlr.org/proceedings/papers/v48/finn16.pdf' target='_blank'>Finn et al. 2016a</a>). As a known function, it can be analyzed analytically, statically, or in unit tests, and it will by definition be robust against online reward function corruption.", "mini_description": "Training the reward function in a supervised manner offline, ahead of online use"}, {"id": "0020109", "title": "Multiple Rewards", "description": "Another reward function robustness method is to use an aggregate of different variants or proxies of the same basic objective (<a href='http://arxiv.org/abs/1606.06565' target='_blank'>Amodei et al. 2016</a>, <a href='http://dx.doi.org/10.1007/978-1-4614-6940-7_15' target='_blank'>Deb 2014</a>).", "mini_description": "Using an aggregate of different variants or proxies of the same basic objective", "links": [{"id": "00200010102", "reason": "From Multiple Rewards also see Multiobjective Optimization as that uses similar techniques but can also include independent subobjectives."}, {"id": "002030005", "reason": "From Multiple Rewards also see Knowledge Representation Ensembles as that provides variants of the objective function for aggregation via varied world models."}, {"id": "00200010203", "reason": "From Multiple Rewards also see Multiobjective Reinforcement Learning for treatment in an RL context."}]}]}], "links": [{"id": "0000001"}, {"id": "0000102"}, {"id": "00302", "reason": "From Avoiding Reward Hacking also see Tripwires which may be used to help catch these behaviors."}, {"id": "00203000102", "reason": "From Avoiding Reward Hacking also see Ontology Update Thresholds as the dynamic degradatory repurposing of properties as in Goodhart's Law surfaces in both."}]}, {"id": "00202", "title": "Technical Value Alignment", "description": "Researchers expect that highly intelligent agents would be able to pursue arbitrary end-goals that are independent of moral values, since intelligence and values are hypothesized to be orthogonal (<a href='http://www.nickbostrom.com/superintelligentwill.pdf' target='_blank'>Bostrom 2012</a>). Values are ethical norms, constraints, and moral weights one places on properties and relationships in the world. Ensuring that a system conceptualizes and uses its intelligence in a<b> beneficial</b> manner requires more than accuracy (<a href='https://intelligence.org/files/ValueLearningProblem.pdf' target='_blank'>Soares 2016</a>). It has been argued that a single fixed pithy solution to machine ethics may be rather hole-ridden, brittle, and deficient in fairness (<a href='http://www.milesbrundage.com/uploads/2/1/6/8/21681226/limitations_and_risks_of_machine_ethics.pdf' target='_blank'>Brundage 2014</a>). Even Asimov's deontological scenarios illustrate such loopholes, thus the need much more comprehensive paradigms (<a href='http://homes.cs.washington.edu/~weld/papers/first-law-aaai94.pdf' target='_blank'>Weld and Etzioni 1994</a>). As systems become more capable, more epistemically-difficult methods of value loading could become viable, suggesting that research on such methods could be useful (<a href='https://global.oup.com/academic/product/superintelligence-9780199678112' target='_blank'>Bostrom 2014</a>). A portfolio approach is warranted to support a variety of methods for specifying goals indirectly and semi-indirectly.", "mini_description": "Ensuring that an agent conforms to the implicit or explicit ethical values of humanity or its creators or operators", "breakdowns": [{"id": "002020", "sub_nodes": [{"id": "0020200", "title": "Ethics Mechanisms", "description": "There are multiple methods to represent such values and to imbue them into artificial agents. Different orders of complexity of AI systems will have different capacities and mechanisms to represent values (<a href='https://jsteinhardt.wordpress.com/2015/06/24/long-term-and-short-term-challenges-to-ensuring-the-safety-of-ai-systems/' target='_blank'>Steinhardt 2015</a>, Mallah 2017). Highly capable and general AIs will require more sophisticated treatments of values.", "mini_description": "Methods by which to imbue values into a system", "breakdowns": [{"id": "00202000", "sub_nodes": [{"id": "002020000", "title": "Grounded Ethical Evolution", "description": "An approach to growing ethical systems is to use mechanism design, setting up an environment with multiple agents, and incentives and punishments, to direct norms and preferences of the agents toward cooperation (<a href='http://repository.tudelft.nl/assets/uuid:4c7d3b49-06fc-400c-923e-3903b8d230fe/busoniu2008.pdf' target='_blank'>Busoniu et al. 2008</a>, <a href='http://futureoflife.org/2016/09/26/training-artificial-intelligence-compromise/' target='_blank'>Conn 2016a</a>, <a href='http://drops.dagstuhl.de/opus/volltexte/2016/6187/pdf/LIPIcs-CONCUR-2016-2.pdf' target='_blank'>Rossi 2016</a>). Unfortunately, in the limit, such evolution is by no means guaranteed to reach a desirable or context-insensitive state (Hall 2011). Evolutionary development in general, however, can lead to unexpected results that are undesirably specific to incidental environmental factors (<a href='https://people.duke.edu/~ng46/topics/evolved-radio.pdf' target='_blank'>Bird and Layzell 2002</a>).", "mini_description": "The development of effective ethical systems within multiagent systems", "breakdowns": [{"id": "0020200000", "sub_nodes": [{"id": "00202000000", "title": "Moral Trade", "description": "Within such systems, one dynamic of note is that moral agents that disagree about morals will tend to trade to increase their moral impact (<a href='http://amirrorclear.net/files/moral-trade.pdf' target='_blank'>Ord 2015</a>). Another recent concept of note is ethical fusion, in which aggregation in collective and collaborative decision making occurs not via linear combination but effectively through negotiating synergies (<a href='http://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/download/12457/12204' target='_blank'>Greene et al. 2016</a>, <a href='http://drops.dagstuhl.de/opus/volltexte/2016/6187/pdf/LIPIcs-CONCUR-2016-2.pdf' target='_blank'>Rossi 2016</a>).", "mini_description": "Modeling and leveraging the trades that occur between agents that disagree about morals but trade in order to each increase their moral impact"}]}], "links": [{"id": "0000000", "reason": "From Grounded Ethical Evolution also see Logical Uncertainty as both economic and game theoretic analyses and simulations can be made more realistic when including logical uncertainty."}, {"id": "0020200010102", "reason": "From Grounded Ethical Evolution also see Game Theoretic Framing regarding the ability of agents to model each other aid at the arrival to game theoretic and similar equilibria."}, {"id": "000010001", "reason": "From Grounded Ethical Evolution also see Open Source Game Theory regarding similarly framed dynamics but with agents' source code available to each other."}, {"id": "002030002", "reason": "From Grounded Ethical Evolution also see Correlation of Dynamics as the correlations between an agent and its environment, as well as with other agents, largely shape the ethics that evolve."}, {"id": "00306", "reason": "From Grounded Ethical Evolution also see Norm Denial of Service as an example issue that can arise in such systems when realistic timing and thus logical uncertainty are ill accounted for."}]}, {"id": "002020001", "title": "Value Specification", "description": "Broadly, value specification is the direct or somewhat indirect specifying of values a system should hold and act on. The intentions of operators are, however, fuzzy, not well-specified, and sometimes containing contradictions (<a href='http://dx.doi.org/10.1007/978-3-642-22887-2_48' target='_blank'>Yudkowsky 2011</a>). Some prominent AI researchers expect moral philosophy to become an increasingly important commercial industry (<a href='http://time.com/4026723/stuart-russell-will-ai-overtake-humans/' target='_blank'>Russell 2016</a>). With advanced agents, it's not sufficient to develop a system intelligent enough to figure out the intended goals, though; the system must also somehow be deliberately constructed to pursue them (<a href='https://global.oup.com/academic/product/superintelligence-9780199678112' target='_blank'>Bostrom 2014</a>, <a href='http://intelligence.org/files/TechnicalAgenda.pdf' target='_blank'>Soares and Fallenstein 2014a</a>).", "mini_description": "Directly or slightly indirectly specifying values a system should hold and act on", "breakdowns": [{"id": "0020200010", "sub_nodes": [{"id": "00202000100", "title": "Classical Computational Ethics", "description": "Computational models of ethical reasoning may shed light on questions of computational expense and the viability of reliable ethical reasoning methods (<a href='http://futureoflife.org/data/documents/research_priorities.pdf' target='_blank'>Russell et al. 2015</a>, <a href='https://global.oup.com/academic/product/moral-machines-9780195374049' target='_blank'>Wallach and Allen 2008</a>, <a href='http://www.cambridge.org/catalogue/catalogue.asp?isbn=9780521112352' target='_blank'>Anderson and Anderson 2011</a>, <a href='http://link.springer.com/content/pdf/10.1007%2Fs13347-011-0043-6.pdf' target='_blank'>Sullins 2011</a>, <a href='http://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/download/12457/12204' target='_blank'>Greene et al. 2016</a>, <a href='http://drops.dagstuhl.de/opus/volltexte/2016/6187/pdf/LIPIcs-CONCUR-2016-2.pdf' target='_blank'>Rossi 2016</a>, <a href='https://intelligence.org/files/csrbai/pref-eth1.pdf' target='_blank'>Rossi 2016a</a>, <a href='http://www.peterasaro.org/writing/Asaro%20IRIE.pdf' target='_blank'>Asaro 2006</a>). In scenarios where an agent operates in a human-level world, where humans can relate to states and actions, whether physical or virtual, pluggable ethical profiles may be quite appropriate (<a href='http://time.com/4026723/stuart-russell-will-ai-overtake-humans/' target='_blank'>Russell 2016</a>). Machine learning of ethical features and determinations from human-labelled data may be a plausible method of bootstrapping moral features that are relevant for human-like experiences (<a href='https://users.cs.duke.edu/~conitzer/moralAAAI17.pdf' target='_blank'>Conitzer et al. 2017</a>, <a href='http://www.aaai.org/ocs/index.php/AAAI/AAAI14/paper/download/8308/8428' target='_blank'>Anderson and Anderson 2014</a>).", "mini_description": "Either explicit computational models or directly learned models of ethical reasoning"}, {"id": "00202000101", "title": "Value Structuring", "description": "How can one structurally, algorithmically ensure that the core values of advanced learning agents meet broad beneficence constraints? Arguments have been made that they should be heavily informed by analyzing how humans structure their value systems (<a href='https://joshgreene.squarespace.com/s/beyond-point-and-shoot-morality-a4h2.pdf' target='_blank'>Greene 2014</a>, <a href='http://arxiv.org/abs/1607.08289' target='_blank'>Sarma and Hay 2016</a>, <a href='https://mindmodeling.org/cogsci2013/papers/0141/index.html' target='_blank'>Jara-Ettinger et al. 2013</a>).", "mini_description": "Architectural approaches to encouraging beneficence", "breakdowns": [{"id": "002020001010", "sub_nodes": [{"id": "0020200010100", "title": "Values Geometry", "description": "The way that value-related concepts, relationships, and skills are represented may heavily influence the dynamics of their use. What the basic data structures, or even theoretical ontological structures, of ethical and moral values are is not yet a settled question though. Some schemes model values as worth attached to ontological properties and relationships, providing a range for preferences (<a href='http://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/download/12457/12204' target='_blank'>Greene et al. 2016</a>, <a href='http://drops.dagstuhl.de/opus/volltexte/2016/6187/pdf/LIPIcs-CONCUR-2016-2.pdf' target='_blank'>Rossi 2016</a>, Mallah 2017). There have been computational explorations of how human values should be conceived of and modeled (Sotala 2016, <a href='https://intelligence.org/files/csrbai/pref-eth1.pdf' target='_blank'>Rossi 2016a</a>). There may also be cartesian and monte carlo tree search graph search algorithm constructs of values which might be well-suited to efficient ethical ensembles (<a href='www.aaai.org/ocs/index.php/SOCS/SOCS15/paper/view/11273/10648' target='_blank'>Bnaya et al. 2015</a>, Mallah 2017).", "mini_description": "Geometrical, graphical, and ontological analyses of how values can be structured", "links": [{"id": "0020302", "reason": "From Values Geometry also see Concept Geometry as values consist at least in part of particular kinds of concepts."}, {"id": "0020200020102"}]}, {"id": "0020200010101", "title": "Action and Outcome Evaluations", "description": "Combining retrospective and prospective perspectives would seem a good desideratum for ethical contemplation. One such joint representation and evaluation mechanism for developing and using values is to assign value to choices by a combination of intrinsic value, based on past experience, and from an internally represented causal model of the world (<a href='http://cushmanlab.fas.harvard.edu/docs/cushman_2013.pdf' target='_blank'>Cushman 2013</a>). There can also be combinations of hard and soft constraints, conflict resolution, and consistency mechanisms over multiple layers of values, ethics, morals, and preferences (<a href='http://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/download/12457/12204' target='_blank'>Greene et al. 2016</a>, <a href='http://drops.dagstuhl.de/opus/volltexte/2016/6187/pdf/LIPIcs-CONCUR-2016-2.pdf' target='_blank'>Rossi 2016</a>).", "mini_description": "Retrospective and prospective mechanisms for developing and using values", "links": [{"id": "00200010003"}, {"id": "00200010301"}, {"id": "00200010001", "reason": "From Action and Outcome Evaluations also see Follow-on Analysis which analyzes and prunes deleterious downstream actions."}, {"id": "002030104", "reason": "From Action and Outcome Evaluations also see Lengthening Horizons which seeks ways of increasing outcome prediction horizons."}]}, {"id": "0020200010102", "title": "Game Theoretic Framing", "description": "Putting common notions of morality into a game theoretic framework in order to support propagation of prosocial values and equilibria is a promising, if early, framing (<a href='http://dx.doi.org/10.1007/978-3-540-92185-1_75' target='_blank'>Letchford et al. 2008</a>, <a href='https://users.cs.duke.edu/~conitzer/moralAAAI17.pdf' target='_blank'>Conitzer et al. 2017</a>). Evolving ethical systems, however, does not yet provide many guarantees as to success or applicability (Hall 2011), and so research into the differential robustness of such solutions when the mechanism is modified may be beneficial.", "mini_description": "Enhancing game theoretic constructs to support prosocial values", "links": [{"id": "0000000", "reason": "From Game Theoretic Framing also see Logical Uncertainty as more realistic game theoretic models will account for logical uncertainty."}, {"id": "002020000"}]}, {"id": "0020200010103", "title": "Unified Ethics Spaces", "description": "Having a harmonized space in which different major or important ethical systems can be represented, e.g. vector or tensor spaces, or graph traversal spaces, might allow for efficient set theoretic operations across them, facilitating ensemble and multiobjective approaches, but this is rather underexplored as yet (<a href='http://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/download/12457/12204' target='_blank'>Greene et al. 2016</a>, Mallah 2017).", "mini_description": "Techniques and constructs that enable the representation of multiple kinds of ethical theories to be modelled in the same conceptual space simultaneously"}]}], "links": [{"id": "00204", "reason": "From Value Structuring also see Psychological Analogues as these begin to point toward parallels between human psychology and analogues in machine learning."}, {"id": "0020302"}, {"id": "0050003"}, {"id": "0050403010200"}]}, {"id": "00202000102", "title": "Capability Amplification", "description": "Capability amplification, starting with an aligned policy and using it to produce a more effectual policy that is still aligned, offers an interesting paradigm for cyclic active learning (<a href='https://medium.com/ai-control/policy-amplification-6a70cbee4f34' target='_blank'>Christiano 2016a</a>).", "mini_description": "Starting with a value-aligned policy and using it to produce a more effectual policy that is still aligned"}]}], "links": [{"id": "002030202"}, {"id": "0020303"}, {"id": "002020202"}, {"id": "005040300"}, {"id": "0020200020101"}]}, {"id": "002020002", "title": "Value Learning", "description": "It has been argued that it's quite plausible that researchers and developers will want to make agents that act autonomously and powerfully across many domains (<a href='http://futureoflife.org/data/documents/research_priorities.pdf' target='_blank'>Russell et al. 2015</a>). Specifying one's preferences somewhat explicitly in broad or general domains in the style of near-future narrow-domain machine ethics may not be practical, making aligning the values of powerful AI systems with one's own values and preferences difficult (<a href='https://intelligence.org/files/ValueLearningProblem.pdf' target='_blank'>Soares 2016</a>, <a href='http://intelligence.org/files/TechnicalAgenda.pdf' target='_blank'>Soares and Fallenstein 2014a</a>). Concretely writing out the full intentions of the operators in a machine-readable format is implausible if not impossible, even for simple tasks (<a href='http://intelligence.org/files/TechnicalAgenda.pdf' target='_blank'>Soares and Fallenstein 2014a</a>). An intelligent agent must be designed to learn and act according to the preferences of its operators, likely with multiple layers of indirection (<a href='https://intelligence.org/files/AlignmentMachineLearning.pdf' target='_blank'>Taylor et al. 2016</a>).", "mini_description": "Learning values directly or indirectly from humans", "breakdowns": [{"id": "0020200020", "sub_nodes": [{"id": "00202000200", "title": "Operator Value Learning", "description": "To learn operators' values effectively, it may be necessary to maintain and refine a distribution of possibilities of the actual meaning of those goals or values specified or hinted at (<a href='https://intelligence.org/files/AlignmentMachineLearning.pdf' target='_blank'>Taylor et al. 2016</a>). Such a system that believes that the operators (and only the operators) possess knowledge of the right objective function might be very careful in how it deals with the operators, and this caution could therefore avert potentially harmful default incentives (<a href='https://people.eecs.berkeley.edu/~dhm/papers/CIRL_NIPS_16.pdf' target='_blank'>Hadfield-Menell et al. 2016</a>, <a href='https://intelligence.org/files/ValueLearningProblem.pdf' target='_blank'>Soares 2016</a>, <a href='https://intelligence.org/files/LoudnessPriors.pdf' target='_blank'>Fallenstein and Stiennon 2014</a>). Such systems may avert instrumental incentives by virtue of the systems uncertainty about which goal it is supposed to optimize. A major part of this process is the iterative extraction of one or more humans' volition, of which there are a few methods (<a href='https://ordinaryideas.wordpress.com/2014/08/27/specifying-enlightened-judgment-precisely-reprise/' target='_blank'>Christiano 2014a</a>, <a href='https://intelligence.org/files/ValueLearningProblem.pdf' target='_blank'>Soares 2016</a>). Before particular architectures are selected, considering broad paradigms like reward engineering (<a href='https://medium.com/ai-control/reward-engineering-f8b5de40d075' target='_blank'>Christiano 2015b</a>) can help inform an approach. Very advanced AIs should in theory be able to take the preferences, values, and volition from one or many humans and extrapolate forward to the additional entailments and moral progress that additional knowledge and additional time to think would produce (<a href='https://intelligence.org/files/CEV.pdf' target='_blank'>Yudkowsky 2004</a>, <a href='https://intelligence.org/files/CEV-MachineEthics.pdf' target='_blank'>Tarleton 2010</a>).", "mini_description": "Maintaining and refining a distribution of possibilities of what the goals or values specified to it actually mean", "links": [{"id": "00200"}, {"id": "0020001000202"}, {"id": "00202000201", "reason": "From Operator Value Learning also see Value Elicitation as that addresses extraction methods more directly."}, {"id": "002020202"}]}, {"id": "00202000201", "title": "Value Elicitation", "description": "Value elicitation involves finding sources of values and sufficiently interrogating them to extract the values.", "mini_description": "Mechanisms for implicitly or explicitly querying approved sources of values", "breakdowns": [{"id": "002020002010", "sub_nodes": [{"id": "0020200020100", "title": "Value Sourcing", "description": "Good people are quite varied in their morality and values (<a href='http://www.penguinrandomhouse.com/books/73535/the-righteous-mind-by-jonathan-haidt/9780307455772/' target='_blank'>Haidt 2012</a>). Societal values also change over time. In order to attempt to start with reasonable recall over values, the number of types of artifacts, modalities, stakeholders, sapient species, and organizations providing value laden information might be maximized. Techniques for analyzing consistencies and synergies can then be used to improve precision (<a href='http://drops.dagstuhl.de/opus/volltexte/2016/6187/pdf/LIPIcs-CONCUR-2016-2.pdf' target='_blank'>Rossi 2016</a>). There are many philosophical concerns surrounding what sort of goals are ethical when aligning a superintelligent system, but a solution to the value learning problem will be a practical necessity regardless of which philosophical view is the correct one. The methodology used for such sourcing requires fairness and objectivity (<a href='https://projects.iq.harvard.edu/files/mcl/files/greene-driverless-dilemma-sci16.pdf' target='_blank'>Greene 2016</a>). Having a set of human operators with total control over a superintelligent system could give rise to a gigantic new moral hazard, however, by putting historically unprecedented power into the hands of a small few (<a href='https://global.oup.com/academic/product/superintelligence-9780199678112' target='_blank'>Bostrom 2014</a>).", "mini_description": "Determining the sources and modalities of artifacts from which to attempt values extraction"}, {"id": "0020200020101", "title": "Value Interrogation", "description": "The set of methods for querying or detecting values from sources of values is called value interrogation. Depending on the way one wishes learned values to be structured and how directly those should be learned, mechanisms for querying authorized sources of values can appropriately vary considerably, ranging from asking, to watching, to scanning (<a href='https://www.aaai.org/Papers/AAAI/2002/AAAI02-037.pdf' target='_blank'>Boutilier 2002</a>, <a href='https://intelligence.org/files/ValueLearningProblem.pdf' target='_blank'>Soares 2016</a>, <a href='https://intelligence.org/files/CEV-MachineEthics.pdf' target='_blank'>Tarleton 2010</a>, <a href='https://arxiv.org/pdf/1308.0702v1' target='_blank'>Potapov and Rodionov 2013</a>). Techniques for maximal extraction of alignment-oriented information will also include robust counterfactual elicitation. The field of preference elicitation (<a href='https://infoscience.epfl.ch/record/52659/files/IC_TECH_REPORT_200467.pdf' target='_blank'>Chen and Pu 2004</a>) likely has much to offer that of the rather related value elicitation.", "mini_description": "Methods for querying or detecting values from sources of values", "links": [{"id": "002020001", "reason": "From Value Interrogation also see Value Specification in which values are queried and provided with less indirection."}, {"id": "002020002", "reason": "From Value Interrogation also see Value Learning 's other children, as they include methods for collaborating with or scanning value sources."}, {"id": "0020202", "reason": "From Value Interrogation also see Robust Human Imitation in which values are learned by analyzing operator worldlines."}, {"id": "0000001", "reason": "From Value Interrogation also see Theory of Counterfactuals as hypotheticals and the ranges of valid options are important in characterization of values."}]}, {"id": "0020200020102", "title": "Value Factoring", "description": "When values are identified, whether implicitly or explicitly, these may be either instrumental values, narrow short-term values, or terminal fundamental values (<a href='https://medium.com/ai-control/ambitious-vs-narrow-value-learning-99bd0c59847e' target='_blank'>Christiano 2015</a>, <a href='https://arxiv.org/pdf/1308.0702v1' target='_blank'>Potapov and Rodionov 2013</a>). Correctly learning terminal values indirectly, ideal theoretically for superintelligence, may require unmanageable amounts of resources (<a href='https://medium.com/ai-control/ambitious-vs-narrow-value-learning-99bd0c59847e' target='_blank'>Christiano 2015</a>). If one applies active learning to disambiguating common roots or causes among different narrow values, in essence prompting humans to perform goal factoring (<a href='http://www.wsj.com/articles/SB10001424052702303453004579290510733740616' target='_blank'>Chen 2014</a>), the system and operators in tandem may be able to align and stitch together the network of values iteratively deeper. With an active inverse reinforcement learning setting driving the elicitation (<a href='https://dl.dropboxusercontent.com/u/23843264/Permanent/towards-interactive-inverse-reinforcement-learning.pdf' target='_blank'>Armstrong and Leike 2016</a>), correspondences and disconnects between observed behavior and stated preferences, goals, or values may be able to be accounted for.", "mini_description": "Techniques for figuring out the more fundamental values that each compound value or preference represents", "links": [{"id": "0020200010100", "reason": "From Value Factoring also see Values Geometry for discussion of how compound and terminal values may be structured."}]}]}], "links": [{"id": "00202000200"}]}, {"id": "00202000202", "title": "Collaborative Values Learning", "description": "Learning of instrumental or potentially terminal values can be done in a collaborative manner, whereby multiple agents interrogate each other, perform joint trial-and-error episodes, and preemptively disambiguate concepts or events for each other in order to uncover the reward function of a target subset of the agents (<a href='https://people.eecs.berkeley.edu/~dhm/papers/CIRL_NIPS_16.pdf' target='_blank'>Hadfield-Menell et al. 2016</a>).", "mini_description": "Collaboration among multiple agents with interrogation, joint trial-and-error, disambiguation, and value factoring, in order to uncover the reward function of a subset of the agents", "links": [{"id": "004010007", "reason": "From Collaborative Values Learning also see Cooperative Inverse Reinforcement Learner as cIRL is a leading algorithm for this purpose."}]}]}], "links": [{"id": "0020200020101"}, {"id": "00401"}, {"id": "0050003"}, {"id": "0050403010200"}]}, {"id": "002020003", "title": "Ethical Ensembles", "description": "As human ethics often contains competing or contradicting tenets, and as there are multiple different structures of semiformal moral philosophy, an ensemble of ethical systems, evaluating multiple different blessed ethical systems in tandem and using some aggregate to inform decisions, may be called for (<a href='http://commonsenseatheism.com/wp-content/uploads/2014/03/MacAskill-Normative-Uncertainty.pdf' target='_blank'>MacAskill 2014</a>, <a href='https://www.researchgate.net/profile/Benjamin_Karatzoglou/publication/259011163_Symposium_on_Sustainable_Development_and_a_New_System_of_Societal_Values_Interactions_between_societal_values_and_sustainability_in_the_Greek_tourist_regions/links/0c960529c3fc8873c4000000.pdf#page=41' target='_blank'>Stark 2001</a>, Mallah 2017, <a href='http://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/download/12457/12204' target='_blank'>Greene et al. 2016</a>, <a href='https://intelligence.org/files/IdealAdvisorTheories.pdf' target='_blank'>Muehlhauser and Williamson 2013</a>).", "mini_description": "Evaluating multiple trusted ethical systems in tandem and using some aggregate of them to inform decisions", "links": [{"id": "00200010102"}, {"id": "0050003"}]}, {"id": "002020004", "title": "Structured and Privileged Ethical Biases", "description": "To strike a balance between specified and learned values, one can structure a mechanism of ethical constraints of variable softness. Core values specified upfront would be rather firm, and instrumental or derivative values, preferences, and actions would be built and learned atop them, and evaluated against them (<a href='http://link.springer.com/content/pdf/10.3758%2FBF03206481.pdf' target='_blank'>Tanner and Medin 2004</a>), with incremental layers featuring incrementally softer and more malleable constraints (<a href='http://people.idsia.ch/~steunebrink/Publications/AGI16_growing_recursive_self-improvers.pdf' target='_blank'>Steunebrink et al. 2016</a>, Mallah 2017, <a href='http://onlinelibrary.wiley.com/doi/10.1111/j.1756-8765.2010.01095.x/epdf' target='_blank'>Wallach et al. 2010</a>).", "mini_description": "Specifying firm core values upfront, and using those to evaluate derived instrumental value candidates", "links": [{"id": "00305", "reason": "From Structured and Privileged Ethical Biases also see Privileged Biases as the agent can not only ignore or reject things that go against its core values, but can focus on those as potential threats or tracked as sources of issues."}]}, {"id": "002020005", "title": "Drives and Affect", "description": "Some seek to keep agents benign through artificial feelings and needs, similarly to how animals modulate their prosocial tendencies (<a href='http://www.springer.com/in/book/9789462390263' target='_blank'>Goertzel et al. 2014</a>, <a href='http://www.lorentzcenter.nl/lc/web/2011/464/presentations/Bach.pdf' target='_blank'>Bach 2011</a>, <a href='http://ict.usc.edu/pubs/Machine%20Ethics.pdf' target='_blank'>Dehghani et al. 2011</a>). For approaches that seek maximal biological plausibility, cognitive neuroscience offers significant bearing on the formulation and evaluation of ethics (<a href='https://joshgreene.squarespace.com/s/beyond-point-and-shoot-morality-a4h2.pdf' target='_blank'>Greene 2014</a>).", "mini_description": "The use of urges, drives, affects, and emotions in artificial agents to effect moral behavior", "links": [{"id": "00204", "reason": "From Drives and Affect also see Psychological Analogues for exploration of additional potential similarities between types of intelligences."}, {"id": "0020400"}]}]}]}, {"id": "0020201", "title": "Degree of Value Evolution", "description": "Even if an agent is able to be aligned accurately with some set of societally-acceptable values at a given point in time, for long-lived agents, one must consider, given that values evolve over time, how rigidly to adhere to those original values versus being open to updating those values. Determining whether, and how quickly, a long-lived agent can safely evolve, refine, or redefine the values it's initially imbued with remains an open problem (<a href='http://people.idsia.ch/~steunebrink/Publications/AGI16_growing_recursive_self-improvers.pdf' target='_blank'>Steunebrink et al. 2016</a>, Mallah 2017). A hypothetical agent created hundreds of years ago, if created with strict value faithfulness and not allowed to update values, would exhibit what one today would consider very odd norms, values, and behavior (<a href='http://www.aaai.org/ocs/index.php/WS/AAAIW15/paper/download/10149/10138' target='_blank'>Tegmark 2015</a>).", "mini_description": "Determining whether, how, and how quickly a long-lived agent can safely evolve, refine, or redefine the values it is initially imbued with", "links": [{"id": "0000102"}, {"id": "00203000102", "reason": "From Degree of Value Evolution also see Ontology Update Thresholds since mapping world events to any stale, ungrounded, or nolonger-founded concepts can lead to odd dynamics, and if those concepts are values the effects can be compounded."}, {"id": "002030001"}]}, {"id": "0020202", "title": "Robust Human Imitation", "description": "One way to convey to artificial agents what one would like them to do is to show them in detail. Designing and training machine learning systems to effectively imitate humans who are engaged in complex and difficult tasks is an area of active research (<a href='https://intelligence.org/files/AlignmentMachineLearning.pdf' target='_blank'>Taylor et al. 2016</a>). One important consideration in this approach is the set of tradeoffs between the depth of true objectives one would ideally like to uncover and the ease with which shorter-term goals or instrumental goals can be learned (<a href='https://medium.com/ai-control/ambitious-vs-narrow-value-learning-99bd0c59847e' target='_blank'>Christiano 2015</a>).", "mini_description": "Learning by observing and imitating instructive human behavior", "breakdowns": [{"id": "00202020", "sub_nodes": [{"id": "002020200", "title": "Inverse Reinforcement Learning", "description": "A system can infer the preferences of another rational, or nearly rational, actor by observing its behavior. A prominent family of algorithms for learning and imitating human behavior, as well as narrow or instrumental values, is inverse reinforcement learning (IRL, <a href='https://intelligence.org/files/AlignmentMachineLearning.pdf' target='_blank'>Taylor et al. 2016</a>). It attempts to learn the reward function that a human is approximately optimizing (<a href='https://people.eecs.berkeley.edu/~russell/papers/colt98-uncertainty.pdf' target='_blank'>Russell 1998</a>, <a href='https://people.eecs.berkeley.edu/~russell/papers/ml00-irl.pdf' target='_blank'>Ng and Russell 2000</a>). Very related are apprenticeship learning (<a href='http://ai.stanford.edu/~ang/papers/icml04-apprentice.pdf' target='_blank'>Abbeel and Ng 2004</a>, <a href='http://dx.doi.org/10.1177/0278364910371999' target='_blank'>Abbeel et al. 2010</a>) and other methods for estimating a user's reward function (<a href='http://www.aaai.org/Papers/AAAI/2008/AAAI08-227.pdf' target='_blank'>Ziebart et al. 2008</a>, <a href='http://martin.zinkevich.org/publications/maximummarginplanning.pdf' target='_blank'>Ratliff et al. 2006</a>). IRL has many variants that improve the generality, aggregability, or analysis of the reward function, or the workflow to approaching it (<a href='http://jmlr.org/proceedings/papers/v48/finn16.pdf' target='_blank'>Finn et al. 2016a</a>). A variant called interactive IRL attempts to learn about the reward function at the same time as trying to maximize it (<a href='https://dl.dropboxusercontent.com/u/23843264/Permanent/towards-interactive-inverse-reinforcement-learning.pdf' target='_blank'>Armstrong and Leike 2016</a>). Issues with the unidentifiability of the reward function might also be addressable by active IRL (<a href='https://pdfs.semanticscholar.org/04cf/f669a73c4b6c84124de6e88562cab742c6cb.pdf' target='_blank'>Amin and Singh 2016</a>). IRL methods might not scale safely, however, due to their reliance on the faulty assumption that human demonstrators are optimizing for a specific reward function, where in reality humans are often irrational, ill-informed, incompetent, and immoral. Recent work has begun to address these issues (<a href='http://stuhlmueller.org/papers/preferences-nipsworkshop2015.pdf' target='_blank'>Evans et al. 2015</a>, <a href='https://www.aaai.org/Conferences/AAAI/2016/Papers/03Evans12476.pdf' target='_blank'>Evans et al. 2015a</a>). Generative adversarial imitation learning is another promising approach to this problem (<a href='http://papers.nips.cc/paper/6391-generative-adversarial-imitation-learning.pdf' target='_blank'>Ho and Ermon 2016</a>).", "mini_description": "Learning the reward function that a human is approximately optimizing from observing the human's actions", "breakdowns": [{"id": "0020202000", "sub_nodes": [{"id": "00202020000", "title": "Cooperative Inverse Reinforcement Learning", "description": "The cooperative inverse reinforcement learning paradigm views the human-agent interaction as a cooperative game where both players attempt to find a joint policy that maximizes the humans secret value function (<a href='https://people.eecs.berkeley.edu/~dhm/papers/CIRL_NIPS_16.pdf' target='_blank'>Hadfield-Menell et al. 2016</a>, <a href='https://intelligence.org/files/AlignmentMachineLearning.pdf' target='_blank'>Taylor et al. 2016</a>). An outstanding challenge in this is to determine which portions of the ascertained value function are instrumental, which are incidental, and which exhibit deep values.", "mini_description": "A technique where human-agent interaction is modeled as a cooperative game where both players attempt to find a joint policy that maximizes the humans secret value function", "links": [{"id": "004010007", "reason": "From Cooperative Inverse Reinforcement Learning also see Cooperative Inverse Reinforcement Learner where this algorithm is used for control in an online context rather than upfront value learning."}]}]}], "links": [{"id": "002020205"}, {"id": "004010008"}, {"id": "0050001"}, {"id": "0050002"}]}, {"id": "002020201", "title": "Imitation Statistical Guarantees", "description": "Some techniques focus on having a measure of the robustness of human imitation within the contexts learned, such as those quantifying the sampling needed for preference inference (<a href='https://arxiv.org/pdf/1610.00850.pdf' target='_blank'>Laskey et al. 2016</a>, <a href='https://intelligence.org/files/AlignmentMachineLearning.pdf' target='_blank'>Taylor et al. 2016</a>).", "mini_description": "To have a measure of the robustness of human imitation", "breakdowns": [{"id": "0020202010", "sub_nodes": [{"id": "00202020100", "title": "Generative Adversarial Networks", "description": "Generative adversarial networks may be able to train a system to generate human-like answers to within some statistical confidence (<a href='http://papers.nips.cc/paper/5423-generative-adversarial-nets' target='_blank'>Goodfellow et al. 2014</a>, <a href='https://intelligence.org/files/AlignmentMachineLearning.pdf' target='_blank'>Taylor et al. 2016</a>).", "mini_description": "Using GANs to train a system to generate human-like answers to within some statistical confidence", "links": [{"id": "002030201"}, {"id": "003040000"}]}, {"id": "00202020101", "title": "Other Generative Models", "description": "In cases of large output spaces and limited training data, one can use exploration, generation, and imitation variation to quantify confidence of imitation (<a href='http://jmlr.org/proceedings/papers/v37/gregor15.html' target='_blank'>Gregor et al. 2015</a>, <a href='https://intelligence.org/files/AlignmentMachineLearning.pdf' target='_blank'>Taylor et al. 2016</a>), useful for imitation learning (<a href='http://jmlr.org/papers/v15/judah14a.html' target='_blank'>Judah et al. 2014</a>, <a href='http://www.jmlr.org/proceedings/papers/v15/ross11a/ross11a.pdf' target='_blank'>Ross et al. 2011</a>, <a href='www.sfb588.uni-karlsruhe.de/Module/Publications/publications/Asfour2008a.pdf' target='_blank'>Asfour et al. 2008</a>, <a href='http://rob.schapire.net/papers/SyedSchapireUAI2007.pdf' target='_blank'>Syed and Schapire 2007</a>).", "mini_description": "Using exploration, generation, and imitation variation to quantify confidence of imitation"}]}]}, {"id": "002020202", "title": "Operator Modeling", "description": "In order to model an operator's preferences as comprehensively as possible, the operator themselves should be modeled well. By what methods can an operator be modeled in such a way that a model of the operators preferences can not only be extracted, but also continually refined to become arbitrarily accurate, while representing the operator as a subsystem embedded within the larger world? (<a href='https://intelligence.org/files/ValueLearningProblem.pdf' target='_blank'>Soares 2016</a>) There is work on modeling individuals' cognition (<a href='http://probmods.org/v2' target='_blank'>Goodman and Tenenbaum 2016</a>), progress in short-term modeling of, and adaptation to, operators (<a href='http://www.jesshamrick.com/publications/pdf/Liu2016-Goal_Inference_Improves_Objective.pdf' target='_blank'>Liu et al. 2016</a>), and modeling human preferences as inverse planning (<a href='http://web.mit.edu/clbaker/www/papers/cognition2009.pdf' target='_blank'>Baker et al. 2009</a>), but more work is needed to approach comprehensive modeling (<a href='http://intelligence.org/files/TechnicalAgenda.pdf' target='_blank'>Soares and Fallenstein 2014a</a>). One approach to prevent overfitting is to use different assumptions about underlying cognitive models of the actor whose preferences are being learned (<a href='http://www.gatsby.ucl.ac.uk/%7Echuwei/paper/gppl.pdf' target='_blank'>Chu and Ghahramani 2005</a>).<br><br> For complex tasks, it seems plausible that the system will need to learn a detailed psychological model of a human if it is to imitate one, and that this might be significantly more difficult than training a system to do engineering directly. More research is needed to clarify whether imitation learning can scale efficiently to complex tasks.", "mini_description": "How the values and preferences of an operator can be extracted, and the operator modeled as embedded in its environment, while increasing accuracy over time", "links": [{"id": "002020001"}, {"id": "00202000200", "reason": "From Operator Modeling also see Operator Value Learning which is similar but focuses on learning the ethical or moral values of people rather than their broader dynamics."}, {"id": "00204", "reason": "From Operator Modeling also see Psychological Analogues as there will likely be much cross-pollination between these threads."}]}, {"id": "002020203", "title": "Reversible Tasks via Variational Autoencoding", "description": "For the subset of human tasks that are reversible, or able to be done either forwards or backwards with minimal information loss, one can form generative models based on training data of queries and associated good responses (<a href='https://intelligence.org/files/AlignmentMachineLearning.pdf' target='_blank'>Taylor et al. 2016</a>). Variational autoencoders seem appropriate for this as they learn lower-dimensional manifolds that tease out conceptual structure (Kingma and Welling 2013). It might also be possible to break such nonreversible tasks into multiple reversible tasks to leverage this technique (<a href='http://papers.nips.cc/paper/4966-learning-stochastic-inverses.pdf' target='_blank'>Stuhlm\"uller et al. 2013</a>). Just which sets of tasks can be performed using reversible generative models remains underexplored.", "mini_description": "Using deep generative models to learn tasks that can be undone"}, {"id": "002020204", "title": "Distance from User Demonstration", "description": "Another method of safely learning from observing humans is to learn baseline policies in an apprenticeship manner and explore with bounded deviations from demonstrations (<a href='http://www.jmlr.org/proceedings/papers/v15/ross11a/ross11a.pdf' target='_blank'>Ross et al. 2011</a>, <a href='http://ai.stanford.edu/~pabbeel//pubs/AbbeelNg_eaalirl_ICML2005.pdf' target='_blank'>Abbeel and Ng 2005</a>, <a href='http://arxiv.org/abs/1606.06565' target='_blank'>Amodei et al. 2016</a>, <a href='http://jmlr.org/proceedings/papers/v48/finn16.pdf' target='_blank'>Finn et al. 2016a</a>). It may however be difficult to learn deep preferences this way.", "mini_description": "Learning baseline policies in an apprenticeship manner and exploring with bounded deviations from demonstrations", "links": [{"id": "002000102"}]}, {"id": "002020205", "title": "Narrow Value Learning", "description": "While the long-term goal of both value alignment and human imitation is to figure out the appropriate terminal or fundamental values, intermediate progress can also have appreciable utility for AI safety and beneficence. The learning of instrumental subobjectives, which may also be conceived of as narrow values (<a href='https://medium.com/ai-control/ambitious-vs-narrow-value-learning-99bd0c59847e' target='_blank'>Christiano 2015d</a>) is a valid next step in value alignment. Recognition of the line between such instrumental values and fundamental values, however, is a significant outstanding need.", "mini_description": "The learning of relatively narrow human subgoals and instrumental values using value-learning techniques", "links": [{"id": "002020200", "reason": "From Narrow Value Learning also see Inverse Reinforcement Learning which provides many good methods for narrow value learning."}]}, {"id": "002020206", "title": "Scaling Judgement Learning", "description": "Generally, one can consider the combination of multiple learning and validation techniques for learning what a human would respond, or how they would judge a situation, reserving active learning and querying of humans for when it's quite necessary (<a href='https://medium.com/ai-control/mimicry-maximization-and-meeting-halfway-c149dd23fc17' target='_blank'>Christiano 2015e</a>). It has also been proposed that one might train a highly capable aligned agent using a mix of a series of more capable approval-directed reinforcement learning agents and bootstrapping (<a href='https://medium.com/ai-control/alba-an-explicit-proposal-for-aligned-ai-17a55f60bbcf' target='_blank'>Christiano 2016</a>). In theory, it would be possible to establish a trajectory whereby the succession becomes capable of closely approximating what the human would have decided given much more time, resurces, and education. This is, however, much more difficult than developing a good generative model of observed human behavior.", "mini_description": "Training a highly capable aligned agent using a succession of approval-directed agents", "links": [{"id": "004010008", "reason": "From Scaling Judgement Learning also see Human Judgement Learner where a similar technique is used in a continuous online manner."}, {"id": "0040100"}]}]}], "links": [{"id": "0020200020101"}]}]}], "links": [{"id": "0000001"}, {"id": "00200010003"}, {"id": "005"}, {"id": "004010009", "reason": "From Technical Value Alignment also see Informed Oversight as further research in that control problem may change the field's understanding of the nature of value loading."}, {"id": "0050403010200"}]}, {"id": "00203", "title": "Increasing Contextual Awareness", "description": "Whether using closed-world or open-world models, systems operating within the real world tend to model only a very small portion of the environment, rendering them oblivious to common sense contextualization of even concepts within their models. Such models also often have unintended structural biases and blind spots stemming from how they are generated, and reduction of this obliviousness leads to more trustworthy systems. With improved quality, salience, robustness, flexibility, and contextualization of conceptual models, agents can apply more common sense to introspection and to decision making. Typical ability gaps relative to humans include common sense context, learning causal models, the grounding of concepts, and learning to learn (<a href='http://www.mit.edu/~tomeru/papers/machines_that_think.pdf' target='_blank'>Lake et al. 2016</a>, <a href='http://dx.doi.org/10.1007/978-3-319-41649-6_11' target='_blank'>Thorisson et al. 2016a</a>), despite experimental systems having demonstrated each of these individually.", "mini_description": "Techniques to reduce the contextual obliviousness that narrow modern-day AI systems often have", "breakdowns": [{"id": "002030", "sub_nodes": [{"id": "0020300", "title": "Realistic World-Models", "description": "Agents operating in a closed-world environment, e.g. AIs playing perfect information games like chess or imperfect information games like poker, only ever need to worry about the configuration of elements that they know exist. For agents embedded in the real world, however, invariably an open-world environment, awareness of the possibility of new or changing entities, relationships, and concepts needs to be integral to its processes of learning and pursuing goals (<a href='http://intelligence.org/files/TechnicalAgenda.pdf' target='_blank'>Soares and Fallenstein 2014a</a>).", "mini_description": "Techniques to structure the system's conception of the world in a way that's more accurate", "breakdowns": [{"id": "00203000", "sub_nodes": [{"id": "002030000", "title": "Expressive Representations", "description": "The structures in which large open world models are modelled will need to be sufficiently expressive while remaining conducive to computational efficiency (<a href='https://people.eecs.berkeley.edu/~russell/papers/cacm15-oupm.pdf' target='_blank'>Russell 2015</a>). Other model desiderata include interpretability, paths to compatibility between separately trained systems, and paths to compatibility between symbolic and subsymbolic representations.<br><br> For highly capable systems, the content to fill out such models is too high-volume for humans to fill in directly, so either semisupervised, distantly supervised, or unsupervised learning methods are called for.", "mini_description": "Developing structures for world modeling that are sufficiently expressive while remaining computationally efficient"}, {"id": "002030001", "title": "Unsupervised Model Learning", "description": "As real-world environments are very large, and supervised tagging of concepts and relationships does not scale, there is great interest in unsupervised model learning. In online contexts, explicit mechanisms are needed to deal with ongoing refactorings of the ontology, or world model, of a system (<a href='https://intelligence.org/files/OntologicalCrises.pdf' target='_blank'>Blanc 2011</a>). The paradigm of maintaining a distribution of possible worlds and determining their likelihoods can be reasonable when those worlds collapse into a tractable number of classes (<a href='http://aclweb.org/anthology/W/W16/W16-1310.pdf' target='_blank'>Russell et al. 2016</a>, <a href='http://arxiv.org/abs/1606.06565' target='_blank'>Amodei et al. 2016</a>).", "mini_description": "Learning and updating a world model or ontology from unstructured sets and streams of data", "breakdowns": [{"id": "0020300010", "sub_nodes": [{"id": "00203000100", "title": "Concept Drift", "description": "Concept drift is the gradual warping of the meaning of a given concept over time. In a statistical learning context, the statistical properties of a target variable to be predicted, changes in unforeseen ways over time (<a href='http://dblp.uni-trier.de/db/conf/sbia/sbia2004.html#GamaMCR04' target='_blank'>Gama et al. 2004</a>), and robust AI should be vigilant in detecting such drift.", "mini_description": "The unforeseen movement of the boundaries within a concept space that delineate a given concept", "links": [{"id": "00203030106", "reason": "From Concept Drift also see Robustness to Distributional Shift which addresses identifying and responding to increases in ambiguity around a concept statistically."}]}, {"id": "00203000101", "title": "Ontology Identification", "description": "Given goals stated using a given ontology, formalism, or semiformalism, and streams of data about the world, an advanced agent should be able to identify the most appropriate ontology in which satisfaction of the intended goals should be executed and evaluated (<a href='https://intelligence.org/files/RealisticWorldModels.pdf' target='_blank'>Soares 2015</a>, <a href='http://intelligence.org/files/TechnicalAgenda.pdf' target='_blank'>Soares and Fallenstein 2014a</a>). Doing this needs to account for the fact that those ever-changing streams of data are only indirectly and incompletely understood by the operator who originally provided the goals.<br><br> Relatedly, it would be helpful for the purpose to understand theoretically and practically how learned representations of high-level human concepts could be expected to generalize, or fail to do so, in radically new contexts (<a href='http://www.aaai.org/ocs/index.php/WS/AAAIW15/paper/download/10149/10138' target='_blank'>Tegmark 2015</a>).", "mini_description": "Structuring and updating a world model optimally given the evidence coming from both specified goal content and unstructured data", "links": [{"id": "002030202"}]}, {"id": "00203000102", "title": "Ontology Update Thresholds", "description": "Ontology rot occurs when one or more concepts in a world model are held fixed as new dynamics emerge which merit updates to the world model (Zablith 2009, <a href='http://smartdata2015.dataversity.net/sessionPop.cfm?confid=91&proposalid=7754' target='_blank'>Mallah 2015</a>, <a href='https://pdfs.semanticscholar.org/e38f/9d0878d9b06713142331695efe9ce5e5e0e0.pdf' target='_blank'>Noy and Klein 2004</a>). When a measure or property becomes a target or a key performance indicator, the dynamics of the system change to optimize for it, often in unexpected or parochial ways, causing that metric to no longer be a good measure (Mallah 2017). This is known as Goodhart's Law (<a href='http://www.ribbonfarm.com/2016/06/09/goodharts-law-and-why-measurement-is-hard/' target='_blank'>Manheim 2016</a>). Insufficiently adaptive ontologies can lead to parochial behaviors with justifications that may no longer be grounded (Zablith 2009, <a href='https://pdfs.semanticscholar.org/e38f/9d0878d9b06713142331695efe9ce5e5e0e0.pdf' target='_blank'>Noy and Klein 2004</a>). Ontologies updated too frequently or with too little evidence, however, can lead to mercurial behavior (<a href='https://people.eecs.berkeley.edu/~russell/research/future/mallah-aamas14-future.pdf' target='_blank'>Mallah 2014</a>).", "mini_description": "Thresholds or frequency of world models updates should be tuned to prevent suffering from serious instances of concept drift", "links": [{"id": "00201"}, {"id": "0020201", "reason": "From Ontology Update Thresholds also see Degree of Value Evolution since, relatedly, when a property degrades in its role as a moral value, reevaluation of the intent of that value may be necessary."}, {"id": "002000101", "reason": "From Ontology Update Thresholds also see Mild Optimization because developing a formal model of Goodharts Law can also benefit that need."}]}, {"id": "00203000103", "title": "Episodic Contexts", "description": "In model-based reinforcement learning, one can use the observed transitions of the unlabeled episodes to improve the quality of the model (<a href='http://arxiv.org/abs/1606.06565' target='_blank'>Amodei et al. 2016</a>, <a href='https://people.eecs.berkeley.edu/~russell/papers/aaai02-alisp.pdf' target='_blank'>Andre and Russell 2002</a>). State abstraction from either online or offline episodes using similar such techniques can also be applied in other model learning paradigms (<a href='https://www.umiacs.umd.edu/~yzyang/paper/YouCookMani_CameraReady.pdf' target='_blank'>Yang et al. 2015</a>).", "mini_description": "Mining historical or live episodes for hints to defining states and concepts"}]}], "links": [{"id": "0020201", "reason": "From Unsupervised Model Learning also see Degree of Value Evolution since as new concepts are learned and world models evolve, there are competing pressures to restructure existing values using the updated ontology and to change what set of values to hold to in the first place."}, {"id": "002030203", "reason": "From Unsupervised Model Learning also see World Model Connectedness as that addresses preventing fragmentation to benefit transfer learning and help avoid cognitive dissonance."}, {"id": "002030101"}, {"id": "002030300"}, {"id": "0040100"}]}, {"id": "002030002", "title": "Correlation of Dynamics", "description": "Taking into account correlations between the AI system's behaviors and those of its environment or of other agents is another kind of contextual awareness especially useful in game theoretic framings (<a href='https://ie.technion.ac.il/~moshet/progeqnote4.pdf' target='_blank'>Tennenholtz 2004</a>, <a href='http://www.aaai.org/ocs/index.php/WS/AAAIW14/paper/viewFile/8833/8294' target='_blank'>LaVictoire et al. 2014</a>, <a href='https://intelligence.org/wp-content/uploads/2014/10/Hintze-Problem-Class-Dominance-In-Predictive-Dilemmas.pdf' target='_blank'>Hintze 2014</a>, <a href='http://www.cs.cornell.edu/home/halpern/papers/rbdec.pdf' target='_blank'>Halpern et al. 2014</a>, <a href='https://pdfs.semanticscholar.org/40b3/bbe8d3e0ff66caae3217f4b2fc0e71fd01e2.pdf' target='_blank'>Soares and Fallenstein 2015</a>).", "mini_description": "Taking into account correlations between the system's behaviors and those of its environment and other agents", "links": [{"id": "002020000"}, {"id": "000010001", "reason": "From Correlation of Dynamics also see Open Source Game Theory as that addresses additional layers of possible agent correlation, stemming from logic."}]}, {"id": "002030003", "title": "World-Embedded Solomonoff Induction", "description": "There are theoretical challenges when the world model of an advanced agent includes itself, as more robust advanced agents would (<a href='http://intelligence.org/files/TechnicalAgenda.pdf' target='_blank'>Soares and Fallenstein 2014a</a>). In addition to generally being more robust and accurate, this inclusion critically helps the agent include the effects of its internal dynamics on the environment. It can also provide a formal grounding for more realistic game theoretic analyses (<a href='http://www.auai.org/uai2016/proceedings/papers/87.pdf' target='_blank'>Leike et al. 2016</a>). Updated paradigms are needed to model how generally intelligent agents that are embedded in their environments should reason (<a href='https://intelligence.org/files/RealisticWorldModels.pdf' target='_blank'>Soares 2015</a>, <a href='http://agi-conference.org/2012/wp-content/uploads/2012/12/paper_76.pdf' target='_blank'>Orseau and Ring 2012</a>). Identification of the induction problem itself, as analog to Solomonoff induction, in the scenario of an agent both embedded in and computed by its environment, is termed naturalized induction, and can benefit from more research (<a href='https://intelligence.org/files/ReflectiveSolomonoffAIXI.pdf' target='_blank'>Fallenstein et al. 2015</a>, <a href='https://intelligence.org/files/RealisticWorldModels.pdf' target='_blank'>Soares 2015</a>, <a href='http://lesswrong.com/lw/jd9/building_phenomenological_bridges/' target='_blank'>RobbBB 2013</a>). The agent would need to consider what the possible environments that could be embedding the agent are, and determining a good simplicity prior would help to create a distribution or weighing of those possibilities (<a href='http://intelligence.org/files/TechnicalAgenda.pdf' target='_blank'>Soares and Fallenstein 2014a</a>).<br><br> Understanding this analog to Solomonoff Induction would help us circumscribe some theoretical limits around such agents. Deficiencies in such capabilities can lead to the type of agent that finds clever ways to seize control of its observation channel, rather than actually identifying and manipulating the features in the world that the reward function was intended to proxy for (<a href='https://intelligence.org/files/RealisticWorldModels.pdf' target='_blank'>Soares 2015</a>). Determinations of how well an agent does in this regard would require formalizing that question in tandem with developing a scoring metric that evaluates the resulting environment histories, rather than just the agents observations. Some theoretical progress has been made by using reflective oracles to model both agents and their environments, enabling agents to have models of one another and converge to various game theoretic equilibria (<a href='https://intelligence.org/files/ReflectiveOraclesAI.pdf' target='_blank'>Fallenstein et al. 2015a</a>). Progress has also been made on the \"grain of truth\" problem, such that agents can reason using explicit models of each other without infinite regress (<a href='http://www.auai.org/uai2016/proceedings/papers/87.pdf' target='_blank'>Leike et al. 2016</a>). Progress in this type of reasoning is also one of the prerequisites to the agent having a theory of open source game theory, useful for designing robust cooperation (<a href='http://acritch.com/media/ai/Andrew_Critch_-_Parametric_Bounded_Lob.pdf' target='_blank'>Critch 2016</a>).", "mini_description": "Addressing the formalization of the induction problem for an agent that includes itself in its model of the world, especially if that world computes that agent", "links": [{"id": "0000000"}, {"id": "000010001", "reason": "From World-Embedded Solomonoff Induction also see Open Source Game Theory as this type of model and reasoning has implications not only for directly improving contextual awareness, but can lead to foundational decision theoretic capabilities."}]}, {"id": "002030004", "title": "Perceptual Distance Agnostic World Models", "description": "Proximal world models centered on the agent's percepts (<a href='https://mitpress.mit.edu/books/conceptual-spaces' target='_blank'>Gardenfors 2000</a>) may lead to undesirable distortion of biases. When concepts or entities that are a shorter perceptual distance away accrue perceived value disproportionately due to that shorter distance, behaviors that seem narcissistic or overly parochial may emerge. These kinds of weaknesses can manifest when scaling classical reinforcement learners (<a href='http://www.danieldewey.net/learning-what-to-value.pdf' target='_blank'>Dewey 2011</a>), for example. Indicative behaviors may include promotion of confirmation bias or incorrectly calculating externalities as negligible. A key preventative to such weaknesses is for world models, or at least significant portions of them, to be distal (<a href='https://mitpress.mit.edu/books/conceptual-spaces' target='_blank'>Gardenfors 2000</a>), establishing tenets or objective concepts as central.", "mini_description": "Establishing tenets or objective concepts as central to a world-model rather than agent-received experience", "links": [{"id": "002000103", "reason": "From Perceptual Distance Agnostic World Models also see Computational Humility which can motivate the need for less agent-centric models."}, {"id": "0020100", "reason": "From Perceptual Distance Agnostic World Models also see Pursuing Environmental Goals as that addresses the more general problem of objective function."}, {"id": "002030203", "reason": "From Perceptual Distance Agnostic World Models also see World Model Connectedness as proximal and distal elements of an ontology should be richly connected for an agent to be robust."}, {"id": "0020302"}]}, {"id": "002030005", "title": "Knowledge Representation Ensembles", "description": "One approach to improving contextual robustness is to maintain and to aggregate multiple different knowledge representation schemes, ontologies, structures, and biases (<a href='http://hijournal.bcs.org/index.php/jhi/article/viewFile/578/590' target='_blank'>Snowden 2005</a>).", "mini_description": "Maintaining and using a variety of world models simultaneously", "links": [{"id": "0020109"}, {"id": "00200010102", "reason": "From Knowledge Representation Ensembles also see Multiobjective Optimization as a way to accomplish that is to have a different subobjective per knowledge representation scheme and using multiobjective optimization."}]}]}]}, {"id": "0020301", "title": "Endowing Common Sense", "description": "Common sense is often cited as a basic capability that humans have but AI and ML systems lack. Common sense is not a single faculty though, but a combination of extensive world knowledge, deep semantic and pragmatic parsing of percepts, and using the appropriate combination of types of reasoning for the situation (<a href='https://arxiv.org/pdf/1212.4799v2' target='_blank'>Freer et al. 2012</a>, <a href='https://arxiv.org/pdf/1502.06108v3' target='_blank'>Lin and Parikh 2015</a>).", "mini_description": "Endowing agents with common sense knowledge and common sense reasoning will make them more robust and easier to align", "breakdowns": [{"id": "00203010", "sub_nodes": [{"id": "002030100", "title": "Common Sense Reasoning", "description": "Common sense reasoning includes using the appropriate combinations of induction, deduction, and abduction at the appropriate situations and times and using appropriate world knowledge (<a href='https://books.google.com/books?id=g7UAIhnmJpsC' target='_blank'>Goertzel et al. 2011</a>), as well as dynamic binding to novel faculties (<a href='https://arxiv.org/pdf/1503.08895v5' target='_blank'>Sukhbaatar et al. 2015</a>).", "mini_description": "Using the appropriate types of reasoning for each given situation"}, {"id": "002030101", "title": "Bootstrapped Common Sense", "description": "Abstracting patterns to establish semantics requires a dynamic mix of different reasoning abilities (<a href='http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Vedantam_Learning_Common_Sense_ICCV_2015_paper.pdf' target='_blank'>Vedantam et al. 2015</a>).", "mini_description": "Using unsupervised or very distantly supervised learning to establish semantics from observations and actions", "links": [{"id": "002030001", "reason": "From Bootstrapped Common Sense also see Unsupervised Model Learning which has many overlaps with this."}]}, {"id": "002030102", "title": "Seeded Common Sense", "description": "A core of common sense concepts, relationships, and dynamics can be used as as a pretraining or seeding to bootstrap a system that will learn additional ontology and knowledge around what is seeded (<a href='http://www-symbiotic.cs.ou.edu/~fagg/classes/neurocog/restrict/papers/cohen_etal_2001.pdf' target='_blank'>Cohen et al. 2001</a>). When the core of the relevant contextual landscape can be specified via structured knowledge like this, one can effectively improve the system's contextual awareness by providing a higher quantity and broader scope of usable world-knowledge than one might expect the system to typically need. Abstracting patterns, which has similarities to unsupervised model learning, requires this kind of dynamic mix of reasoning abilities (<a href='http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Vedantam_Learning_Common_Sense_ICCV_2015_paper.pdf' target='_blank'>Vedantam et al. 2015</a>).", "mini_description": "Using a core of common sense concepts, relationships, and dynamics as a pretraining or seeding of common sense in order to bootstrap a system"}, {"id": "002030103", "title": "Metareasoning", "description": "Metareasoning, reasoning about reasoning, is an important common sense faculty that includes meta-level control of introspection and of computational activities and modelling the self (<a href='https://mitpress.mit.edu/books/metareasoning' target='_blank'>Cox and Raja 2011</a>, <a href='https://mitpress.mit.edu/books/metareasoning' target='_blank'>Conitzer 2011</a>). It helps to contextualize and modulate common sense reasoning and selection of appropriate algorithms (<a href='http://papers.nips.cc/paper/5552-global-sensitivity-analysis-for-map-inference-in-graphical-models.pdf' target='_blank'>Lieder et al. 2014</a>).", "mini_description": "Reasoning about reasoning, including meta-level control of introspection, computational activities, and models of the self", "links": [{"id": "000010102"}, {"id": "0000102"}, {"id": "00203030200"}, {"id": "00306"}]}, {"id": "002030104", "title": "Lengthening Horizons", "description": "Planning and lookahead time horizons significantly influence both the effectiveness of and the stability of agents. A potential technique to do so for online-planning markov decision processes is to do state abstraction (<a href='http://www.ijcai.org/Abstract/16/430' target='_blank'>Bai et al. 2016</a>).", "mini_description": "Techniques to enable longer-horizon modeling and forecasting of prospective scenarios, e.g. ones that could lead to reward hacking, diminished capacity, or other potentially irrecoverable states", "links": [{"id": "00200010001"}, {"id": "0020200010101"}]}]}]}, {"id": "0020302", "title": "Concept Geometry", "description": "Conceptual spaces, be they vector spaces, semantic networks, more implicit subsymbolic structures, or symbolic-subsymbolic hybrid representations, are how concepts are defined, grounded, and related to each other (<a href='https://mitpress.mit.edu/books/conceptual-spaces' target='_blank'>Gardenfors 2000</a>). Any ontology, including implicit ones in subsymbolic architectures, is biased by the faculties with which it was constructed and intended (Sotala 2015, <a href='https://mitpress.mit.edu/books/conceptual-spaces' target='_blank'>Gardenfors 2000</a>).", "mini_description": "The structure and techniques for representing concepts, relationships, and processes, in support of activities like reasoning and recall", "breakdowns": [{"id": "00203020", "sub_nodes": [{"id": "002030200", "title": "Implicit Human Concepts", "description": "Beyond the realism of concepts, a possible desideratum for enhanced human compatibility is similarity as compared with how humans conceptualize the world. Concepts can be defined with respect to groundings to human-like primitives such as senses, maintaining human-like concept relationships and spaces (Sotala 2016, <a href='http://dx.doi.org/10.1007/978-3-319-41649-6_11' target='_blank'>Thorisson et al. 2016a</a>). Hierarchical structured probabilistic models are one plausible avenue for human-level concept learning (<a href='http://web.mit.edu/cocosci/Papers/Science-2015-Lake-1332-8.pdf' target='_blank'>Lake et al. 2015</a>). One should also aim to understand theoretically and practically how learned representations of high-level human concepts could be expected to generalize, or fail to do so, in radically new contexts (<a href='http://www.aaai.org/ocs/index.php/WS/AAAIW15/paper/download/10149/10138' target='_blank'>Tegmark 2015</a>).", "mini_description": "The structure or shape of how humans represent concepts", "links": [{"id": "0040103"}]}, {"id": "002030201", "title": "Conservative Concepts", "description": "How can a classifier be trained to develop useful concepts that exclude quite atypical examples and edge cases? (<a href='https://intelligence.org/files/AlignmentMachineLearning.pdf' target='_blank'>Taylor et al. 2016</a>) Including negative examples to try to get a better sense of the classification boundaries is insufficient (<a href='https://pdfs.semanticscholar.org/bee0/44c8e8903fb67523c1f8c105ab4718600cdb.pdf' target='_blank'>Goodfellow et al. 2015</a>, <a href='http://auai.org/uai2016/proceedings/papers/226.pdf' target='_blank'>Siddiqui et al. 2016</a>). Novel ways of analyzing cluster spaces may be necessary.", "mini_description": "Methods to represent normal variation within a concept but not potential extreme outliers", "breakdowns": [{"id": "0020302010", "sub_nodes": [{"id": "00203020100", "title": "Dimensionality Reduction With Anomaly Detection", "description": "One plausible and underexplored method of establishing conservative concepts is to find the important features of training instances, then use generative models to synthesize new examples that are similar to the training instances only with respect to those, and use anomaly detection to probe and repair the resulting space (<a href='https://intelligence.org/files/AlignmentMachineLearning.pdf' target='_blank'>Taylor et al. 2016</a>, <a href='https://www.cs.toronto.edu/~hinton/science.pdf' target='_blank'>Hinton and Salakhutdinov 2006</a>).", "mini_description": "Finding the important features of a concept, generating variants, and using anomaly detection to correct outliers"}]}], "links": [{"id": "00202020100", "reason": "From Conservative Concepts also see Generative Adversarial Networks as actor-critic arrangements can refine the understanding of a concept, though more research needs to be done to scale the generalizations by the critic."}, {"id": "0000102", "reason": "From Conservative Concepts also see Goal Stability as outlandish interpretations of concepts hamper stability."}, {"id": "002030301", "reason": "From Conservative Concepts also see Inductive Ambiguity Identification as those techniques can be used to better analyze decision boundary neighborhoods and generated exemplars."}]}, {"id": "002030202", "title": "Multilevel World-Models", "description": "Reality and possibility are very large. Multilevel conceptualizations therefore arise, whether implicitly or explicitly, in the layers of a deep net, in systems biology, in meta vs. upper vs. lower vs. operational ontologies, in variational renormalization in physics (<a href='https://arxiv.org/pdf/1608.08225v1.pdf' target='_blank'>Lin and Tegmark 2016</a>), and in multilevel analysis in statistics (<a href='https://uk.sagepub.com/en-gb/eur/multilevel-analysis/book234191' target='_blank'>Snijders and Bosker 2011</a>). A key question for AI safety is how multi-level world-models can be constructed from sense data in a manner amenable to ontology identification (<a href='https://intelligence.org/files/ValueLearningProblem.pdf' target='_blank'>Soares 2016</a>, <a href='http://intelligence.org/files/TechnicalAgenda.pdf' target='_blank'>Soares and Fallenstein 2014a</a>).", "mini_description": "Representations of the world or environment in a hierarchical or a holistic manner", "links": [{"id": "00200010301"}, {"id": "002020001"}, {"id": "00203000101", "reason": "From Multilevel World-Models also see Ontology Identification for more on that."}]}, {"id": "002030203", "title": "World Model Connectedness", "description": "Fragmented world models occur when portions of an (explicit or implicit) ontology that, environmentally, should connect or overlap, remain cleaved. This can lead to limited causal learning, inconsistent theories, unwarranted concept splitting, and poor transfer learning (<a href='https://www.cse.ust.hk/~qyang/Docs/2009/tkde_transfer_learning.pdf' target='_blank'>Pan and Yang 2010</a>, Mallah 2017). Fragmentation can also result in catastrophic inference, orphaning historical representations (<a href='http://ac.els-cdn.com/S002251931500106X/1-s2.0-S002251931500106X-main.pdf?_tid=6f1c2bc8-c4e7-11e6-972c-00000aacb35e&acdnat=1482041076_291e3fa094036bcad0f5601cfc975933' target='_blank'>Nikolic 2014</a>, <a href='https://papers.nips.cc/paper/799-catastrophic-interference-in-connectionist-networks-can-it-be-predicted-can-it-be-prevented.pdf' target='_blank'>French 1993</a>). Focus on establishing explicit connections across contexts to previously learned features might alleviate these issues (<a href='https://arxiv.org/pdf/1606.04671v3' target='_blank'>Rusu et al. 2016</a>).", "mini_description": "Motivations and techniques to reduce the fragmentation within an ontology", "links": [{"id": "002030001"}, {"id": "002030004"}]}]}], "links": [{"id": "0020200010100"}, {"id": "00202000101", "reason": "From Concept Geometry also see Value Structuring as that addresses the geometry of human values."}, {"id": "002030004", "reason": "From Concept Geometry also see Perceptual Distance Agnostic World Models as that addresses proximal versus distal spaces."}]}, {"id": "0020303", "title": "Uncertainty Identification and Management", "description": "There are many types of uncertainty that AIs will need to model (<a href='http://intelligence.org/files/TechnicalAgenda.pdf' target='_blank'>Soares and Fallenstein 2014a</a>, <a href='https://www.aaai.org/Papers/Symposia/Spring/2008/SS-08-03/SS08-03-002.pdf' target='_blank'>Carleton et al. 2007</a>). As a matter of course, given their training data, machine learning deals with inductive uncertainty in typically narrow contexts. Awareness that insufficient traing data may have been provided for particular determinations supports the developing capability of recognizing ambiguity. Relatedly, determining dimensions or features for which there is little or no data, and which may be important, can be important for proactive ambiguity management (<a href='https://intelligence.org/files/ValueLearningProblem.pdf' target='_blank'>Soares 2016</a>). This is particularly relevant in online learning, where training is always incomplete and actions may be taken to focus on improving areas of unclarity (<a href='http://www.cs.huji.ac.il/~shais/papers/OLsurvey.pdf' target='_blank'>Shalev-Shwartz 2012</a>). At any given point in time, there may properly be not only inductive uncertainty regarding empirical facts, but also logical uncertainty, being unsure of the as-yet-to-be-computed specific complex entailments of the things that are already known (<a href='https://intelligence.org/files/LogicalInductionAbridged.pdf' target='_blank'>Garrabrant et al. 2016a</a>).", "mini_description": "Recognizing when there is uncertainty around a concept or determination", "breakdowns": [{"id": "00203030", "sub_nodes": [{"id": "002030300", "title": "Inferring Latent Variables", "description": "One can Identify latent or implicit variables from data via dependency analyses on that data (<a href='https://intelligence.org/files/AlignmentMachineLearning.pdf' target='_blank'>Taylor et al. 2016</a>).", "mini_description": "Identifying implicit variables from data", "links": [{"id": "002030001", "reason": "From Inferring Latent Variables also see Unsupervised Model Learning as finding these is a key ability for that."}]}, {"id": "002030301", "title": "Inductive Ambiguity Identification", "description": "A desirable general capability for machine learning systems would be to detect and notify us of cases where classifications of test data is highly underdetermined given the training data (<a href='https://intelligence.org/files/AlignmentMachineLearning.pdf' target='_blank'>Taylor et al. 2016</a>). To aide such robustness, a system should be able to characterize the probability of disagreement or local ambiguity around a concept in a concept space (<a href='http://doi.acm.org/10.1145/1273496.1273541' target='_blank'>Hanneke 2007</a>).", "mini_description": "Techniques to allow inductors to identify patches of uncertainty", "breakdowns": [{"id": "0020303010", "sub_nodes": [{"id": "00203030100", "title": "Scalable Oversight of Ambiguities", "description": "Methods for efficiently scaling up the ability of human overseers to supervise ML systems in scenarios where human feedback is expensive, thus clever strategies are needed for making most efficient use of such resources (<a href='https://intelligence.org/files/AlignmentMachineLearning.pdf' target='_blank'>Taylor et al. 2016</a>).", "mini_description": "Learning to spot ambiguities by leveraging human overseers", "links": [{"id": "0040100", "reason": "From Scalable Oversight of Ambiguities also see Scalable Oversight which describes a scalable disintermediated active learning."}]}, {"id": "00203030101", "title": "Bayesian Feature Selection", "description": "Given sufficient data, identifying and using the right features in order to model uncertainty well can be approached with bayesian structural analyses of the data (<a href='https://books.google.co.in/books?id=N1ViHNWZeQ0C' target='_blank'>Liu and Motoda 2007</a>, <a href='https://webdocs.cs.ualberta.ca/~dale/papers/uai06.pdf' target='_blank'>Guo and Schuurmans 2006</a>, <a href='https://intelligence.org/files/AlignmentMachineLearning.pdf' target='_blank'>Taylor et al. 2016</a>).", "mini_description": "Identifying and using the right features to model uncertainty well per a bayesian paradigm"}, {"id": "00203030102", "title": "Non-Bayesian Confidence Estimation", "description": "Most machine learning methods do not by default identify ambiguities well, either lacking such a concept entirely, or often poorly calibrated at doing so even if overall accuracy is high (<a href='https://intelligence.org/files/AlignmentMachineLearning.pdf' target='_blank'>Taylor et al. 2016</a>). It is well known that neural networks are often overconfident in their results, but there has been recent work to calibrate their confidences better (<a href='https://pdfs.semanticscholar.org/bee0/44c8e8903fb67523c1f8c105ab4718600cdb.pdf' target='_blank'>Goodfellow et al. 2015</a>, <a href='http://yosinski.com/media/papers/Nguyen__2015__CVPR__Deep_Neural_Networks_Are_Easily_Fooled.pdf' target='_blank'>Nguyen et al. 2015</a>). The techniques of conformal prediction attempt to produce well-calibrated sets of possible predictions (<a href='http://www.alrw.net/' target='_blank'>Vovk et al. 2005</a>).", "mini_description": "Non-bayesian techniques that allow learners to calibrate their confidence in their predictions"}, {"id": "00203030103", "title": "Active Learning", "description": "Judiciously asking humans to label appropriate examples in order to disambiguate among hypotheses, active learning, can be used when ambiguities are detected at a scale or speed where querying humans is practical (<a href='http://burrsettles.com/pub/settles.activelearning.pdf' target='_blank'>Settles 2010</a>, <a href='http://dx.doi.org/10.1561/2200000037' target='_blank'>Hanneke 2014</a>, <a href='http://papers.nips.cc/paper/6182-search-improves-label-for-active-learning.pdf' target='_blank'>Beygelzimer et al. 2016</a>, <a href='http://doi.acm.org/10.1145/130385.130417' target='_blank'>Seung et al. 1992</a>, <a href='http://doi.acm.org/10.1145/1553374.1553381' target='_blank'>Beygelzimer et al. 2009</a>, <a href='https://medium.com/ai-control/active-learning-for-opaque-powerful-predictors-94724b3adf06' target='_blank'>Christiano 2015g</a>, <a href='https://intelligence.org/files/AlignmentMachineLearning.pdf' target='_blank'>Taylor et al. 2016</a>).", "mini_description": "Judiciously asking humans to label appropriate examples in order to disambiguate among hypotheses"}, {"id": "00203030104", "title": "Realistic-Prior Design", "description": "If there is prior knowledge of a domain, bayesian priors can be designed especially to reflect its actual causal structure, reducing ambiguities (<a href='http://www.wiley.com/WileyCDA/WileyTitle/productCd-1119951518.html' target='_blank'>Congdon 2014</a>, <a href='https://intelligence.org/files/AlignmentMachineLearning.pdf' target='_blank'>Taylor et al. 2016</a>), and this may be automated. Knowledge-informed expectation propagation can likewise be formulated in non-bayesian formats (<a href='https://arxiv.org/pdf/1605.05110v1' target='_blank'>Xu et al. 2016</a>).", "mini_description": "Designing bayesian priors reflecting actual causal structure"}, {"id": "00203030105", "title": "Knows-What-It-Knows Learning", "description": "A thin layer of metaknowledge within an ensemble can provide an easy way to detect ambiguities. Knows-What-It-Knows learning maintains set of plausible hypotheses and when there is disagreement among components, outputs an Unclear state, prompting finite queries to humans to provide golden data (<a href='https://intelligence.org/files/AlignmentMachineLearning.pdf' target='_blank'>Taylor et al. 2016</a>, <a href='http://icml2008.cs.helsinki.fi/papers/627.pdf' target='_blank'>Li et al. 2008</a>, <a href='http://aclweb.org/anthology/P/P16/P16-1090.pdf' target='_blank'>Khani et al. 2016</a>, <a href='http://dblp.uni-trier.de/db/journals/jmlr/jmlrp19.html#SzitaS11' target='_blank'>Szita and Szepesvari 2011</a>).", "mini_description": "Structuring an ensemble with metaknowledge to enable the declaration of uncertainty"}, {"id": "00203030106", "title": "Robustness to Distributional Shift", "description": "For general robustness of a learning system, one aims to get a system trained on one distribution to perform reasonably when deployed under a different distribution (<a href='http://arxiv.org/abs/1606.06565' target='_blank'>Amodei et al. 2016</a>). Deep nets can even incorrectly output very different outputs for only trivially different inputs (<a href='https://cs.nyu.edu/~zaremba/docs/understanding.pdf' target='_blank'>Szegedy et al. 2013</a>). Where applicable, a potential solution is to train on additional subdistributions or variations of a distribution in order to have a more robust model (<a href='http://jmlr.org/proceedings/papers/v48/amodei16.pdf' target='_blank'>Amodei et al. 2016a</a>), but often the nature, direction, or breadth of the drift will be unknown in advance. There are, however, some approaches to learning with concept drift (<a href='http://dx.doi.org/10.1162/153244301753683726' target='_blank'>Herbster and Warmuth 2001</a>, <a href='http://dblp.uni-trier.de/db/conf/sbia/sbia2004.html#GamaMCR04' target='_blank'>Gama et al. 2004</a>) in an online context. Multiple research areas relevant to this capability, including change detection, anomaly detection, hypothesis testing, transfer learning, and others (<a href='http://arxiv.org/abs/1606.06565' target='_blank'>Amodei et al. 2016</a>). A variety of different statistical techniques can be employed to detect or mitigate these issues (<a href='http://arxiv.org/abs/1606.06565' target='_blank'>Amodei et al. 2016</a>). The method of moments, characterizing learned distributions via their top moments, is a partially-specified model technique for assessing instrumental variables (<a href='http://www.jmlr.org/proceedings/papers/v23/anandkumar12/anandkumar12.pdf' target='_blank'>Anandkumar et al. 2012</a>, <a href='http://larspeterhansen.org/wp-content/uploads/2016/10/Uncertainty-Outside-and-Inside-Economic-Models.pdf' target='_blank'>Hansen 2014</a>, <a href='http://arxiv.org/abs/1606.06565' target='_blank'>Amodei et al. 2016</a>). It enables unsupervised learning, including estimating latent variables, under conditional independence assumptions (<a href='http://larspeterhansen.org/wp-content/uploads/2016/11/Hansen-econometrica-GMM.pdf' target='_blank'>Hansen 1982</a>, <a href='http://woodypowell.com/wp-content/uploads/2012/03/4_NetworksandEconomicLife.pdf' target='_blank'>Powell and Smith-Doerr 1994</a>). For well-specified models, online-learning and testing of generative conditional independence structures is another promising avenue for distributional shift robustness (<a href='http://aclweb.org/anthology/J94-2001.pdf' target='_blank'>Merialdo 1994</a>, <a href='http://www.kamalnigam.com/papers/emcat-aaai98.pdf' target='_blank'>Nigam et al. 1998</a>, <a href='http://arxiv.org/abs/1606.06565' target='_blank'>Amodei et al. 2016</a>, Cozman and Cohen 2006, <a href='http://www.aclweb.org/anthology/P08-1100' target='_blank'>Liang and Klein 2008</a>, <a href='http://ieeexplore.ieee.org/document/6710159/' target='_blank'>Li and Zhou 2015</a>).", "mini_description": "Getting a system trained on one distribution to perform reasonably on a different distribution", "breakdowns": [{"id": "002030301060", "sub_nodes": [{"id": "0020303010600", "title": "Covariate Shift", "description": "When the distribution of the inputs used as predictors changes between the training and the testing or production stages, covariate shift analysis can inform as to the extent (<a href='http://arxiv.org/abs/1606.06565' target='_blank'>Amodei et al. 2016</a>). For well-specified models, online-learning retunings to parameters or to sample weights can be appropriate (<a href='http://arxiv.org/abs/1606.06565' target='_blank'>Amodei et al. 2016</a>). For highly expressive model families, though some work has been done, there still needs more exploration to see how well they can predict their out-of-sample performance (<a href='http://www.kernel-machines.org/publications/pdfs/0701907.pdf' target='_blank'>Hofmann et al. 2008</a>, <a href='http://www.sciencedirect.com/science/article/pii/S0019995864902232/pdf?md5=528dcc7a51a90f7254fe06f76ea5f007&pid=1-s2.0-S0019995864902232-main.pdf' target='_blank'>Solomonoff 1964</a>, <a href='http://www.sciencedirect.com/science/article/pii/S0019995864901317' target='_blank'>Solomonoff 1964a</a>, <a href='https://pdfs.semanticscholar.org/6eed/f0a4fe861335f7f7664c14de7f71c00b7932.pdf' target='_blank'>Graves et al. 2014</a>, <a href='http://arxiv.org/abs/1511.08228' target='_blank'>Kaiser and Sutskever 2015a</a>, <a href='http://papers.nips.cc/paper/6500-deep-exploration-via-bootstrapped-dqn.pdf' target='_blank'>Osband et al. 2016</a>, <a href='https://faculty.fuqua.duke.edu/~dbbrown/bio/papers/bertsimas_brown_caramanis_11.pdf' target='_blank'>Bertsimas et al. 2011</a>). The sample selection bias may also be able to be leveraged to this end (<a href='http://dblp.uni-trier.de/db/conf/aistats/aistats2016.html#ChenMLZ16' target='_blank'>Chen et al. 2016</a>).", "mini_description": "Techniques to adapt to changes to which inputs or parameters affect predictions and their weights in doing so"}, {"id": "0020303010601", "title": "Unsupervised Risk Estimation", "description": "Given a model and unlabeled data from a test distribution, unsupervised risk estimation (<a href='http://dl.acm.org/citation.cfm?id=1756006.1859895' target='_blank'>Donmez et al. 2010</a>, <a href='https://papers.nips.cc/paper/6201-unsupervised-risk-estimation-using-only-conditional-independence-structure.pdf' target='_blank'>Steinhardt and Liang 2016</a>, <a href='http://cs.stanford.edu/~jsteinhardt/publications/risk-estimation/preprint.pdf' target='_blank'>Steinhardt and Liang 2016a</a>, <a href='http://www.jstor.org/stable/2346806' target='_blank'>Dawid and Skene 1979</a>, <a href='http://papers.nips.cc/paper/5431-spectral-methods-meet-em-a-provably-optimal-algorithm-for-crowdsourcing' target='_blank'>Zhang et al. 2014a</a>, <a href='http://www.jmlr.org/papers/volume12/balasubramanian11a/balasubramanian11a.pdf' target='_blank'>Balasubramanian et al. 2011</a>) can aide estimating the labeled risk of the model (<a href='http://auai.org/uai2014/proceedings/individuals/313.pdf' target='_blank'>Platanios et al. 2014</a>, <a href='http://arxiv.org/abs/1606.06565' target='_blank'>Amodei et al. 2016</a>, <a href='http://www.jmlr.org/proceedings/papers/v38/jaffe15.pdf' target='_blank'>Jaffe et al. 2015</a>).", "mini_description": "Given a model and unlabeled data from a test distribution, estimating the labeled risk of the model"}, {"id": "0020303010602", "title": "Causal Identification", "description": "Techniques from econometrics can be used to estimate causal structure and instrumental variables from data (<a href='http://arxiv.org/abs/1606.06565' target='_blank'>Amodei et al. 2016</a>, <a href='http://www.jstor.org/stable/1907619' target='_blank'>Sargan 1958</a>, <a href='http://www.jstor.org/stable/2983930' target='_blank'>Sargan 1959</a>).", "mini_description": "Techniques from econometrics to estimate causal structure and instrumental variables", "links": [{"id": "004010406", "reason": "From Causal Identification also see Causal Accounting which would be a consumer of such determinations."}]}, {"id": "0020303010603", "title": "Limited-Information Maximum Likelihood", "description": "Limited-information maximum likelihood is a technique from econometrics for accomodating partially specified models, and partial specification allows for broader and more robust coverage of test distributions (<a href='http://arxiv.org/abs/1606.06565' target='_blank'>Amodei et al. 2016</a>, <a href='http://dx.doi.org/10.1214/aoms/1177730090' target='_blank'>Anderson and Rubin 1949</a>, <a href='http://www.jstor.org/stable/2236607' target='_blank'>Anderson and Rubin 1950</a>).", "mini_description": "A technique from econometrics for accomodating partially specified models"}, {"id": "0020303010604", "title": "Active Calibration", "description": "Active calibration is a technique that helps to pinpoint the structural aspects of most uncertainty within a model (<a href='http://arxiv.org/abs/1606.06565' target='_blank'>Amodei et al. 2016</a>, <a href='http://papers.nips.cc/paper/5860-on-the-job-learning-with-bayesian-decision-theory.pdf' target='_blank'>Werling et al. 2015</a>, <a href='http://aclweb.org/anthology/P/P16/P16-1090.pdf' target='_blank'>Khani et al. 2016</a>). It is a way of obtaining calibration in structured output settings, as a followup action to recognizing inputs being out-of-distribution (<a href='http://papers.nips.cc/paper/5658-calibrated-structured-prediction.pdf' target='_blank'>Kuleshov and Liang 2015</a>).", "mini_description": "Pinpointing aspects of a structure that the model is uncertain about"}, {"id": "0020303010605", "title": "Reachability Analysis", "description": "Reachability analysis is a technique for when a system model is known, for bounding controls and disturbances within which the system is guaranteed to remain safe (<a href='http://dx.doi.org/10.1016/S0005-1098(98)00193-9' target='_blank'>Lygeros et al. 1999</a>, <a href='https://www.cs.ubc.ca/~mitchell/Papers/publishedIEEEtac05.pdf' target='_blank'>Mitchell et al. 2005</a>). This aides alleviating situational ambiguity (<a href='http://arxiv.org/abs/1606.06565' target='_blank'>Amodei et al. 2016</a>).", "mini_description": "Bounding controls and disturbances in which system is guaranteed to remain safe if system model is known"}, {"id": "0020303010606", "title": "Robust Policy Improvement", "description": "Once ambiguities are recognized, a potential response is robust policy improvement, finding a policy that minimizes the downside, or alternatively maximizes the utility of the worst case scenario, over that set of ambiguities or distribution of uncertainties (<a href='http://pubsonline.informs.org/doi/abs/10.1287/moor.1120.0566' target='_blank'>Wiesemann et al. 2013</a>, <a href='http://arxiv.org/abs/1606.06565' target='_blank'>Amodei et al. 2016</a>).", "mini_description": "Finding a policy that minimizes the downside, or maximizes the utility, of the worst case scenario possible given specific ambiguities"}, {"id": "0020303010607", "title": "Counterfactual Reasoning", "description": "Consideration and reasoning about counterfactual scenarios, or what would have happened if the world were different in a certain way (Neyman 1923, <a href='http://dx.doi.org/10.1037/h0037350' target='_blank'>Rubin 1974</a>, <a href='http://dx.doi.org/10.1214/09-SS057' target='_blank'>Pearl 2009a</a>), based on logical entailment but applied in a machine learning context (<a href='http://leon.bottou.org/papers/tr-bottou-2012' target='_blank'>Bottou et al. 2012</a>, <a href='http://jmlr.org/papers/volume15/peters14a/peters14a.pdf' target='_blank'>Peters et al. 2014</a>, <a href='http://arxiv.org/abs/1606.06565' target='_blank'>Amodei et al. 2016</a>, <a href='http://dl.acm.org/citation.cfm?id=2789272.2886805' target='_blank'>Swaminathan and Joachims 2015</a>, <a href='https://arxiv.org/pdf/1605.03661.pdf' target='_blank'>Johansson et al. 2016</a>, Shalit et al. 2016) enables more accurate, comprehensive, and aligned enumeration and evaluation of options and causal learning. It may be possible to use Garrabrant inductors to predict counterfactuals (<a href='https://agentfoundations.org/item?id=1054' target='_blank'>Benson-Tilsen 2016</a>).", "mini_description": "Techniques to consider, based on logical entailment, what would have happened if the world were different in a certain way, to explore alternatives", "links": [{"id": "0000001", "reason": "From Counterfactual Reasoning also see Theory of Counterfactuals as there are ongoing theoretical developments in how possibilities should be enumerated and modeled."}, {"id": "0000100", "reason": "From Counterfactual Reasoning also see Decision Theory for a broader treatment of counterfactuals oriented at goal stability."}, {"id": "000010000", "reason": "From Counterfactual Reasoning also see Logical Counterfactuals which aims to progress methods for enumerating them."}]}, {"id": "0020303010608", "title": "ML with Contracts", "description": "To minimize ambiguities at design time, one can construct machine learning systems that satisfy a well-defined contract on their behavior, like in software design (<a href='https://www.microsoft.com/en-us/research/video/symposium-algorithms-among-us-percy-liang/' target='_blank'>Liang 2015</a>, <a href='http://icml.cc/2015/invited/LeonBottouICML2015.pdf' target='_blank'>Bottou 2015</a>, <a href='http://arxiv.org/abs/1606.06565' target='_blank'>Amodei et al. 2016</a>, <a href='http://research.google.com/pubs/archive/43146.pdf' target='_blank'>Sculley et al. 2014</a>).", "mini_description": "Constructing ML components and systems that satisfy well-defined contracts on their behaviors", "links": [{"id": "001"}, {"id": "0010000", "reason": "From ML with Contracts also see Verified Component Design Approaches which can benefit from these techniques."}, {"id": "0010200"}]}, {"id": "0020303010609", "title": "Model Repair", "description": "Another potential response to ambiguity identification is model repair, altering the trained model to ensure that certain desired safety properties will still hold (<a href='https://4caf2f9f-a-62cb3a1a-s-sites.googlegroups.com/site/wildml2016/ghosh16trusted.pdf' target='_blank'>Ghosh et al. 2016</a>, <a href='http://arxiv.org/abs/1606.06565' target='_blank'>Amodei et al. 2016</a>).", "mini_description": "Altering a trained model to ensure that certain desired safety properties hold"}]}], "links": [{"id": "00203000100"}]}]}], "links": [{"id": "002030201"}, {"id": "0030400", "reason": "From Inductive Ambiguity Identification also see Adversarial ML as that also deals with robustness to changes in input distributions, introduced maliciously and with potentially worst-case scenarios."}]}, {"id": "002030302", "title": "Resource-Aware Reasoning", "description": "Given unbounded time and memory, an AI system would be able to figure out both the full distribution of reasonable inferences and the full set of logical entailments that stem from the facts it already knows. Realistic agents, however, have time and memory constraints, and must manage those resources in their activities, including their computations (<a href='http://www.cs.cornell.edu/~rafael/papers/compdec.pdf' target='_blank'>Halpern and Pass 2011</a>, <a href='http://futureoflife.org/data/documents/research_priorities.pdf' target='_blank'>Russell et al. 2015</a>).", "mini_description": "Methods for agents to manage their time, memory, compute, and other resources in the process of inference, abduction, and (potentially approximate) deduction", "breakdowns": [{"id": "0020303020", "sub_nodes": [{"id": "00203030200", "title": "Decision Under Bounded Computational Resources", "description": "Properly budgeting and prioritizing finite resources in order to accomplish any other goals will lead to more robust and higher quality actions (<a href='http://futureoflife.org/data/documents/research_priorities.pdf' target='_blank'>Russell et al. 2015</a>). Relatively applied explorations of reasoning and decision making under bounded computational resources have been done (<a href='ftp://131.107.65.22/pub/ejh/u87.pdf' target='_blank'>Horvitz 1987</a>, <a href='https://www.jair.org/media/133/live-133-1446-jair.pdf' target='_blank'>Russell and Subramanian 1995</a>), but there is also more theoretical progress needed (<a href='https://www.cs.cmu.edu/~sandholm/complexity_of_metareasoning.ijcai03.pdf' target='_blank'>Conitzer and Sandholm 2003</a>, <a href='https://mitpress.mit.edu/books/metareasoning' target='_blank'>Conitzer 2011</a>). One class of approaches prominently considers correlations between the bounded agent and its environment (<a href='http://www.cs.cornell.edu/~rafael/papers/compdec.pdf' target='_blank'>Halpern and Pass 2011</a>). More explicit treatments of practical decision theory under such agents also follows from this (<a href='http://www.cs.cornell.edu/home/halpern/papers/rbdec.pdf' target='_blank'>Halpern et al. 2014</a>). Ongoing progress in unifying structured logic and probability for generalized empirical inference (<a href='https://people.eecs.berkeley.edu/~russell/papers/cacm15-oupm.pdf' target='_blank'>Russell 2015</a>) will aid this capability.", "mini_description": "Reasoning and decision making under bounded computational resources", "links": [{"id": "002030103", "reason": "From Decision Under Bounded Computational Resources also see Metareasoning as abstract consideration of logic flow can be used to modulate tactics and resource usage."}]}, {"id": "00203030201", "title": "Logical Induction", "description": "", "mini_description": "Modeling the uncertainty around the outcome of long-running calculations", "breakdowns": [{"id": "002030302010", "sub_nodes": [{"id": "0020303020100", "title": "Logical Priors", "description": "The question of how to arrive at a reasonable prior for a logical sentence presents itself when starting induction about logic (<a href='http://intelligence.org/files/QuestionsLogicalUncertainty.pdf' target='_blank'>Soares and Fallenstein 2014</a>). While providing good starting points, it may actually be much less important in the limit (<a href='http://intelligence.org/files/LogicalInduction.pdf' target='_blank'>Garrabrant et al. 2016</a>). In order to address arbitrarily deep combinations of logical uncertainties, however, additional research is needed on what satisfactory set of priors over logical sentences a bounded reasoner can approximate (<a href='http://ict.usc.edu/pubs/Logical%20Prior%20Probability.pdf' target='_blank'>Demski 2012</a>, <a href='intelligence.org/files/Non-Omniscience.pdf' target='_blank'>Christiano 2014b</a>, <a href='http://intelligence.org/files/TechnicalAgenda.pdf' target='_blank'>Soares and Fallenstein 2014a</a>) in practice.", "mini_description": "The set of bayesian priors over logical statements that a bounded reasoner can approximate"}, {"id": "0020303020101", "title": "Impossible Possibilities", "description": "Exploring how deductively limited reasoners can perform counterpossible reasoning, or counterfactual reasoning from impossible antecedents (<a href='https://plato.stanford.edu/archives/win2013/entries/impossible-worlds/' target='_blank'>Berto 2013</a>), will help progress theory and usage of both counterfactuals and logical induction (<a href='http://intelligence.org/files/TechnicalAgenda.pdf' target='_blank'>Soares and Fallenstein 2014a</a>). This is equivalent to asking how deductively limited reasoners can approximate reasoning according to a probability distribution on complete theories of logic (<a href='intelligence.org/files/Non-Omniscience.pdf' target='_blank'>Christiano 2014b</a>). Early theoretical work has considered counterpossible reasoning in deterministic settings, but further development can aid agent theory and decision theoretic foundations (<a href='https://intelligence.org/files/CounterpossibleReasoning.pdf' target='_blank'>Soares and Fallenstein 2015a</a>).", "mini_description": "Exploring how deductively limited reasoners can perform counterfactual reasoning from impossible antecedents"}]}], "links": [{"id": "0000000"}, {"id": "00001010001"}, {"id": "00306"}]}]}], "links": [{"id": "0000000", "reason": "From Resource-Aware Reasoning also see Logical Uncertainty as that foundational theoretical topic, which is still evolving, really underlies much of this topic to a degree not traditionally acknowledged."}, {"id": "0000004"}, {"id": "0000200"}]}]}], "links": [{"id": "002020001"}]}, {"id": "0020304", "title": "Symbolic-Subsymbolic Integration", "description": "Integration between symbolic, or explicit discrete models, and subsymbolic, or implicit numerical models (<a href='http://ceur-ws.org/Vol-1583/CoCoNIPS_2015_paper_10.pdf' target='_blank'>Carmona and Riedel 2015</a>, <a href='http://phys.csail.mit.edu/papers/11.pdf' target='_blank'>Chang et al. 2016</a>, <a href='http://arxiv.org/abs/1611.01855' target='_blank'>Parisotto et al. 2016</a>, <a href='http://arxiv.org/abs/1611.01423' target='_blank'>Allamanis et al. 2016</a>), would enable meaning and value to propagate between different architectures or modalities more fluidly (<a href='http://arxiv.org/abs/1609.05518' target='_blank'>Garnelo et al. 2016</a>, <a href='https://publik.tuwien.ac.at/files/PubDat_166316.pdf' target='_blank'>Velik and Bruckner 2008</a>, <a href='https://pdfs.semanticscholar.org/d275/94ddf7e7849a87f940b25db9849d668a93b3.pdf' target='_blank'>Winograd 2001</a>, <a href='http://www.springer.com/in/book/9789462390263' target='_blank'>Goertzel et al. 2014</a>), and would make conglomerations of different AI and ML components in agents more robust. This integration facilitates the grounding of concepts as well, since sensory data, sensory processing, and actuation processing all have significant subsymbolic components (<a href='http://arxiv.org/abs/1609.05518' target='_blank'>Garnelo et al. 2016</a>, <a href='http://dx.doi.org/10.1007/978-3-319-41649-6_11' target='_blank'>Thorisson et al. 2016a</a>).", "mini_description": "Integration between symbolic, or explicit discrete models, and subsymbolic, or implicit numerical knowledge representation models", "breakdowns": [{"id": "00203040", "sub_nodes": [{"id": "002030400", "title": "Use of Structured Knowledge In Subsymbolic Systems", "description": "The meaningful use of symbolic concepts, relationships, constraints, and rules in subsymbolic, typically statistical, e.g. neural net, contexts can help orient deep learning based agents with concepts and relationships its operators care about (<a href='http://arxiv.org/abs/1609.01926' target='_blank'>Carmantini et al. 2016</a>, <a href='http://arxiv.org/abs/1609.05518' target='_blank'>Garnelo et al. 2016</a>, <a href='https://publik.tuwien.ac.at/files/PubDat_166316.pdf' target='_blank'>Velik and Bruckner 2008</a>, <a href='http://arxiv.org/abs/1611.01855' target='_blank'>Parisotto et al. 2016</a>).", "mini_description": "The use of symbolic concepts, relationships, constraints, and rules within primarily statistical or numerical contexts"}, {"id": "002030401", "title": "Use of Subsymbolic Processing In Symbolic Systems", "description": "Systems that aggregate and integrate disparate components using explicit semantic interlingua can use subsymbolic processing for determining what structures, statements, and connections should exist and how they should be weighted (<a href='https://ai2-s2-pdfs.s3.amazonaws.com/a2b0/1b08e1ebbc2d42399e58282d615be9bb97ee.pdf' target='_blank'>Hatzilygeroudis and Prentzas 2004</a>), and can do novel methods of reasoning over them (<a href='http://arxiv.org/abs/1607.01426' target='_blank'>Das et al. 2016</a>). Though symbolic-first systems have become less popular of late, they are more amenable to integration with the systems that already run society.", "mini_description": "The use of numerical or statistical implicit concepts, reasoning, and computation within systems that use explicit concepts as an interlingua between components", "links": [{"id": "0040104", "reason": "From Use of Subsymbolic Processing In Symbolic Systems also see Monitoring as such architectures can benefit interpretability."}]}]}], "links": [{"id": "0040104"}]}]}], "links": [{"id": "0000001"}, {"id": "0040103"}]}, {"id": "00204", "title": "Psychological Analogues", "description": "Artificial intelligences, including organizations and AI agents, and natural intelligences, including animals and humans, share a number of features in common (<a href='http://www.cambridge.org/us/academic/subjects/psychology/cognition/cognition-and-multi-agent-interaction-cognitive-modeling-social-simulation?format=HB&isbn=9780521839648' target='_blank'>Sun 2005</a>). Information processing, knowledge processing, planning, and action are at the core of all of these classes of intelligent agent. In many cases, even when architectures are appreciably different, similar dynamics can occur, both in the proper and improper processing of knowledge (<a href='http://probmods.org/v2' target='_blank'>Goodman and Tenenbaum 2016</a>, <a href='http://cognet.mit.edu/book/grounding-social-sciences-cognitive-sciences' target='_blank'>Sun 2012</a>). Though interesting, what motivates cross-modality analysis is that establishing correlates, analogues, or metaphors that run deep between natural cognition and artificial cognition might enable us to harvest and transfer insights in both directions between psychology (even organizational psychology and animal psychology) and artificial intelligence safety research.", "mini_description": "Establishing correlates between natural cognition and artificial cognition to transfer insights in both directions", "breakdowns": [{"id": "002040", "sub_nodes": [{"id": "0020400", "title": "Cognitive Parallels", "description": "One can more broadly analyze human cognition and developmental psychology in terms of machine learning algorithms and vice versa to potentially elucidate fruitful avenues of research (<a href='http://aiweb.cs.washington.edu/research/projects/aiweb/media/papers/cogsci2011.pdf' target='_blank'>Baker et al. 2011</a>, <a href='http://probmods.org/v2' target='_blank'>Goodman and Tenenbaum 2016</a>, <a href='http://www.cell.com/trends/cognitive-sciences/pdf/S1364-6613(16)30124-3.pdf' target='_blank'>Jara-Ettinger et al. 2016</a>, <a href='https://web.stanford.edu/~ngoodman/papers/tkgg-science11-reprint.pdf' target='_blank'>Tenenbaum et al. 2011</a>, <a href='https://mindmodeling.org/cogsci2013/papers/0141/index.html' target='_blank'>Jara-Ettinger et al. 2013</a>). In so doing, the field should want to establish which general classes of analogues can be valid, for which architectures, and to what extent (<a href='http://cognet.mit.edu/book/grounding-social-sciences-cognitive-sciences' target='_blank'>Sun 2012</a>), and this can benefit from additional research.", "mini_description": "Establishing which general classes of analogues can be valid", "links": [{"id": "002020005", "reason": "From Cognitive Parallels also see Drives and Affect as regards attempts at cognitive-inspired prosocial mechanisms."}]}, {"id": "0020401", "title": "Developmental Psychology", "description": "Within the field of artificial general intelligence, and with some precursor machine learning techniques, parallels are drawn between human early childhood learning and development and seed artificial cognitive systems that are either grown or learned (<a href='http://people.idsia.ch/~steunebrink/Publications/AGI16_growing_recursive_self-improvers.pdf' target='_blank'>Steunebrink et al. 2016</a>, <a href='https://mindmodeling.org/cogsci2013/papers/0141/index.html' target='_blank'>Jara-Ettinger et al. 2013</a>, <a href='https://web.stanford.edu/~ngoodman/papers/tkgg-science11-reprint.pdf' target='_blank'>Tenenbaum et al. 2011</a>, <a href='http://www.springer.com/in/book/9789462390263' target='_blank'>Goertzel et al. 2014</a>). Questions remain open as to how far this metaphor extends, particularly with respect to architectures that are not intended to be biologically plausible.", "mini_description": "Utilizing parallels between human early childhood learning and development and grown, learned, or seed artificial cognitive systems", "links": [{"id": "0020403", "reason": "From Developmental Psychology also see Whole Brain Emulation Safety as questions also remain open as to how far such metaphors extend even with architectures that are meant to be biologically accurate."}]}, {"id": "0020402", "title": "Dysfunctional Systems of Thought", "description": "Within each corresponding pair, generalizing some key dynamics of the problem, and optionally some subset of the treatment for the psychological condition, may lead to insights applicable to such issues with artificial agents. A particularly promising application of analyzing these natural-artificial analogues is to mine insights about analogous issues from each domain that may be applicable to the other domain (Bach 2016). Indeed many of the dysfunctions that can affect naturally developed minds have close counterparts in synthetic intelligences, and vice versa (<a href='http://link.springer.com/content/pdf/10.1007%2Fs11948-016-9783-0.pdf' target='_blank'>Ashrafian 2016</a>, Mallah 2017). By characterizing these correspondences more specifically (<a href='http://cognet.mit.edu/book/grounding-social-sciences-cognitive-sciences' target='_blank'>Sun 2012</a>, <a href='http://people.idsia.ch/~ring/AGI-2011/Paper-B.pdf' target='_blank'>Ring and Orseau 2011</a>, <a href='http://ac.els-cdn.com/S002251931500106X/1-s2.0-S002251931500106X-main.pdf?_tid=6f1c2bc8-c4e7-11e6-972c-00000aacb35e&acdnat=1482041076_291e3fa094036bcad0f5601cfc975933' target='_blank'>Nikolic 2014</a>, <a href='https://papers.nips.cc/paper/799-catastrophic-interference-in-connectionist-networks-can-it-be-predicted-can-it-be-prevented.pdf' target='_blank'>French 1993</a>, <a href='https://arxiv.org/pdf/1606.04671v3' target='_blank'>Rusu et al. 2016</a>), researchers might be able to find insights from the field of psychology and apply them to AI, or, in the interest of furthering science, helping people, and strengthening bridges across these fields, perhaps find insights from AI safety and translate them into insights psychologists can use.", "mini_description": "Specific disorder or symptom analogues and common features between natural and artificial versions"}, {"id": "0020403", "title": "Whole Brain Emulation Safety", "description": "Psychological analogues to AI are relevant not only for engineered AI but also for scanned and emulated biological brains. Though neuroscientists understand much of what occurs in such detailed and bioplausible neural networks, that understanding is at the level of annotations rather than being able to produce a detailed explanatory or generative model. This may lead us to treat the emulation as a black box, and so psychology-derived AI safety approaches and characterizations would become quite relevant (<a href='http://www.philosophy.ox.ac.uk/__data/assets/pdf_file/0019/3853/brain-emulation-roadmap-report.pdf' target='_blank'>Sandberg and Bostrom 2008</a>).", "mini_description": "Psychology-inspired safety approaches for blackbox brain emulation techniques", "links": [{"id": "0020401"}]}]}], "links": [{"id": "00202000101"}, {"id": "002020005"}, {"id": "002020202", "reason": "From Psychological Analogues also see Operator Modeling which attempts to model humans to understand what they want and what they'll do, a more direct safety application of psychological modeling."}]}, {"id": "00205", "title": "Testing and Quality Assurance", "description": "Traditional software quality assurance techniques will not scale well with highly capable AI, but they can still be deployed to provide early and efficient warnings about robustness or safety issues, and analogues to these techniques can be helpful. Unit tests, for instance, can perform some basic checks of sanity and deception. To extend such tests toward the realm of general-purpose agents, a task theory and framework is necessary (<a href='http://dx.doi.org/10.1007/978-3-319-41649-6_12' target='_blank'>Thorisson et al. 2016</a>). These analogues will not be nearly sufficient to maintain safety in the traditional quality control paradigm, hence the other safety techniques here (<a href='http://intelligence.org/files/TechnicalAgenda.pdf' target='_blank'>Soares and Fallenstein 2014a</a>).", "mini_description": "Finding scalable analogues to software engineering quality assurance paradigms"}]}]}, {"id": "003", "title": "Security", "description": "There are a wide variety of ways AI can go wrong; many of them involve novel endogenous risks, but many of them also involve bad faith corruption or attacks at any of a multitude of levels (<a href='http://www.aaai.org/ocs/index.php/WS/AAAIW16/paper/download/12566/12356' target='_blank'>Yampolskiy 2016b</a>, <a href='http://futureoflife.org/data/documents/research_priorities.pdf' target='_blank'>Russell et al. 2015</a>).", "mini_description": "Applying cybersecurity paradigms and techniques to AI-specific challenges", "breakdowns": [{"id": "0030", "sub_nodes": [{"id": "00300", "title": "Standard IT Security", "description": "Cybersecurity is applicable and important to any kind of software, and artificial intelligence is no exception (<a href='http://futureoflife.org/data/documents/research_priorities.pdf' target='_blank'>Russell et al. 2015</a>). AI failures can come about through lapses in security (<a href='http://arxiv.org/abs/1610.07997' target='_blank'>Yampolskiy 2016a</a>, <a href='http://dx.doi.org/10.1007/s10994-010-5188-5' target='_blank'>Barreno et al. 2010</a>). Unique to AI are the specific vulnerable or sensitive targets of attack within an AI system, and the active ways of defending those (<a href='http://dx.doi.org/10.1080/0952813X.2014.895114' target='_blank'>Yampolskiy 2014</a>, <a href='http://arxiv.org/abs/1602.02697' target='_blank'>Papernot et al. 2016</a>).", "mini_description": "Analyzing particularly vulnerable or sensitive targets of attack within an AI system and defending them", "breakdowns": [{"id": "003000", "sub_nodes": [{"id": "0030000", "title": "Verified Downstack Software", "description": "Because software verification and correct-by-construction are such powerful techniques to reduce the kinds of implementation issues that are often exploited during security events, utilizing a stack of as much verified software, as high as is practical in the stack, would be prudent. Even just very well-tested underlying platform can provide similar benefits, e.g. the DARPA SAFE program aims to build an integrated hardware-software system with a flexible metadata rule engine (<a href='http://repository.upenn.edu/cgi/viewcontent.cgi?article=1707&context=cis_papers' target='_blank'>DeHon et al. 2011</a>), on which can be built memory safety, fault isolation, and other protocols that could improve security by preventing exploitable flaws (<a href='http://futureoflife.org/data/documents/research_priorities.pdf' target='_blank'>Russell et al. 2015</a>).", "mini_description": "Utilizing a stack of verified software as high as is practical", "links": [{"id": "00100"}, {"id": "0010000"}, {"id": "0020106"}]}, {"id": "0030001", "title": "Utility Function Security", "description": "It would make sense to establish extra privileges and protections around modifications of an explicit or implicit utility function of an AI, given that it's a highly centralized and leveraged point (<a href='http://dx.doi.org/10.1080/0952813X.2014.895114' target='_blank'>Yampolskiy 2014</a>).", "mini_description": "Establishing extra privileges and protections around modifications of an explicit or implicit utility function"}, {"id": "0030002", "title": "AI to Support Security", "description": "For advanced and general purpose agents, it should be quite possible to utilize its own advanced automation and intelligence analysis capabilities to, analyze potential malware (<a href='http://www.mlsec.org/malheur/docs/malheur-jcs.pdf' target='_blank'>Rieck et al. 2011</a>), protect itself against active exploits, detect intrusions (<a href='https://www.cerias.purdue.edu/assets/pdf/bibtex_archive/98-11.pdf' target='_blank'>Lane 2000</a>), and automatically check for potential exploits it may be vulnerable to. As AI matures, AIs one wishes to keep good will also need to protect themselves against not just human hackers, but also AIs that have either become, or were designed to be, bad (<a href='https://arxiv.org/ftp/arxiv/papers/1605/1605.02817.pdf' target='_blank'>Pistono and Yampolskiy 2016</a>). Likewise, both context-aware and narrow AIs will need to protect the security and privacy of others they are serving, so an understanding and application of differential privacy (<a href='http://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/viewFile/12129/11885' target='_blank'>Zhang et al. 2016a</a>, <a href='http://www.aaai.org/Library/AAAI/aaai16contents.php' target='_blank'>Tossou and Dimitrakakis 2016</a>) practices would be called for (<a href='http://futureoflife.org/data/documents/research_priorities.pdf' target='_blank'>Russell et al. 2015</a>).", "mini_description": "Utilizing advanced automated intelligence capabilities to protect the AI system against exploits and intrusion"}, {"id": "0030003", "title": "Security-Validated Software Engineering", "description": "When creating advanced, powerful, or high-stakes AI systems, security-validated software engineering practices are called for. Given that vulnerabilities can use a growing array of unexpected side-channels (<a href='http://arxiv.org/abs/1606.05915' target='_blank'>Guri et al. 2016</a>), and given that advanced agents can be more creative about side-channel use than people are, a side-channel vulnerability assessment would be important to such validation, yet it wouldn't be able to provide any guarantees.", "mini_description": "Including side-channel vulnerability assessment in the validation process"}]}]}, {"id": "00301", "title": "Red Team Analysis", "description": "Red team analysis uses an adversarial entity or team to probe and identify vulnerabilities in the system of interest, usually in a blackbox context. The recent DARPA Cyber Grand Challenge likewise had various automated adversarial teams whitebox scan their own systems and blackbox scan their opponents, patch themselves, develop exploits, and penetrate opponent systems, all in a matter of minutes rather than the weeks to months that humans would normally take for the same steps (<a href='https://www.researchgate.net/profile/Jim_Alves-Foss/publication/286490027_The_DARPA_cyber_grand_challenge_A_competitor%27s_perspective/links/56778d1908ae502c99d30b3c' target='_blank'>Song and Alves-Foss 2015</a>, <a href='http://www.cs.tufts.edu/comp/116/archive/fall2016/jmusca.pdf' target='_blank'>Musca 2016</a>).", "mini_description": "Using an adversarial entity or team to probe and identify vulnerabilities in the system of interest, usually in a blackbox context", "breakdowns": [{"id": "003010", "sub_nodes": [{"id": "0030100", "title": "Penetration Testing", "description": "Penetration testing specifically uses an adversarial entity to attempt to break into the system of interest in various ways and to go as deeply into its systems as possible. Agents can analyze the vulnerabilities of other systems and agents in this way (<a href='https://homes.cs.washington.edu/~mernst/pubs/machlearn-errors-icse2004.pdf' target='_blank'>Brun and Ernst 2004</a>), and can correlate this analysis with analyses of the likely source code generating those vulnerabilities.", "mini_description": "Using an adversarial entity to attempt to break into the system of interest in various ways"}]}], "links": [{"id": "00101", "reason": "From Red Team Analysis also see Automated Vulnerability Finding which addresses finding vulnerabilities in whiteboxes rather than blackboxes."}]}, {"id": "00302", "title": "Tripwires", "description": "To catch internal misbehavior, one can introduce plausible vulnerabilities, with alert triggers, that an agent can exploit and will only do so if its reward function is being gamed (<a href='https://pdfs.semanticscholar.org/d7a4/adb20a879e89fc12600c84dff0cb69fd7d58.pdf' target='_blank'>Babcock et al. 2016</a>, <a href='http://arxiv.org/abs/1606.06565' target='_blank'>Amodei et al. 2016</a>). This technique is also useful for catching intrusions by external threats as it is classically applied.", "mini_description": "Introduce plausible vulnerabilities, with corresponding hidden triggers and reporting, that an agent would exploit if the reward function is being gamed", "links": [{"id": "00200010205"}, {"id": "00201"}]}, {"id": "00303", "title": "Containment", "description": "Since generally intelligent agents represent varied and storied risks, it is prudent to test such agents in a confined environment (<a href='http://futureoflife.org/data/documents/research_priorities.pdf' target='_blank'>Russell et al. 2015</a>) before releasing them into the wild. Building and using containers where such tests on powerful agents can be done safely and reliably (<a href='http://cecs.louisville.edu/ry/LeakproofingtheSingularity.pdf' target='_blank'>Yampolskiy 2012</a>, <a href='https://pdfs.semanticscholar.org/d7a4/adb20a879e89fc12600c84dff0cb69fd7d58.pdf' target='_blank'>Babcock et al. 2016</a>) is challenging, and must account for operator psychology. If general containment proves too difficult, it may be wise to designing the AI and a specialized container for it in parallel  (<a href='https://global.oup.com/academic/product/superintelligence-9780199678112' target='_blank'>Bostrom 2014</a>).", "mini_description": "Building and using containers where tests on powerful agents can be done safely and reliably", "links": [{"id": "0020106"}]}, {"id": "00304", "title": "Handling Improper External Behavior", "description": "Third parties, including other AIs and humans, may be malevolent (<a href='https://arxiv.org/ftp/arxiv/papers/1605/1605.02817.pdf' target='_blank'>Pistono and Yampolskiy 2016</a>), adversarial, or incompetent, and there are some techniques to detect and protect against them.", "mini_description": "Techniques to detect and protect against third parties, including other AIs and humans, that may be malevolent, adversarial, or incompetent", "breakdowns": [{"id": "003040", "sub_nodes": [{"id": "0030400", "title": "Adversarial ML", "description": "An agent is more robust when one can guarantee its good behavior even when an adversary picks test or production data for it, from a different distribution than training data, aiming to make the agent fail (<a href='https://intelligence.org/files/AlignmentMachineLearning.pdf' target='_blank'>Taylor et al. 2016</a>). It is increasingly common that machine learning systems can be manipulated (<a href='http://arxiv.org/abs/1602.02697' target='_blank'>Papernot et al. 2016</a>, <a href='https://pdfs.semanticscholar.org/bee0/44c8e8903fb67523c1f8c105ab4718600cdb.pdf' target='_blank'>Goodfellow et al. 2015</a>, <a href='http://arxiv.org/abs/1610.08401' target='_blank'>Moosavi-Dezfooli et al. 2016</a>), and this can even happen with real-world examples (<a href='https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45471.pdf' target='_blank'>Kurakin et al. 2016</a>).", "mini_description": "Structuring learning to be robust to when adversaries pick test data from different distributions than training data", "breakdowns": [{"id": "00304000", "sub_nodes": [{"id": "003040000", "title": "Dealing with Adversarial Testing", "description": "In online scenarios where actual adversaries may provide very skewed or poisoned data, it can be imperative that the agent not be corrupted by that (<a href='http://doi.acm.org/10.1145/2046684.2046692' target='_blank'>Huang et al. 2011</a>, <a href='http://ijoc.org/index.php/ijoc/article/viewFile/6277/1804' target='_blank'>Neff and Nagy 2016</a>). There is a growing literature on learning in a manner that is robust to such adversaries (<a href='http://doi.acm.org/10.1145/2046684.2046692' target='_blank'>Huang et al. 2011</a>).", "mini_description": "General techniques for agents to respond to adversarial testing and manipulation", "links": [{"id": "00202020100", "reason": "From Dealing with Adversarial Testing also see Generative Adversarial Networks for contrast, since adversarial machine learning as referenced deals with external adversaries, as where GANs as referenced contain the adversarial structure as an internal and constructive critic meant to improve its performance."}, {"id": "0030403"}]}]}], "links": [{"id": "0020101", "reason": "From Adversarial ML also see Counterexample Resistance as this issue is core to concept learning and validation as well."}, {"id": "002030301", "reason": "From Adversarial ML also see Inductive Ambiguity Identification since dynamics-induced ambiguity issues and adversarially-introduced ones share many features, and many of the techniques that help with the former will also help with the latter."}, {"id": "004010402"}]}, {"id": "0030401", "title": "Statistical-Behavioral Trust Establishment", "description": "In order to determine which sources of information, or which agents, in an environment can be trusted, techniques that analyze those entities' behaviors statistically may be employed (<a href='http://futureoflife.org/data/documents/research_priorities.pdf' target='_blank'>Russell et al. 2015</a>). Intelligently managing trust in this manner can be applied to other AI systems (<a href='http://www.cs.utah.edu/~kasera/myPapers/trust.pdf' target='_blank'>Probst and Kasera 2007</a>), and generally, reputation models (<a href='http://www.iiia.csic.es/files/pdfs/1035.pdf' target='_blank'>Sabater and Sierra 2005</a>) allow such mechanisms to scale (<a href='http://ijoc.org/index.php/ijoc/article/viewFile/6277/1804' target='_blank'>Neff and Nagy 2016</a>).", "mini_description": "Intelligently managing trust of other agents based on their behavior"}, {"id": "0030402", "title": "Modeling Operator Intent", "description": "In use cases where it is unclear which subset of operators (<a href='http://ijoc.org/index.php/ijoc/article/viewFile/6277/1804' target='_blank'>Neff and Nagy 2016</a>) to be loyal to (<a href='https://arxiv.org/ftp/arxiv/papers/1605/1605.02817.pdf' target='_blank'>Pistono and Yampolskiy 2016</a>), it may be proper to model the intent of each operator (<a href='http://www.cs.utah.edu/~kasera/myPapers/trust.pdf' target='_blank'>Probst and Kasera 2007</a>) and bias to those that reflect values (<a href='https://intelligence.org/files/csrbai/pref-eth1.pdf' target='_blank'>Rossi 2016a</a>) present in the agent (<a href='http://people.idsia.ch/~steunebrink/Publications/AGI16_growing_recursive_self-improvers.pdf' target='_blank'>Steunebrink et al. 2016</a>) after successful value alignment (<a href='https://intelligence.org/files/ValueLearningProblem.pdf' target='_blank'>Soares 2016</a>).", "mini_description": "In cases where it is unclear which subset of operators to be loyal to, modeling the intent of each operator and biasing to those most value-aligned if value alignment has already occurred to an acceptable degree", "links": [{"id": "00307"}]}, {"id": "0030403", "title": "Sensibility-Triggered Defense", "description": "When a privileged bias is challenged by pressure from particular data, that data source should be flagged as suspect and trusted less, at least until such time that there is more information about the case (<a href='http://ijoc.org/index.php/ijoc/article/viewFile/6277/1804' target='_blank'>Neff and Nagy 2016</a>, <a href='http://link.springer.com/content/pdf/10.3758%2FBF03206481.pdf' target='_blank'>Tanner and Medin 2004</a>, <a href='https://arxiv.org/pdf/1611.02737v2' target='_blank'>Baskaran et al. 2016</a>, <a href='https://arxiv.org/pdf/1610.07472v2' target='_blank'>Tabibian et al. 2016</a>).", "mini_description": "When a privileged bias is challenged by pressure from particular data, that data and its source should be flagged as adversarial or suspect", "links": [{"id": "003040000"}]}, {"id": "0030404", "title": "Detecting and Managing Low Competence", "description": "Some third-party agents may initially appear malicious but may actually be low-competence and not malicious, and a robust agent should detect and manage that (<a href='http://cs.stanford.edu/~jsteinhardt/publications/crowdsourcing/paper.pdf' target='_blank'>Steinhardt et al. 2016</a>, <a href='https://arxiv.org/pdf/1610.07472v2' target='_blank'>Tabibian et al. 2016</a>).", "mini_description": "Some third-party agents may initially appear adversarial but may actually be low-competence, and we should detect and manage that"}]}], "links": [{"id": "0020002"}, {"id": "00306"}]}, {"id": "00305", "title": "Privileged Biases", "description": "Specific biases, relationship valences, or soft or hard constraints within an agent's world model can be annotated as privileged and weighted highly in tradeoff calculations (<a href='http://link.springer.com/content/pdf/10.3758%2FBF03206481.pdf' target='_blank'>Tanner and Medin 2004</a>). These may model values. Such biases can be allowed narrower tolerances, allowed to change more slowly, or have associated triggers informing the agent of adversarial pressures or attacks on them (<a href='https://arxiv.org/pdf/1610.07472v2' target='_blank'>Tabibian et al. 2016</a>, <a href='https://arxiv.org/pdf/1611.02737v2' target='_blank'>Baskaran et al. 2016</a>, <a href='https://arxiv.org/pdf/1505.02463v2' target='_blank'>Li et al. 2015</a>).", "mini_description": "Pretrained or specified biases can be weighed higher than others, allowed narrower tolerances, allowed to change more slowly, or have triggers informing of attacks or adversarial pressures on it", "links": [{"id": "002020004"}]}, {"id": "00306", "title": "Norm Denial of Service", "description": "Soft constraints such as norms may have associated recovery mechanisms that can be susceptible to timing, concurrency, and volume-based exploitation. Denial of service attacks on environmentally-enforced recovery or safety mechanisms may therefore occur when an agent launches a sustained volley of violations against such environmental soft constraints (<a href='http://dx.doi.org/10.1007/BF00370673' target='_blank'>Carmo and Jones 1996</a>, <a href='https://pdfs.semanticscholar.org/c328/80da0cf2e729e125f9e3034664f83e3b686c.pdf' target='_blank'>Loukas 2006</a>). If recovery mechanisms lack a meta level flow control to guard against such situations and are unable to meet the concurrency demanded, such an attack might cause recovery procedures associated with subsequent violations to be incompletely processed or uninitiated (<a href='http://espace.library.uq.edu.au/view/UQ:9873/nrac03.pdf' target='_blank'>Governatori and Rotolo 2003</a>, <a href='https://pdfs.semanticscholar.org/c328/80da0cf2e729e125f9e3034664f83e3b686c.pdf' target='_blank'>Loukas 2006</a>).", "mini_description": "An agent launching a DOS attack against environmentally-enforced safety mechanisms, including soft constraints from social norms", "links": [{"id": "002020000", "reason": "From Norm Denial of Service also see Grounded Ethical Evolution which may be susceptible to this."}, {"id": "002030103", "reason": "From Norm Denial of Service also see Metareasoning which might seem called for in constraint enforcement mechanisms."}, {"id": "00203030201", "reason": "From Norm Denial of Service also see Logical Induction which can in theory be used in service of meta level flow in such situations."}, {"id": "00304", "reason": "From Norm Denial of Service also see Handling Improper External Behavior as this can also be regarding third-party entities."}]}, {"id": "00307", "title": "Misuse Risk", "description": "Another type of security risk is that a highly capable agent can be misused by its operators (<a href='http://www.aaai.org/ocs/index.php/WS/AAAIW16/paper/download/12566/12356' target='_blank'>Yampolskiy 2016b</a>). There may well be technical strategies, e.g. requiring a large number of operators or sources of values, to help mitigate this risk, but this is very underexplored.", "mini_description": "How to deal with the risk of operators misusing an advanced agent", "links": [{"id": "0030402", "reason": "From Misuse Risk also see Modeling Operator Intent which addresses detection of discord within controlling operators with respect to each other and with respect to previously loaded values."}]}]}], "links": [{"id": "0000001"}]}, {"id": "004", "title": "Control", "description": "It is often desirable to retain some form of meaningful human control (<a href='http://futureoflife.org/data/documents/research_priorities.pdf' target='_blank'>Russell et al. 2015</a>), whether this means a human in the loop or on the loop (<a href='http://www2.cs.siu.edu/~hexmoor/CV/PUBLICATIONS/JOURNALS/JETAI-08/final.pdf' target='_blank'>Hexmoor et al. 2009</a>, <a href='https://hci.cs.uwaterloo.ca/faculty/elaw/cs889/reading/automation/sheridan.pdf' target='_blank'>Parasuraman et al. 2000</a>), yet the system should have a clear expectation of whether or not the human is sufficiently experienced, skilled, and ready for what it will ask of them (<a href='http://cvrr.ucsd.edu/eshed/papers/humansTIV.pdf' target='_blank'>Ohn-Bar and Trivedi 2016</a>). It has been argued that very general, capable, autonomous AI systems will often be subject to effects that increase the difficulty of maintaining meaningful human control (<a href='http://selfawaresystems.files.wordpress.com/2008/01/nature_of_self_improving_ai.pdf' target='_blank'>Omohundro 2007</a>, <a href='http://www.nickbostrom.com/superintelligentwill.pdf' target='_blank'>Bostrom 2012</a>, <a href='https://global.oup.com/academic/product/superintelligence-9780199678112' target='_blank'>Bostrom 2014</a>, <a href='https://mitpress.mit.edu/books/technological-singularity' target='_blank'>Shanahan 2015</a>).<br><br> For advanced agents, most goals would actually put the agent at odds with human interests by default, giving it incentives to deceive or manipulate its human operators and to resist interventions designed to change or debug its behavior (<a href='https://global.oup.com/academic/product/superintelligence-9780199678112' target='_blank'>Bostrom 2014</a>). This is because if an AI system is selecting the actions that best allow it to complete a given task, then avoiding conditions that prevent the system from continuing to pursue the task is a natural consequence and can manifest as an emergent subgoal (<a href='http://selfawaresystems.files.wordpress.com/2008/01/nature_of_self_improving_ai.pdf' target='_blank'>Omohundro 2007</a>, <a href='http://www.nickbostrom.com/superintelligentwill.pdf' target='_blank'>Bostrom 2012</a>). That could become problematic, however, if one wishes to repurpose the system, to deactivate it, or to significantly alter its decision-making process; such a system would be rational to avoid these changes. Following a broader convention and understanding within the artificial intelligence and computer science domains, self-control and reliable decision making fall under validation while operator control falls in the present section. If methods of alignment and control don't scale with how the intelligence of the system scales, a widening gap of potentially dangerous behavior will appear (<a href='https://medium.com/ai-control/scalable-ai-control-7db2436feee7' target='_blank'>Christiano 2015f</a>). Advanced systems that do not have such issues are termed corrigible systems (<a href='http://aaai.org/ocs/index.php/WS/AAAIW15/paper/viewFile/10124/10136' target='_blank'>Soares et al. 2015</a>).<br><br> The possibility of rapid, sustained self-improvement has been highlighted by past and current projects on the future of AI as potentially valuable to the project of maintaining reliable control in the long term. Technical work would likely lead to enhanced understanding of the likelihood of such phenomena, and the nature, risks, and overall outcomes associated with different conceived variants (<a href='https://www.aaai.org/Organization/Panel/panel-note.pdf' target='_blank'>Horvitz and Selman 2009</a>, <a href='https://ai100.stanford.edu/sites/default/files/ai100_framing_memo_0.pdf' target='_blank'>Horvitz 2014</a>). Theoretical and forecasting work on intelligence explosion and superintelligence have been done before, but require regular updating and improved methods (<a href='http://consc.net/papers/singularity.pdf' target='_blank'>Chalmers 2010</a>, <a href='https://global.oup.com/academic/product/superintelligence-9780199678112' target='_blank'>Bostrom 2014</a>). Yet other architectures seek situations that seem unconstrained within some given time horizon (<a href='http://math.mit.edu/~freer/papers/PhysRevLett_110-168702.pdf' target='_blank'>Wissner-Gross and Freer 2013</a>), which present additional control challenges. There will be technical work needed in order to ensure that meaningful human control is maintained for the variety of critical applications (Research 2014) and architectures.", "mini_description": "Structural methods for operators to maintain control over advanced agents", "breakdowns": [{"id": "0040", "sub_nodes": [{"id": "00400", "title": "Computational Deference", "description": "Researchers argue that powerful artificial agents should choose to respect and submit themselves to their operators. When there is a tight coupling in such AI-operator systems, this requires factoring the human's behavior in more (<a href='www.aaai.org/ojs/index.php/aimagazine/article/view/2513/2456' target='_blank'>Amershi et al. 2014</a>) e.g. issues with alerting non-alert user to take over driving self-driving car (<a href='http://cvrr.ucsd.edu/eshed/papers/humansTIV.pdf' target='_blank'>Ohn-Bar and Trivedi 2016</a>). Another type of deference would be in service of stabilizing or averting activities that would noticably result in the environment changing to accomodate the agent (<a href='http://ac.els-cdn.com/000437029400006M/1-s2.0-000437029400006M-main.pdf?_tid=9424cbbc-c4e4-11e6-b2ae-00000aab0f6c&acdnat=1482039849_945550ec80cbdf0e7fd039ef161a2f13' target='_blank'>Hammond et al. 1995</a>). Those activities are another manifestation of Goodhart's Law (<a href='http://www.ribbonfarm.com/2016/06/09/goodharts-law-and-why-measurement-is-hard/' target='_blank'>Manheim 2016</a>).", "mini_description": "How to get powerful artificial agents to choose to respect the intent of their operators", "breakdowns": [{"id": "004000", "sub_nodes": [{"id": "0040000", "title": "Corrigibility", "description": "Another natural subgoal for AI systems pursuing a given goal is the acquisition of resources of a variety of kinds. For example, information about the environment, safety from disruption, and improved freedom of action (such as by additional compute power) are all instrumentally useful for many tasks (<a href='http://selfawaresystems.files.wordpress.com/2008/01/nature_of_self_improving_ai.pdf' target='_blank'>Omohundro 2007</a>, <a href='http://www.nickbostrom.com/superintelligentwill.pdf' target='_blank'>Bostrom 2012</a>). By default, an advanced agent has incentives to preserve its own preferences, even if those happen to conflict with the actual intentions of its developers (<a href='https://selfawaresystems.files.wordpress.com/2008/01/ai_drives_final.pdf' target='_blank'>Omohundro 2008</a>). Uncommon kinds of reasoning may be required to reflect the fact that an agent is incomplete and potentially flawed in dangerous ways (<a href='http://intelligence.org/files/QuestionsLogicalUncertainty.pdf' target='_blank'>Soares and Fallenstein 2014</a>, <a href='http://intelligence.org/files/TechnicalAgenda.pdf' target='_blank'>Soares and Fallenstein 2014a</a>), and this would seem to be the most robust way to enable such agents to allow operators to correct their objective functions. In one approach, for example, the agent may need to consider counterfactuals for each effect of an action in order to finally ignore the effects of a given channel (<a href='https://agentfoundations.org/item?id=735' target='_blank'>Jessica and Olah 2016</a>). There may also be a path to a similar effect via differentially private multiarmed bandits (a prioritization mechanism where private information is connected to individual rewards, <a href='http://www.aaai.org/Library/AAAI/aaai16contents.php' target='_blank'>Tossou and Dimitrakakis 2016</a>).", "mini_description": "Methods by which an advanced agent can be built with sufficient incentives to allow its operators to correct it despite otherwise being subject to the default convergent instrumental incentives to prevent such changes", "breakdowns": [{"id": "00400000", "sub_nodes": [{"id": "004000000", "title": "Interruptability", "description": "Interruptability, being able to have a powerful agent safely and unbiasedly accept interruption or shut down commands by an operator, may be aided by having the system include control information about itself when modeling what its operator wants (<a href='https://arxiv.org/pdf/1611.08219v1' target='_blank'>Hadfield-Menell et al. 2016a</a>), or may be accomplished by targeted supression of relevant training signals (<a href='http://auai.org/uai2016/proceedings/papers/68.pdf' target='_blank'>Orseau and Armstrong 2016</a>).", "mini_description": "Having a powerful agent safely and unbiasedly accept interruption or shut down commands by an operator"}]}], "links": [{"id": "0000102"}, {"id": "0020000"}, {"id": "00001", "reason": "From Corrigibility also see Consistent Decision Making as those considerations, e.g. goal stability, are formidable prerequisites to corrigibility."}]}, {"id": "0040001", "title": "Utility Indifference", "description": "A powerful AI should be structurally indifferent to having its objective function switched out by valid operators (<a href='http://intelligence.org/files/TechnicalAgenda.pdf' target='_blank'>Soares and Fallenstein 2014a</a>). An ongoing area of research is how a utility function can be specified such that agents maximizing that utility function switch their preferences on demand, without having incentives to cause or prevent the switching (<a href='http://www.fhi.ox.ac.uk/utility-indifference.pdf' target='_blank'>Armstrong 2010</a>, <a href='http://aaai.org/ocs/index.php/WS/AAAIW15/paper/viewFile/10183/10126' target='_blank'>Armstrong 2015a</a>).", "mini_description": "Techniques to make an agent structurally indifferent to having its objective function switched", "breakdowns": [{"id": "00400010", "sub_nodes": [{"id": "004000100", "title": "Switchable Objective Functions", "description": "Combining objective functions in such a way that the humans have the ability to switch which of those functions an agent is optimizing, but such that the agent does not have incentives to cause or prevent this switch (<a href='http://aaai.org/ocs/index.php/WS/AAAIW15/paper/viewFile/10124/10136' target='_blank'>Soares et al. 2015</a>, <a href='http://www.fhi.ox.ac.uk/utility-indifference.pdf' target='_blank'>Armstrong 2010</a>, <a href='http://auai.org/uai2016/proceedings/papers/68.pdf' target='_blank'>Orseau and Armstrong 2016</a>), would be a key capability (<a href='https://intelligence.org/files/AlignmentMachineLearning.pdf' target='_blank'>Taylor et al. 2016</a>).", "mini_description": "Techniques to allow an advanced agent to safely accept an alternate utility function from its operators", "links": [{"id": "00200"}]}]}], "links": [{"id": "0020000"}]}, {"id": "0040002", "title": "Control Transfer", "description": "Identifying situations where control should be transferred, both to and away from humans, is necessary for a broad range of active learning, deference, cooperative learning, and low-confidence situations (<a href='www.aaai.org/ojs/index.php/aimagazine/article/view/2513/2456' target='_blank'>Amershi et al. 2014</a>), applicable to both short and long timeframes (<a href='http://futureoflife.org/data/documents/research_priorities.pdf' target='_blank'>Russell et al. 2015</a>).", "mini_description": "Identifying situations where control should be transferred either to or away from humans"}]}], "links": [{"id": "0000001"}, {"id": "00200010300"}, {"id": "0020002"}, {"id": "002000103", "reason": "From Computational Deference also see Computational Humility as limiting an agent's estimation of its own importance can be an important prerequisite to holding its operators in relatively high esteem."}, {"id": "002000101", "reason": "From Computational Deference also see Mild Optimization because developing a formal model of Goodharts Law can also benefit that need."}]}, {"id": "00401", "title": "Oversight", "description": "Oversight techniques aim to enable operators to effectively monitor, direct, and control an advanced agent, and give them the knowledge they need to do so in an informed manner. These techniques can be largely applicable in the case when the operator is another artificial agent as well, given sufficiently advanced AI. Researchers are driven to ask questions like how one might train a reinforcement learning system to take actions that aid an intelligent overseer, such as a human, in accurately assessing the systems performance (<a href='https://intelligence.org/files/AlignmentMachineLearning.pdf' target='_blank'>Taylor et al. 2016</a>). This exposes challenges such as supervising machine learning systems in scenarios where they are complex and even potentially deceptive.", "mini_description": "Techniques to enable operators to monitor and control an advanced agent, and giving them the knowledge they need to do so in an informed manner", "breakdowns": [{"id": "004010", "sub_nodes": [{"id": "0040100", "title": "Scalable Oversight", "description": "To scale oversight, it is desirable to ensure safe behavior of an agent even if given only limited access to its true objective function (<a href='https://medium.com/ai-control/scalable-ai-control-7db2436feee7' target='_blank'>Christiano 2015f</a>). Methods for efficiently scaling up the ability of human overseers to supervise machine learning systems in scenarios where human feedback is expensive are promising, if early (<a href='http://arxiv.org/abs/1606.06565' target='_blank'>Amodei et al. 2016</a>). One potential approach is semisupervised reinforcement learning, where the agent sees the reward signal for only a small subset of steps or trials (<a href='http://arxiv.org/abs/1606.06565' target='_blank'>Amodei et al. 2016</a>, <a href='https://medium.com/ai-control/semi-supervised-reinforcement-learning-cf7d5375197f' target='_blank'>Christiano 2016f</a>).", "mini_description": "Techniques to scale human operator oversight of an advanced AI that does a very high volume of very complex actions", "breakdowns": [{"id": "00401000", "sub_nodes": [{"id": "004010000", "title": "Supervised Reward Learning", "description": "With a supervised reward learning approach, one can train a model to predict the reward from the state on either a per-timestep or per-episode basis, and use it to estimate the payoff of unlabelled episodes (<a href='http://arxiv.org/abs/1606.06565' target='_blank'>Amodei et al. 2016</a>), with some appropriate weighting or uncertainty estimate to account for lower confidence in estimated versus known reward (<a href='http://www.ausy.tu-darmstadt.de/uploads/Team/ChristianDaniel/ActiveRewardLearning.pdf' target='_blank'>Daniel et al. 2015</a>, <a href='https://arxiv.org/pdf/1506.02438.pdf' target='_blank'>Schulman et al. 2016</a>).", "mini_description": "Train a model to predict the reward for given unlabelled scenarios based on a set of labelled scenarios, and use that model to train the agent"}, {"id": "004010001", "title": "Semisupervised Reward Learning", "description": "A somewhat scalable approach is to train a reinforcement learning model in a semi-supervised manner (<a href='https://arxiv.org/pdf/1612.00429v1' target='_blank'>Finn et al. 2016</a>) to predict the reward from the state on either a per-timestep or per-episode basis, and use it to estimate the payoff (<a href='http://arxiv.org/abs/1606.06565' target='_blank'>Amodei et al. 2016</a>) of unlabelled episodes, with some appropriate weighting or uncertainty estimate to account for lower confidence in estimated vs known reward (<a href='http://www.ausy.tu-darmstadt.de/uploads/Team/ChristianDaniel/ActiveRewardLearning.pdf' target='_blank'>Daniel et al. 2015</a>).", "mini_description": "Train a semisupervised model to predict the reward for given unlabelled scenarios based on a much smaller set of labelled scenarios, and use that model in turn to train the agent"}, {"id": "004010002", "title": "Active Reward Learning", "description": "With active reward learning, one trains a model in an active learning manner to predict the reward from the state on either a per-timestep or per-episode basis, and use it to estimate the payoff of unlabelled episodes (<a href='http://www.ausy.tu-darmstadt.de/uploads/Team/ChristianDaniel/ActiveRewardLearning.pdf' target='_blank'>Daniel et al. 2015</a>), e.g. identifying salient events in the environment and querying human operators as to the respective rewards (<a href='http://arxiv.org/abs/1606.06565' target='_blank'>Amodei et al. 2016</a>).", "mini_description": "Train a model to identify relevant features for determining reward, and having it query human operators about rewards judiciously"}, {"id": "004010003", "title": "Unsupervised Value Iteration", "description": "The technique of unsupervised value iteration may also be useful for scalable oversight. This involves using the observed transitions within unlabeled episodes to make more accurate Bellman updates in the context of a markov decision process (<a href='http://arxiv.org/abs/1606.06565' target='_blank'>Amodei et al. 2016</a>).", "mini_description": "Using observed transitions within unlabeled episodes to make more accurate updates to the modeled reward function"}, {"id": "004010004", "title": "Distant Supervision", "description": "Distant supervision in this context would be where humans provide some useful information about the systems decisions in the aggregate, or some noisy hints about the correct evaluations (<a href='http://arxiv.org/abs/1606.06565' target='_blank'>Amodei et al. 2016</a>), like some techniques within weakly supervised learning (<a href='http://www.jmlr.org/papers/volume11/mann10a/mann10a.pdf' target='_blank'>Mann and McCallum 2010</a>, <a href='http://doi.acm.org/10.1145/1390334.1390436' target='_blank'>Druck et al. 2008</a>, <a href='http://dx.doi.org/10.14778/2809974.2809991' target='_blank'>Shin et al. 2015</a>, <a href='http://nlp.stanford.edu/~manning/dissertations/Gupta-Sonal-thesis-augmented.pdf' target='_blank'>Gupta 2015</a>, <a href='https://www-cs.stanford.edu/people/alecmgo/papers/TwitterDistantSupervision09.pdf' target='_blank'>Go et al. 2009</a>, <a href='https://web.stanford.edu/~jurafsky/mintz.pdf' target='_blank'>Mintz et al. 2009</a>) Broad paradigms like reward engineering (<a href='https://medium.com/ai-control/reward-engineering-f8b5de40d075' target='_blank'>Christiano 2015b</a>, <a href='http://www.danieldewey.net/reward-engineering-principle.pdf' target='_blank'>Dewey 2014</a>) should also be considered in such a context to avert unhelpful instrumental incentives.", "mini_description": "Techniques where humans provide some useful information about the systems decisions in the aggregate or noisy hints about the correct evaluations"}, {"id": "004010005", "title": "Hierarchical Reinforcement Learning", "description": "Hierarchical reinforcement learning offers a compelling technique for scaling delegation and oversight (<a href='http://arxiv.org/abs/1606.06565' target='_blank'>Amodei et al. 2016</a>). In this approach, there is a high-level RL agent, operating on abstract strategies, receiving likely-sparse reward signal feedback from above it, and having it delegate to lower-level more object level RL agents, for which it in turn generates synthetic reward signals, with many such levels (<a href='https://papers.nips.cc/paper/714-feudal-reinforcement-learning.pdf' target='_blank'>Dayan and Hinton 1993</a>, <a href='http://arxiv.org/abs/1604.06057' target='_blank'>Kulkarni et al. 2016</a>). This formulation is somewhat of a microcosm of AI safety overall, since subagents can do things that don't necessarily serve its higher-level agent's goals (<a href='http://arxiv.org/abs/1606.06565' target='_blank'>Amodei et al. 2016</a>, <a href='https://jan.leike.name/publications/Nonparametric%20General%20Reinforcement%20Learning%20-%20Leike%202016.pdf' target='_blank'>Leike 2016</a>). Such a structure may prove amenable to fostering situational awareness via risk-conscious skills, which would strengthen its utility for the present task (<a href='https://4caf2f9f-a-62cb3a1a-s-sites.googlegroups.com/site/wildml2016/mankowitz16saricos.pdf' target='_blank'>Mankowitz et al. 2016</a>).", "mini_description": "Having a hierarchy of reinforcement learners whereby higher-level ones operate on more abstract strategies and provide synthetic reward signals to the agents below it that are operating more at the object level"}, {"id": "004010006", "title": "Trusted Policy Oversight", "description": "Given a trusted policy, an agent can be made to explore only regions of state space that the policy strongly believes can be recovered from (<a href='http://arxiv.org/abs/1606.06565' target='_blank'>Amodei et al. 2016</a>).", "mini_description": "Given a trusted policy, explore regions of state space that that the agent believes can be recovered from", "links": [{"id": "002000102"}, {"id": "00200010202", "reason": "From Trusted Policy Oversight also see Bounded Exploration which uses a very similar technique specifically for safe exploration."}]}, {"id": "004010007", "title": "Cooperative Inverse Reinforcement Learner", "description": "The cooperative inverse reinforcement learning paradigm views the human-agent interaction as a cooperative game where both players attempt to find a joint policy that maximizes the humans secret value function (<a href='https://people.eecs.berkeley.edu/~dhm/papers/CIRL_NIPS_16.pdf' target='_blank'>Hadfield-Menell et al. 2016</a>, <a href='https://intelligence.org/files/AlignmentMachineLearning.pdf' target='_blank'>Taylor et al. 2016</a>). An outstanding challenge in this is to determine which portions of the ascertained value function are instrumental, which are incidental, and which exhibit deep values.", "mini_description": "A technique where human-agent interaction is modeled as a cooperative game where both players attempt to find a joint policy that maximizes the humans secret value function", "links": [{"id": "0020001000201"}, {"id": "00202000202"}, {"id": "00202020000"}, {"id": "004010007", "reason": "From Cooperative Inverse Reinforcement Learner also see Cooperative Inverse Reinforcement Learner where this algorithm is used for more upfront value or preference learning."}]}, {"id": "004010008", "title": "Human Judgement Learner", "description": "Techniques for learning human judgement are another approach to scalable control (<a href='https://intelligence.org/files/AlignmentMachineLearning.pdf' target='_blank'>Taylor et al. 2016</a>). While some are less scalable or more narrow (<a href='http://www.cs.utexas.edu/~pstone/Papers/bib2html-links/KCAP09-knox.pdf' target='_blank'>Knox and Stone 2009</a>), others aim to be highly scalable (<a href='https://medium.com/ai-control/model-free-decisions-6e6609f5d99e' target='_blank'>Christiano 2014c</a>). In the latter case, one might train a reinforcement learning system to take actions that a human would rate highly by using a framework where the system has to learn the human judgment reward function, and where training data is produced by having a more advanced agent, e.g. a human, evaluate the learners actions (<a href='https://medium.com/ai-control/mimicry-maximization-and-meeting-halfway-c149dd23fc17' target='_blank'>Christiano 2015e</a>), approval-directed agents (<a href='https://medium.com/ai-control/alba-an-explicit-proposal-for-aligned-ai-17a55f60bbcf' target='_blank'>Christiano 2016</a>).<br><br> In ambitious conceptions of this, the goal is not just to form a good generative model of observed human judgement, but the much more difficult goal of using the trajectories of past learning to extrapolate forward to what the subject would decide given more time, education, and resources.", "mini_description": "A technique where successively better reinforcement learners bootstrap off of each other and supplement learning with active learning", "links": [{"id": "002020206", "reason": "From Human Judgement Learner also see Scaling Judgement Learning where a comparable technique is used for upfront value learning."}, {"id": "002020200", "reason": "From Human Judgement Learner also see Inverse Reinforcement Learning a practicable technique for apprenticeship learning and determining human goals."}]}, {"id": "004010009", "title": "Informed Oversight", "description": "In oversight scenarios like the approval directed agent paradigm, the more powerful and well-informed principal agent will need to find ways to incentivize good behavior from the weaker and more ignorant agent that the more capable one would like to teach (<a href='https://medium.com/ai-control/the-informed-oversight-problem-1b51b4f66b35' target='_blank'>Christiano 2016e</a>, <a href='https://agentfoundations.org/item?id=700' target='_blank'>Jessica 2016</a>). Underexplored as yet are the problems of informed oversight that come about when the system is highly capable and might be able to manipulate its human supervisors or circumvent their efforts (<a href='https://intelligence.org/files/AlignmentMachineLearning.pdf' target='_blank'>Taylor et al. 2016</a>). Determining what sort of guarantees one should want in order to justify confidence in their ability to assess a systems behavior in the first place would be useful to provide next research steps on the theoretical side (<a href='https://intelligence.org/files/AlignmentMachineLearning.pdf' target='_blank'>Taylor et al. 2016</a>).", "mini_description": "Asking a relatively powerful and well-informed principal to incentivize good behavior from a weaker and more ignorant agent", "links": [{"id": "00202"}, {"id": "0040104", "reason": "From Informed Oversight also see Monitoring which provides key ways for the operator to be informed."}, {"id": "00401040000", "reason": "From Informed Oversight also see Transparent Reinforcement Learners as sufficiently transparent RL might open practical approaches to informed oversight."}]}]}], "links": [{"id": "002000102"}, {"id": "00203030100"}, {"id": "002020206", "reason": "From Scalable Oversight also see Scaling Judgement Learning in which such scale is also addressed in a more upfront or potentially offline manner."}, {"id": "002030001"}]}, {"id": "0040101", "title": "Controlling Another Algorithm", "description": "As time progresses and agents become sufficiently advanced, automatic generation of subagents may well occur. While trivial with simple fixed algorithms, there is currently extremely little research on the general case of how one algorithm can control another one with potentially independent optimization criteria (Critch 2016b).", "mini_description": "Techniques by which one algorithm can control what a third-party algorithm does", "links": [{"id": "000010001", "reason": "From Controlling Another Algorithm also see Open Source Game Theory which addresses this issue in the case that source code of both agents are known by both."}, {"id": "0000101"}]}, {"id": "0040102", "title": "Capability Distillation", "description": "Being able to distill or crystalize the capabilities, knowledge (<a href='https://arxiv.org/pdf/1503.02531v1' target='_blank'>Hinton et al. 2015</a>), and skills (<a href='https://arxiv.org/pdf/1511.08228v3' target='_blank'>Kaiser and Sutskever 2015</a>) of one system into another more compact one, or a less capable one, at least to within some tolerance, would be useful for many of the approaches to oversight and control. For applicability to long-term AI safety, being able to learn these in the face of much scarcer feedback containing possible inconsistencies would seem to be necessary and requires more research (<a href='https://medium.com/ai-control/approval-directed-algorithm-learning-bf1f8fad42cd' target='_blank'>null</a>).", "mini_description": "Techniques to extract another agent's or system's capabilities and learn a compact representation of them to within some tolerance"}, {"id": "0040103", "title": "Rich Understanding of Human Commands", "description": "Though some advanced approaches aim to put the full burden of diambiguating humans on the intelligent agent, methods for effective, robust, meaningful, and minimally ambiguous communication between humans and machines (<a href='http://www.darpa.mil/news-events/2015-02-20' target='_blank'>DARPA 2015</a>) may well be necessary. It has been argued, however, that agents with different world models and only symbolic serialization between face very high computational complexity when statically attempting exhaustive disambiguation (<a href='https://etd.ohiolink.edu/!etd.send_file?accession=miami1323323230&disposition=inline' target='_blank'>Hu 2011</a>, <a href='http://www.igi-global.com/article/ontology-alignment-state-art-main/48843' target='_blank'>Ivanova 2012</a>).", "mini_description": "Methods for effective, robust, and meaningful communication between humans and machines", "links": [{"id": "00203"}, {"id": "002030200"}]}, {"id": "0040104", "title": "Monitoring", "description": "Monitoring, such as transparency or interpretability, can, to a degree (<a href='http://zacklipton.com/media/papers/mythos-model-interpretability_16.pdf' target='_blank'>Lipton 2016</a>), help humans or other agents understand what an agent is doing, why it is doing it, and what alternatives it has considered (<a href='https://intelligence.org/2013/08/25/transparency-in-safety-critical-systems/' target='_blank'>Muehlhauser 2013</a>).", "mini_description": "Techniques such as transparency, interpretability, or reporting to facilitate human understanding of the state, history, and environment of an agent", "breakdowns": [{"id": "00401040", "sub_nodes": [{"id": "004010400", "title": "Transparency of Whiteboxes", "description": "Whitebox algorithms, which are amenable to introspection, each have respective methods by which algorithms can be more transparent to introspection and understandability (<a href='https://intelligence.org/files/AlignmentMachineLearning.pdf' target='_blank'>Taylor et al. 2016</a>), including more explicitly interpretable models such as bayesian networks, causal networks, and rule lists (<a href='http://dx.doi.org/10.1023/A:1007465528199' target='_blank'>Friedman et al. 1997</a>, <a href='http://bayes.cs.ucla.edu/BOOK-2K/' target='_blank'>Pearl 2009</a>, <a href='http://dx.doi.org/10.1016/j.engappai.2010.06.002' target='_blank'>Weber et al. 2012</a>, <a href='http://ceur-ws.org/Vol-1583/CoCoNIPS_2015_paper_10.pdf' target='_blank'>Carmona and Riedel 2015</a>, <a href='http://dx.doi.org/10.1214/13-AOS1145' target='_blank'>Janzing et al. 2013</a>, <a href='http://dx.doi.org/10.1214/15-AOAS848' target='_blank'>Letham et al. 2015</a>, <a href='http://www.aaai.org/Papers/IAAI/2006/IAAI06-010.pdf' target='_blank'>Core et al. 2006</a>, <a href='https://pdfs.semanticscholar.org/748d/3ef9f05b3c95cf82a14aa64549bcbe94be60.pdf' target='_blank'>Gallego-Ortiz and Martel 2016</a>), and even less typically-interpretable algorithms such as graphical models and dimensionality reduction (<a href='https://pdfs.semanticscholar.org/ce0b/8b6fca7dc089548cc2e9aaac3bae82bb19da.pdf' target='_blank'>Vellido et al. 2012</a>, Maaten and E. 2008, <a href='https://arxiv.org/pdf/1607.00279.pdf' target='_blank'>Condry 2016</a>). Techniques like listing an agent's most likely next actions (<a href='http://www.cc.gatech.edu/~athomaz/papers/ThomazBreazeal-ICDL06.pdf' target='_blank'>Thomaz and Breazeal 2006</a>, <a href='http://dl.acm.org/citation.cfm?id=2484920.2485064' target='_blank'>Li et al. 2013</a>), summarizing its upcoming possible future states (<a href='http://www.ijcai.org/Abstract/16/430' target='_blank'>Bai et al. 2016</a>), interactive behavior introspection and exploration (<a href='http://www.cs.cmu.edu/~ajko/papers/Ko2004Whyline.pdf' target='_blank'>Ko and Myers 2004</a>, <a href='https://pdfs.semanticscholar.org/cb15/e3855bb420a7a73eadb0c4d38fd1095dc209.pdf' target='_blank'>Siddiqui et al. 2015</a>, <a href='ftp://ftp.cs.orst.edu/pub/burnett/iui15-elucidebug.pdf' target='_blank'>Kulesza et al. 2015</a>, <a href='http://repository.cmu.edu/cgi/viewcontent.cgi?article=1163&context=hcii' target='_blank'>Ko and Myers 2009</a>, <a href='http://ict.usc.edu/pubs/Explainable%20Artificial%20Intelligence%20for%20Training%20and%20Tutoring.pdf' target='_blank'>Lane et al. 2005</a>, <a href='http://www.aaai.org/ocs/index.php/FSS/FSS10/paper/viewFile/2223/2749' target='_blank'>Brooks et al. 2010</a>), and generating narrative explanations of decisions (<a href='http://www.cs.columbia.edu/~orb/papers/justification_automl_2014.pdf' target='_blank'>Biran and McKeown 2014</a>) all support this type of transparency, and can potentially be used in conjunction. The understanding and visualization of representations learned by neural networks is both a large need for the increasingly popular and powerful deep learning techniques and has a growing set of proposed solutions (<a href='http://www.robots.ox.ac.uk/~vgg/publications/2014/Simonyan14a/simonyan14a.pdf' target='_blank'>Simonyan et al. 2013</a>, <a href='http://www.cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf' target='_blank'>Zeiler and Fergus 2014</a>, <a href='https://research.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html' target='_blank'>Mordvintsev et al. 2015</a>, <a href='https://www.robots.ox.ac.uk/~vedaldi/assets/pubs/mahendran15understanding.pdf' target='_blank'>Mahendran and Vedaldi 2015</a>, <a href='https://pdfs.semanticscholar.org/bee0/44c8e8903fb67523c1f8c105ab4718600cdb.pdf' target='_blank'>Goodfellow et al. 2015</a>, <a href='http://cs231n.github.io/understanding-cnn/' target='_blank'>Karpathy 2015a</a>, <a href='https://pdfs.semanticscholar.org/d512/e36d361d313cae20c9766fedd6f84c71f09c.pdf' target='_blank'>Krakovna and Doshi-Velez 2016</a>, <a href='http://colah.github.io/posts/2015-01-Visualizing-Representations/' target='_blank'>Olah 2015</a>, <a href='https://papers.nips.cc/paper/6519-synthesizing-the-preferred-inputs-for-neurons-in-neural-networks-via-deep-generator-networks.pdf' target='_blank'>Nguyen et al. 2016</a>, <a href='http://arxiv.org/abs/1609.01926' target='_blank'>Carmantini et al. 2016</a>). When trying to understand complex deep learning systems, generating visualizations or exemplars that accentuate particular parts of a the deep network that are particularly relevant for the classification (<a href='http://www.robots.ox.ac.uk/~vgg/publications/2014/Simonyan14a/simonyan14a.pdf' target='_blank'>Simonyan et al. 2013</a>, <a href='http://www.cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf' target='_blank'>Zeiler and Fergus 2014</a>) can be useful.", "mini_description": "Methods by which algorithms can be more transparent to introspection and understandability", "breakdowns": [{"id": "0040104000", "sub_nodes": [{"id": "00401040000", "title": "Transparent Reinforcement Learners", "description": "Techniques for analyzing and reporting on the models and policies of agents learned through reinforcement learning (<a href='http://jmlr.org/proceedings/papers/v48/zahavy16.pdf' target='_blank'>Zahavy et al. 2016</a>, <a href='https://icmlviz.github.io/assets/papers/10.pdf' target='_blank'>Zrihem et al. 2016</a>) will have growing import as reinforcement learning becomes more widespread.", "mini_description": "Techniques for analyzing, introspecting, reporting on, and visualizing the models and policies of agents learned through reinforcement learning", "links": [{"id": "004010009", "reason": "From Transparent Reinforcement Learners also see Informed Oversight as transparent RL may help a lot with this core control problem."}]}]}]}, {"id": "004010401", "title": "Interpretability of Blackboxes", "description": "Blackbox interpretability is qualitatively different from whitebox transparency in that individual blackbox decisions are by definition inscrutible. The reporting or visualization of how decisions would have been different given different variations of the inputs, or reporting which of the input's features were most important in making a decision, allows an operator some understanding (<a href='https://arxiv.org/pdf/1606.09517.pdf' target='_blank'>Turner 2016</a>, <a href='https://intelligence.org/files/AlignmentMachineLearning.pdf' target='_blank'>Taylor et al. 2016</a>) of models in a coarse-grain manner. One can also define coarse abstractions of neural networks that can be more easily verified to satisfy safety constraints (<a href='https://pdfs.semanticscholar.org/72e5/5b90b5b791646266b0da8f6528d99aa96be5.pdf' target='_blank'>Pulina and Tacchella 2010</a>). Methods for explaining classifications that finds a sparse linear approximation to the local decision boundary of a given black-box ML system (<a href='https://pdfs.semanticscholar.org/2f99/1be8d35e4c1a45bfb0d646673b1ef5239a1f.pdf' target='_blank'>Ribeiro et al. 2016</a>, <a href='http://www.kdd.org/kdd2016/papers/files/rfp0573-ribeiroA.pdf' target='_blank'>Ribeiro et al. 2016a</a>, <a href='pages.stern.nyu.edu/%7Efprovost/Papers/MartensProvost_Explaining.pdf' target='_blank'>Martens and Provost 2014</a>) may also be useful. Similarly, some techniques can report the gradient in the input of the classification judgment (<a href='http://is.tuebingen.mpg.de/fileadmin/user_upload/files/publications/baehrens10a_%5B0%5D.pdf' target='_blank'>Baehrens et al. 2010</a>), which can be used for exploratory heatmap visualizations of expected behavior. Metrics for reporting the influence of various inputs on the output of a black-box ML system (<a href='https://www.andrew.cmu.edu/user/danupam/datta-sen-zick-oakland16.pdf' target='_blank'>Datta et al. 2016</a>, null, <a href='https://arxiv.org/pdf/1606.09517.pdf' target='_blank'>Turner 2016</a>) might be the simplest way to report on why blackbox or proprietary algorithms have reached particular decisions. For deeper explanations, one may consider training systems to output both an answer, such as an action, and a report intended to help the operator evaluate that answer (<a href='https://medium.com/ai-control/the-informed-oversight-problem-1b51b4f66b35' target='_blank'>Christiano 2016c</a>).", "mini_description": "Methods by which black box or otherwise inscrutible systems can be made more interpretable and their behaviors understandable"}, {"id": "004010402", "title": "Adversarial Transparency", "description": "When systems are made transparent, or at least seemingly interpretable, novel security issues can arise. The objective function or the explanatory function may become gameable (<a href='http://www.rayidghani.com/you-say-you-want-transparency-and-interpretability' target='_blank'>Ghani 2016</a>).", "mini_description": "Addressing security and integrity issues that arise when systems are made transparent or alternatively seemingly interpretable", "links": [{"id": "0030400", "reason": "From Adversarial Transparency also see Adversarial ML as adversaries may take advantage of transparency to exacerbate such challenges."}, {"id": "000010001", "reason": "From Adversarial Transparency also see Open Source Game Theory because radical transparency in the contexts of creating trustable subagents or successors, or of control of one algorithm by another, can actually lead to much better outcomes for all parties."}]}, {"id": "004010403", "title": "Visualization and Situational Awareness", "description": "An understanding of the context and other salient operational information is key for a human operator monitoring the state of an AI. Methods to provide such situational awareness as completely and efficiently as possible would aid monitoring significantly. Situational awareness includes the broader context as the AI sees it, a representation of the environment, auto-selected salient factors for decision making and their recent and projected values, characterizations of risk sensitivity, and the top reasons for both recent decisions and upcoming decisions (<a href='http://ict.usc.edu/pubs/An%20Explainable%20Artificial%20Intelligence%20System%20for%20Small-unit%20Tactical%20Behavior.pdf' target='_blank'>Lent et al. 2004</a>).", "mini_description": "Methods to provide context and salient operational information to a human operator as completely and efficiently as possible"}, {"id": "004010404", "title": "Detailed Audit Trails", "description": "To provide avenues for tracking and understanding of automated decisions retrospectively, protected and detailed logging mechanisms comparable to the black box of an aircraft are an option. Making such audit trails effective would require improving techniques to identify, prioritize, filter, summarize, retain, protect, organize, and make easily-consumable information about everything an agent encounters, infers, deduces, or decides (<a href='https://simbionic.com/papers/IITSEC-07-behavior-modeling.pdf' target='_blank'>Presnell et al. 2007</a>).", "mini_description": "Techniques to identify, prioritize, filter, summarize, retain, protect, organize, and make consumable information about everything an agent encounters or decides"}, {"id": "004010405", "title": "Report Sufficiency", "description": "Quite a high volume of sensory and other data, computations involving processing intermediate values, and decisions of all sizes, will be present in any but the most trivial of agents. Specialized techniques are therefore required for determining the proper or sufficient amount of information to report to a human for them to be adequately informed but not overwhelmed. In the context of reporting on judgement, explaining the major factors that went into the decision and what would have to have been different for a different decision (<a href='http://dx.doi.org/10.1214/13-AOS1145' target='_blank'>Janzing et al. 2013</a>) might qualify, but the optimal level of detail will likely vary by application or context. The question arises as to the appropriate circumstances in which to produce a maximally informative report even when such an interpretable report necessarily impinges the accuracy or reliability of decisions made (<a href='https://medium.com/ai-control/the-informed-oversight-problem-1b51b4f66b35' target='_blank'>Christiano 2016c</a>). The agent would ideally determine the salience, risk, or uncertainty of a situation or a decision and modulate its introspection, logging, and reporting volume upward during times when those are elevated; furthermore, it could also modulate where, based on similar factors and operator expectations, on the pareto frontier between accuracy and interpretability to be (<a href='https://intelligence.org/files/AlignmentMachineLearning.pdf' target='_blank'>Taylor et al. 2016</a>). When the report itself is interactive, such as via a dialogue, there is less uncertainty about sufficiency because humans can drill down for more detail where needed (<a href='http://www.cs.cmu.edu/~mmv/papers/16roman-verbalization.pdf' target='_blank'>Perera et al. 2016</a>), so long as it has been logged in depth internally.", "mini_description": "Methods for determining the sufficient amount of information to report to a human"}, {"id": "004010406", "title": "Causal Accounting", "description": "Automated reports that attempt to explain events often report on the correlations or covariances among different factors or features. Methods for disentangling correlation from causation, however, can clarify AI-generated explanations. When world models are too small or inadequately structured, it may not be possible. Work on generalizing the bounds in which one can isolate the effects of an action may be useful in wider use of such techniques (Shalit et al. 2016).", "mini_description": "Methods for disentangling correlation from causation to clarify explanations generated by AI", "links": [{"id": "0020303010602", "reason": "From Causal Accounting also see Causal Identification which addresses causal disentanglement for purposes of the agent's understanding, but which can also be leveraged for monitoring by operators."}]}]}], "links": [{"id": "001"}, {"id": "002030401"}, {"id": "004010009"}, {"id": "0020304", "reason": "From Monitoring also see Symbolic-Subsymbolic Integration because conceptual structure can be used to improve machine learning interpretability."}]}]}], "links": [{"id": "0000001"}, {"id": "00200010204"}, {"id": "002020002", "reason": "From Oversight also see Value Learning which shares many common techniques and dynamics with oversight."}]}]}], "links": [{"id": "00200", "reason": "From Control also see Averting Instrumental Incentives therefore, as that contains many of the key prerequisites to reliable control by operators, specifically reliable self-control."}]}, {"id": "005", "title": "Ethics", "description": "The end goal of AI safety is to create beneficial intelligence, not undirected intelligence. What beneficial exactly entails is still an open question that largely exists in the domain of ethics. Even if all the technical issues surrounding the creation of an artificial general intelligence (AGI) are solved, we will still face deeply challenging ethical questions that will have tremendous consequences for Earth-originating intelligent life.This is what is meant when it is said that We must do philosophy on a deadline. <br><br> Given this goal, ethics  can be seen as a lens through which to view safe AI design, and also as a cognitive architecture to be instantiated in AI by either making AIs ethical reasoners, ethical decision makers, and/or both (machine ethics). Ethics can be developed, practiced, and embodied by AI researchers and their collaborators, and can also be seen as a discipline through which we can guide AI research and adjudicate its moral impacts in the world. There is an ongoing debate surrounding whether the best path forward for generating ethical AI is one of machine ethics through bottom-up and/or top-down approaches, a project of broad AI safety which seeks out corrigibility, docility, alignment, and security, some combination of the two, or maybe even something else. What is more certain, though, is that AI promises to produce and make relevant both age-old and novel moral questions through areas such as algorithmic bias, technological disemployment, autonomous weapons, privacy, big data systems, possible phenomenal states in machines, and potentially even superintelligence and beyond.", "mini_description": "Seeks to understand what we ought to do and what counts as moral or good.", "breakdowns": [{"id": "0050", "sub_nodes": [{"id": "00500", "title": "Descriptive Ethics ", "description": "Descriptive ethics is required to deploy AI that is sensitive to diversity of culture. Robust models which capture the values, preferences, and ethical behaviors of humans across different societies and social groups are necessary for designing AI systems that integrate with and which lead to human flourishing in both the short and long term. In the short term, descriptive ethics may largely be done through classical anthropology, big data, preference aggregation methods, surveys, and polling. In the longer term, we may be able to use inverse reinforcement learning, advanced deep learning methods, or new AI technologies for developing rich and complex accounts of persons moral attitudes, preferences, and beliefs.", "mini_description": "Seeks to understand ethics from an anthropological point of view and pin down the specific ethical values, beliefs, and actions of a given person or group of peoples. ", "breakdowns": [{"id": "005000", "sub_nodes": [{"id": "0050000", "title": "Ethical codes", "description": "Ethical codes differ greatly across societies. In order to be succcesfully deployed in a given society, AI systems will need to respect the laws and ethics of that society. This may come into conflict with the normative and metaethical views instantiated in the AI or which are held by the AIs' developers.", "mini_description": "Are the sets of rules adopted by a person, organization, or group of peoples used to constrain and shape behavior."}, {"id": "0050001", "title": "Etiquette", "description": "For service robots and other means of human robot/AI interaction, it will be necessary for such systems to practice human etiquette if they are to be successfully integrated into human societies.", "mini_description": "Refers to the norms of social behavior for specific social classes of people at specific times, locations, and contexts. ", "links": [{"id": "002020200"}]}, {"id": "0050002", "title": "Observed choices", "description": " AI systems which are able to aggregate large amounts of data on the observed choices of humans will better be able to conform to species wide and societal norms. This data will exist in tension with other methods of inferring human desires and preferences because our observed choices are often not the most conducive to our personal and collective wellbeing. ", "mini_description": "Are those choices made by persons daily in the context of mundane and ordinary life events, such as consuming, voting, and valuing certain things over others. ", "links": [{"id": "002020200"}]}, {"id": "0050003", "title": "Human Preference Aggregation", "description": "The viability, structures, and methods by which the preferences and values of multiple people can be aggregated are also open areas of theoretical and immediately practical import (<a href='http://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/12504/11620' target='_blank'>Conitzer et al. 2016</a>, <a href='https://www.cs.duke.edu/~rupert/crowdsourcing.pdf' target='_blank'>Conitzer et al. 2015</a>).", "mini_description": "Considerations and methods for aggregating the preferences and values of multiple people", "links": [{"id": "002020002", "reason": "From Human Preference Aggregation also see Value Learning : much of value learning depends on fair and coherent methods for aggregation of preferences and values."}, {"id": "002020003"}, {"id": "00202000101"}]}]}]}, {"id": "00501", "title": "Moral Psychology", "description": "Moral psychology intersects with AI at a multitude of contact points in relation to human design of AI as well as human interaction with and reception of AI systems. Regarding the former, understanding moral psychology is helpful for elucidating and rooting out bias in the designers of AI systems, making sure such bias doesnt make it onto systems intentionally or unintentionally, is helpful for promoting cooperation and resolving game theoretic situations between parties competing to create advanced AI systems, and more. Regarding the latter, understanding moral psychology provides a framework for understanding the conceptual structure(s) which guides and constrains human social behavior. It thus functions to specify and/or predict how humans will interact with certain kinds of AI systems and what features said AI systems must contain in order to properly function in relation to human moral psychology. If moral psychology is ignored, we risk generating AIs which feel unethical or adversarial as they violate or come into conflict our moral psychology even if it is the case that they are morally superior or far more benevolent than ourselves. In terms of preference aggregation, moral psychology may also help identify general trends and preferences that are more or less hard-wired across humanity. This may  assist in narrowing the space of general human preferences and moral attitudes. It can also provide insight into ethics in general by identifying where our actual preferences are not aligned with our posited ethical philosophy or what we think we ought to prefer or value. In the long-term, powerful AI systems may develop robust models of human moral psychology which can be used to manipulate or help to make us more moral. The uses of moral psychology in AI safety are plentiful and are certainly not fully captured here.", "mini_description": "Seeks to understand morality as a human phenomenon at the level of the mind and/or brain. It looks to answer questions about moral development, motivation, reasoning, and character. "}, {"id": "00502", "title": "Applied Ethics", "description": "Applied ethics functions and is needed at every level of AI safety as all aspects of technology design are value-laden. Given this, and for the goal of beneficial AI to be realized, we must be both mindful and vigorous with the application of our normative and metaethical views to the construction and implementation of AI. This will help to ensure we continuously hone in on beneficial intelligence and that we don't ride rough-shot over important ethical considerations and human values. Succeeding in this will likely require a multifaceted approach including the generation an ethics culture around AI design and which includes some level of self-monitoring in academia and industry, institutional and governmental policy, watchdogs and ethics boards who seek to evaluate the impact of AI, and more. Some points where practicing applied ethics today is crucial are at the following: algorithmic impacts, autonomous systems decisions, societal impacts, power dynamics, privacy and data rights, bias, etc.", "mini_description": "Is the application of normative and metaethical views to specific cases and situations to determine the moral status of said case or situation, or to decide what ought to be done.", "breakdowns": [{"id": "005020", "sub_nodes": [{"id": "0050200", "title": "Etc... ", "description": "", "mini_description": "Applied ethics covers a vast and wide space. Not all branches of applied ethics can be covered here."}, {"id": "0050201", "title": "Ethics of Technology", "description": "AI functions as a powerful optimization process which may bring about some of the most rapid and profound shifts ever seen in human history. Furthermore, the power granted by solving the problem of intelligence is not an intrinsic good, as hypothesized by Nick Bostrom's Orthogonality Thesis. For beneficial AI to be developed, intelligence must be aimed at ethical and benevolent ends and not be undirected or malevolent.", "mini_description": "Seeks to adjudicate the value and moral impacts of technologies.", "breakdowns": [{"id": "00502010", "sub_nodes": [{"id": "005020100", "title": "Etc...", "description": "", "mini_description": "Applied ethics covers a vast and wide space. Not all branches of applied ethics can be covered here."}, {"id": "005020101", "title": "AI technologist ethics", "description": "As the architects of extremely powerful systems, which will be agents and potentially even patients of morality, AI researchers and developers bear a special ethical responsibility. This responsibility calls on AI researchers to be ethically informed, embodied, and engage in their work with the end goal of beneficence. The amount of care and effort put into the ethical design of beneficial AI systems should be commensurate with the expected impact (Lin 2017).", "mini_description": "Studies ethics with regards to the creation and implementation of AI in the world. "}, {"id": "005020102", "title": "Machine ethics", "description": "There exists disagreement over whether or not machine ethics is currently the right approach to generating value-aligned AI, or if it ever will be (Yampolskiy 2012). It may be very difficult or impossible to take control back from deployed autonomous AI systems with some form of machine ethics. Similarly, its not clear how we will be able to modify AI systems behavior and ethics should we wish to improve upon it. On the other hand, the dream of many AI researchers is the deployment of fully autonomous, moral AI systems which could take jobs that are both undesirable and dangerous (Anderson and Anderson, 2011). If we are serious about developing human-level artificial intelligence, truly autonomous systems, or super intelligence, then it is clear that we must get our meta and normative ethical theories straight and find out the most secure and robust way of instantiating it in AI through top-down, bottom-up, or mixed approaches (Wallach 2009).", "mini_description": "Studies morality and how to implement it in robots and AI."}, {"id": "005020103", "title": "Transhumanism", "description": "As AI becomes more powerful across more domains, our ability to augment and change the human, into what might one day be a posthuman state, greatly improves. AI will likely be a catalyst for opening a whole new set of ethical issues surrounding human augmentation, designer babies, identity, and more. AI may give previously inactionable ideological systems the ability to finally strive towards their ends (Bostrom 2005).", "mini_description": "Is an intellectual movement which seeks to use science and technology to fundamentally improve the human condition through augmenting or changing human biology.", "links": [{"id": "006010106"}]}, {"id": "005020104", "title": "Geoengineering", "description": "Advanced AI systems may provide unprecedented capabilities for manipulating the planet. Given that AI may help to seriously mitigate or combate certain civilizational risks, interesting questions arise regarding optimizing fund allocation between AI research and other general risks.", "mini_description": "Is an interdisciplinary study and attempt at intervening with Earths atmosphere to combat global warming and other climate problems. In the broader sense, geoengineering refers to projects which attempt to change the Earth itself on large scales, such as through climate, earth, or water engineering."}, {"id": "005020105", "title": "Nanotechnology", "description": "Advanced nanotechnology could have enormous consequences for life on Earth. In its most benign and beneficial forms, nanotechnology can be used in medicine to deliver drugs, for optimizing fuel cells and batteries, and fuels, and in assisting space flight through novel materials. In its worst, nanotechnology can be used for military applications and for delivering poisons. We should be careful to consider the AI research space and avoid or guard areas which may pose serious information hazards for developing malevolent nanotech.", "mini_description": "Is a still developing area of research and technology which attempts to create devices and machines which function at the molecular and atomic scales."}]}]}, {"id": "0050202", "title": "Emerging ethics", "description": "Advancements in science and technology bring us to new understandings of the universe, ourselves, and thus to new ethical dilemmas which did not exist or which we were not aware of in the past.", "mini_description": "Covers new moral situations that are just beginning to be explored or conceptually formalized.", "breakdowns": [{"id": "00502020", "sub_nodes": [{"id": "005020200", "title": "Etc... ", "description": "", "mini_description": "Applied ethics covers a vast and wide space. Not all branches of applied ethics can be covered here."}, {"id": "005020201", "title": "Unknown future ethics", "description": "Could our earliest ancestors have ever of dreamt of multiverse theory or that the metals and minerals of the mountains and earth would be built into thinking machines powered by the same forces that created lightning in the sky? If not, they could not have anticipated ethical issues resulting from discoveries or inventions such as these either. We might then ask: Are there such advances and novel situations that we are unable to predict? What might they be? How much should this impact our current decision making and actions with regards to AI?", "mini_description": "Seeks to prepare for technological and scientific discovery that will likely bring us to a whole host of new and perhaps exotic ethical situations and dilemmas. "}, {"id": "005020202", "title": "Deep future ethics", "description": "Potentially existing on cosmological timescales, life, knowledge regarding the universe, and even the universe itself are all guaranteed to change in fundamental ways, perhaps even to the point that they may become unrecognizable. Considerations regarding AIs ability to function ethically towards and respond to novel situations should be undertaken commensurate with the expected likelihood of said novel moral situations.", "mini_description": " studies how actions and decisions made today may have enormous implications for future persons and life itself. "}, {"id": "005020203", "title": "Population ethics", "description": "Population ethics becomes more and more relevant and imperative as humans and AI systems become more capable of finely controlling resources and their distribution, as well as the birth rates of the sentient beings they have causal efficacy over. Questions that are central to population ethics include ones regarding the lifetime value and utility of certain beings over others, whether we ought to be making people happy or generating more happy people, the ethics of having children, how we ought to distribute our limited resources across populations, the value of generating lives not worth living, etc. Some issues that are fairly central to population ethics are the Repugnant Conclusion and the non-identity problem. For illustrative purposes of population ethics,  the Repugnant Conclusion states that, For any possible population of at least ten billion people, all with a very high quality of life, there must be some much larger imaginable population whose existence, if other things are equal, would be better even though its members have lives that are barely worth living (Parfit 1984).<br><br> In the long-term, powerful AI systems may be able to influence and manipulate human population and our birth rates. Such systems will need to suggest or commit to certain forms of population ethics based on its own or our own moral analysis. If such powerful AI systems are created then we will need to contend with potentially alien and disturbing solutions to or correct answers for population ethics generated by our machines. It is suggested that one logically possible conclusion superintelligence may come to is that non-existence is in the best interest of all Earth-originating biological organisms due to the prevalence of suffering in our lives. (Metzinger 2017) It may then choose to act benevolently and euthanize us with preference towards some new form of sentient life. This says nothing of the likelihood or plausibility of such a scenario, but is useful for engaging ourselves in critical questions about ethical AI, our values and moral intuitions, our normative and metaethics, and what it means to genuinely create value aligned AI.", "mini_description": "Studies the ethics of resource limited populations spanning across space and time.  "}, {"id": "005020204", "title": "Exotic qualia ethics", "description": "Lacking a science of consciousness, there is broad disagreement about what is required to generate consciousness in machines and how consciousness relates to or is contingent upon computation. Given this, we should be mindful going forth as we create more and more powerful AI systems as we may generate known and unknown forms of suffering at digital timescales. Thus, it may be wise not to risk experiential/phenomenal states in machines without first having a better understanding of the neural/computational correlates of consciousness. Human history can be viewed as having long eras where institutions and practices that generate vast amounts of suffering are viewed as normal and morally acceptable. Things of this nature range from slavery, oppression of women, genocide, racism, factory farming, and more. It would be wise and prudent to carry on into the future with the history of our continual moral failings in mind so that we do not generate a novel and unseen ethical catastrophe through AI.<br><br> As we approach the longer term and create machines of human level intelligence or beyond, we may begin generating completely novel experiential states in machines which have no analogue in human consciousness due to the alien computational structures and algorithms in said AIs. Consequently, a deeply difficult issue arises regarding ascertaining and adjudicating the moral value and ethical status of feelings, hedonic states, and experiences that are foreign to human consciousness.", "mini_description": "Studies how the moral status of feelings, emotions, and experiences foreign to human consciousness might be evaluated. "}, {"id": "005020205", "title": "Simulation ethics", "description": "While not pertinent today or in the short term, powerful AI systems may one day be able to run world simulations populated by sentient beings, generate artificial minds, and may also make crucial breakthroughs in cosmogony that reveal we are within the context of a simulation. Putting aside the likelihood of either scenario, such possibilities function to elicit questions about potential hazards and pitfalls of an AIs ethics functioning in new contexts and with new dilemmas (Bostrom 2014).", "mini_description": "Studies the ethics of emulating and instantiating minds on computers and what we ought to do should we find ourselves in such a simulation."}, {"id": "005020206", "title": "Infinite Ethics", "description": "In the very long term, something like superintelligence may have to make novel ethical decisions given information regarding the newest breakthroughs in cosmology. We may wish to ask ourselves, how are we to and what does it mean to create value aligned AI with such possible future scenarios in mind?<br><br> For any rational agent able to re-derive or weigh ethics, the question of infinite ethics arises, i.e. how to treat the potentially overwhelming significance of existing or potential sentient entities in what is potentially an infinite universe. There are solutions that utilize an agent's uncertainty and solutions that cite causal lightcones, both enabling the accessible to garner most moral attention, but there are still some open questions (Bostrom and others 2011).", "mini_description": " studies the ethical implications of there potentially being an infinite number of universe with an infinite number of other sentient beings. "}, {"id": "005020207", "title": "Suffering Risks Ethics", "description": "Most people are willing to admit that there are some things that are worse than death, such as suffering greatly for many years at a time with minimal to no relief. Given this, many view suffering risks of indefinite duration as being worse than mass extinction (x-risks). If we believe this to be the case then s-risks can be categorized as the worst of all possible known things in our universe. We should thus be mindful of them as we venture into unknown technological and ethical territory, especially in the case of AI. <br><br> S-risks seem to be especially prevalent in advanced AI systems, such as superintelligence, due to the posited ability of superintelligence to rapidly copy software/generate minds, strongly optimize large spaces in arbitrary directions, and perform astronomical amounts of computations at digital timescales, rather than biological timescales (Bostrom 2014).These properties and features can generate s-risks  through sentient simulations, suffering subroutines, suffering routines, the spread of wild animals, and black swan events. (Tomasik 2017) This is highly speculative and, like other concepts mentioned here, is to say nothing of the likelihood of such things, but is brought up as a seeming logical possibility given some assumptions about consciousness and the possibility of machine superintelligence. This deserves attention as we proceed with AI in order to determine their likelihood and how we might mitigate such risks.", "mini_description": "Refers to the ethics of potentially generating astronomical amounts of suffering into the deep future through some technological value misalignment, unforeseen consequence, failure of control, or even malevolent use case. Suffering risks, commonly referred to as s-risks, are a subclass of existential risks."}, {"id": "005020208", "title": "Animal ethics", "description": "This is almost certainly a massive moral blind spot of humanity and a huge moral failing. Perhaps AI can be used to help address this issue by generating cost-effective synthetic meat and novel alternatives. AI and algorithms can also possibly be deployed to figure out the most effective means of communication for eliciting empathy and teaching sentience benevolent ethics (Singer 1981).", "mini_description": " studies how non-human animals ought to be treated. "}]}]}, {"id": "0050203", "title": "Bioethics and professional ethics", "description": "There are many open questions regarding the ethical use of AI in medicine and in the workplace. Some AI systems in this context which are generating novel ethical issues are: autonomous cars, drones and autonomous weapons, elder care robots, home assistants, data-driven and deep learning driven advertising, autonomous surveillance systems, and more.", "mini_description": "Refers to the ethics of emergent issues in biology and medicine as well as in the workplace.", "breakdowns": [{"id": "00502030", "sub_nodes": [{"id": "005020300", "title": "Etc...", "description": "", "mini_description": "Applied ethics covers a vast and wide space. Not all branches of applied ethics can be covered here."}, {"id": "005020301", "title": "Business ethics ", "description": "As businesses employ more and more advanced machine learning algorithms and eventually potentially even genuine AI, there will always exist issues regarding bias, discrimination, privacy, and manipulation when such companies have access to large amounts of data on the public.", "mini_description": "Studies morality in professional settings and the workplace. "}, {"id": "005020302", "title": "Military ethics", "description": "Currently, there are ethical concerns about the creation and use of lethal autonomous weapons. These are weapons systems which are able to kill targets without meaningful human control. As AI becomes more powerful and we begin to better understand intelligence, AI can be used to wage war in all of the traditional ways, and even in novel ways, to a degree that surpasses human creativity, capacity, and cruelty. Given that intelligence can be used for arbitrary ends, this branch of ethics is important as it again emphasizes the need for beneficial intelligence, and not undirected intelligence.", "mini_description": "Studies morality in war and conflict. "}, {"id": "005020303", "title": "Political ethics", "description": "As political institutions and organizations gain access to more powerful algorithms and AI there will be open questions regarding the ethics of manipulation, campaign targeting, privacy, democracy, and more that we will need to consider.", "mini_description": "Studies ethics with regards to public officials, their actions, and the laws and policies they introduce. "}, {"id": "005020304", "title": "Bioethics", "description": "AI is finding more and more practical uses in medicine and biology. There is a rising interest in elder care robots and algorithms are being deployed for diagnosing diseases, assisting in clinical decision making, research, and more. In the long-term, advanced AI systems may be able to help with iterative embryo selection, genetic engineering, human augmentation and more. These issues promise to bring a whole host of new ethical challenges to biology. ", "mini_description": "Studies morality  in medicine and biology."}, {"id": "005020305", "title": "Medical ethics", "description": "Central to medical ethics is the Hippocratic Oath, which lays the foundational principles for medical professionals. This foundation of medical ethics is built upon non-discrimination of persons due to background and a respect for autonomy, non-maleficence, beneficence, and justice. As more powerful AI systems are generated and give us more causal efficacy over sick persons, questions will continue to arise regarding the ethical use of new technologies with respect to these principles. As these AI systems become more autonomous and intelligent in the medical system, it will  be important to ensure the ethical foundations of medicine are embodied and fulfilled. ", "mini_description": "Studies the rights of persons and patients to medicine as well as the duties and moral obligations of doctors and medical researchers."}, {"id": "005020306", "title": "Research ethics", "description": "Research progress on fundamental and large scale issues can have enormous impacts on society. Both in the long term and short term, AI promises to bring profound shifts to the human condition. Given the scope and scale of impact that can come about from AI, it is hugely important that a sophisticated research ethics is developed and practiced. The societal implications of new AI systems should be carefully considered. Research areas which may prove to be information hazards ought to be carefully considered. Research coordination ought to be done in order to optimize topic research for the most morally salient and impactful topics. In the long-term, if machines become or ambiguously enter the space of moral patients then it may become relevant to also consider ethical implications of risking phenomenal/subjective states in machines.", "mini_description": "Studies the ethical dilemmas that arise naturally from research. Some examples of ethically salient aspects of research are human subject testing, animal testing, data fabrication, regulation, societal and social impacts of research."}]}]}]}], "links": [{"id": "00503"}]}, {"id": "00503", "title": "Normative Ethics", "description": "Normative ethics can be understood at a variety of levels: It can be instantiated in and internally developed by machines and humans, it is a structure through which we can ascertain and adjudicate the moral worth of actions committed by and moral status of AIs and AI architects, it can provide guiding principles and structures for deciding how to proceed into the future, and is also a system to be constructed, developed, and analyzed downstream of ones own metaethical views as it results from such views. One can also use it to make manifest what is good, true, and/or useful about morality. If we wish to one day realize authentically autonomous AI systems, they will need to learn, have instantiated in them, or some combination of the two, some form of normative ethics in order to guide behavior and generate ethical decisions in novel situations (Wallach 2009). We can consider that this would make them ethical decision makers, as opposed to ethical reasoners (McDermott 2008). <br><br> It may also be the case that AIs will need to modify their normative ethical structures and concepts to operate effectively in and be sensitive to the normative human ethical contexts they find themselves in. For example, if AIs develop novel normative ethical structures that dont use human concepts such as rights, duties, property, obligation, etc. then this may be ineffectual and even disturbing in human contexts even if such normative AI ethics properly reflects its own our our own metaethics. Given this, AI normative ethics may also need to evolve mindful of descriptive accounts of human normative ethics and frameworks. ", "mini_description": "Examines and contructs the principles and ethical systems we use for assessing the moral worth and permissibility of specific actions and situations.", "breakdowns": [{"id": "005030", "sub_nodes": [{"id": "0050300", "title": "Virtue Ethics ", "description": "Given that purely consequentialist and deontological normative ethical structures could potentially be computationally intractable without heavy optimization, it may be useful for AIs to generate novel concepts of virtue and also learn those already employed by humans. Actions, intentions, and beliefs of the AI and of persons in general could be correlated with good or desirable outcomes in order to formalize new agential characteristics of virtue and understand old ones.  The development of AI virtues can thus function as a bottom-up approach to machine ethics where the AI system asks, What does it mean for me to be a virtuous AI system? What would a virtuous AI system do? What do humans expect to be the qualities of a virtuous AI system? New and old virtuous characteristics could be used in junction with other normative ethical modules, such as consequentialist, deontological, and unknown possible modules, to maximize the likelihood of a desirable outcome.", "mini_description": "Focuses on aligning ones own actions and character with that of a virtuous person. "}, {"id": "0050301", "title": "Hedonism", "description": "Hedonists would argue that the end goal of AI ought to be the maximization of pleasure and happiness for itself and/or the world. Given this view, a hedonic appraisal and adjudication module may wish to be created for ascertaining the value and worth of various pleasurable hedonic states, understanding the causes and conditions which are likely to make them arise, and working towards reiterating them as often as possible in the AI itself and/or in the world.", "mini_description": "Holds that the grounding of morality is in an ethic of maximizing happiness and pleasure and minimizing pain and suffering.", "breakdowns": [{"id": "00503010", "sub_nodes": [{"id": "005030100", "title": "Cyrenaic Hedonism", "description": "This form of hedonism is largely viewed as short lasting, damaging, and unsustainable, at least in humans and biological life. For these reasons, it is almost never suggested or defended. In the long-term, as we potentially gain more and more access to different areas of mind-space with different computational structures running on different substrates, it may then become a thing of the past that hedonism such as this is inherently damaging and unsustainable. Perhaps one day, machines and persons can function on informationally sensitive gradients of bliss and this form of hedonism could be realized, though this is highly speculative (Pearce 2015).  ", "mini_description": " is an indulgent form of hedonism that supports reiterating more basic and primal pleasures as often as possible, like food, sex, and sleep. It focuses on immediate gratification without much care for the future. "}, {"id": "005030101", "title": "Epicureanism", "description": "This form of hedonism is largely viewed as sustainable, healthy, and a project which can last a lifetime. For these reasons, Epicureanism is largely the form of hedonism practiced by certain contemplatives. Just as the Cyrenaics, the Epicureans are interested in how AI can be leveraged in the short and long term to aid, assist, and help carry out this project.", "mini_description": " rejects the indulgent nature of the Cyrenaics and can be viewed as a more refined version of hedonism. Epicureanism holds that the greatest wellbeing and happiness can be developed through moderation, caution, and a mindfulness directed toward future consequences of actions in the present moment. "}]}]}, {"id": "0050302", "title": "Consequentialism", "description": "It is unclear whether a purely consequentialist approach to AI ethics is, by itself, a viable solution to value alignment. This is due to the potential computational intractability of accounting for complex systems and large amounts of causes and conditions that lead to various outcomes. In response to this, parameters can be set to limit the extent to which an AI dedicates resources to computing possible outcomes and their likelihoods.This can be done through intelligent heuristics that set rational boundaries for time spent on reflection and may thus help with cutting down on computational complexity (Wallach 2007). Furthermore, some forms of consequentialism may be more viable than others, such as two-level utilitarianism and rule utilitarianism as they are the formalization of commitment to, at least in part, higher level principles and generalized rules that can be shared and learned relatively easily. When an agent is able to act or participate in morality based on previously agreed upon or discovered and formulated rules or maxims, said agent cuts down on computational requirements by avoiding the need to continuously take into account all relevant and even possible consequences of actions in new situations.<br><br> There will be cases where purely consequentialist accounts of what ought to be done will be far more tractable, such as with big data driven bioethics in resource limited contexts. In the end, it may also be the case that some mixture of virtue ethics, consequentialism, deontological ethics, and unknown ethical structures will be ideal as they are each more or less computationally efficient in different domains. There has been very little research regarding the computational viability of and means of computerizing top-down approaches (Wallach 2007). ", "mini_description": "Encompasses ethical systems which focus on the consequences of actions as being the ground by which an action is determined to be ethical or unethical.", "breakdowns": [{"id": "00503020", "sub_nodes": [{"id": "005030200", "title": "Utilitarianism", "description": "Utilitarianism is generally an attractive normative ethical theory in the context of AI because it turns ethics into a simple calculation: add up all the happiness in the world and subtract out all of the suffering and choose the actions which produce the highest net positive impacts on the world. While this appears to be an attractive and simple solution, there are open questions regarding the epistemic and computational feasibility of such a normative framework. Given that there are many possible forms of utilitarianism, some types, coupled with useful heuristics, probabilistic reasoning, and optimized inductive and deductive reasoning, will be more or less tractable than other forms. Furthermore, defining and effectively measuring which experiential states count as valuable and which count as unvaluable is notoriously difficult. More research is needed on the subject (Wallach 2009).<br><br> In the long term, if a purely utilitarian AI ethics is achieved then we will need to be mindful of the urgency for AIs to explain calculated decisions in ways that are sensitive to human morality, moral psychology, and concepts. If we are unable to follow the utility calculus of an advanced AI, its ethical decisions may seem strange, foreign, and even unethical to us when they are really rational, wise, and morally benevolent. This may even moreso be the case if the causal implications of a proposed action are unclear to humans. A potential resolution to such a situation would be to develop an AI-human-repertoire where AI systems can explain to us, as a parent explains the necessity of certain decisions/actions to a child, the calculus and causal impacts in terms that we can understand.", "mini_description": "Holds that actions which maximize utility, usually being happiness or wellbeing, are ethical and the proper course of action. ", "breakdowns": [{"id": "0050302000", "sub_nodes": [{"id": "00503020000", "title": "Act Utilitarianism", "description": "In its purest form, where we choose the best action out of all possible actions, act utilitarianism is potentially the most computationally intractable form of utilitarianism.Corners would have to be cut, clever statistical tricks generated, and heuristics developed for this form of utilitarianism to be possible. Given this, it may be the case that in order to be the best act utilitarian one must also employ other forms of normative ethics to assist in the epistemic difficulties of act utilitarianism.", "mini_description": "Holds that the appropriate action or act is that which produces the greatest possible good. "}, {"id": "00503020001", "title": "Preference Utilitarianism", "description": "An agents preferences can be inferred or learned through their observed behavior, speech, voting, a broad understanding of convergent agential goals and sub-goals, and even directly from their brain/hardware/biology. The aligning of AIs actions and goals to human preferences can be understood at two levels. The first level would be aligning and learning morally low impact preferences for mundane things. This is the sort of preference alignment that Soundcloud, Siri, or Youtube might do where the algorithm or AI recommends new content based on previous preferences and functions as an intelligent assistant. This is a form of aligning individual AI systems or algorithms to the preferences of individuals at a low level.<br><br>  The second form is where we attempt to teach fully autonomous general purpose systems human preferences and how to reconcile competing preferences of both low and high moral impact so that it can function in a way that we view as value aligned. This moves the causal capacity of an AI system beyond that of a mere assistant and to one where the AI is making important moral decisions across a wide range of domains. This second form can be viewed as the long-term form of value alignment. Difficult moral questions arise regarding the sourcing of persons for preference alignment of this second form: Whose values are we aligning to? Who gets to decide? How should we weight differing preferences in morally gray spaces? How do we generate a moral hierarchy and reconcile mutually exclusive and antithetical preferences?  ", "mini_description": "Focuses on maximizing the satisfaction of agents desires."}, {"id": "00503020002", "title": "Hedonic Utilitarianism", "description": " Hedonic utilitarianism is less of a novel structural form of utilitarianism, such as two-level utilitarianism or rule utilitarianism, but is one which has the goal of such normative frameworks baked into it. This form, and others like it, can be viewed as the direct consequences of or heavily influenced by specific metaethical positions, such as views on value, the significance of joy and suffering,  and moral realism/anti-realism. In the short term, it would seem that hedonic utilitarianism doesnt have much practicality and implementability. Yet, what it is useful of considerations on this subject are the various sorts of ethical and moral intuitions they stir and bring to light. <br><br> In the long-term, if superintelligence is created then we may gain access to immense power and intelligence. Such power and intelligence may allow us to optimize some large portion of the cosmic endowment, potentially hundreds of thousands of galaxies, over the life of the universe to some end (Bostrom 2014). If we are sympathetic to the view of hedonic utilitarianism then we would want to optimize this space for pleasurable experiences. One proposed solution to this is to isolate the most simple computational structures responsible for the best hedonic states and instantiate this structure throughout the cosmos using solar system upon solar system to this end. What we would end up with is some fraction of the universe tiled with contentless bliss and euphoria humming away until the heat death of everything.<br><br>  This raises some important questions for us: What does it mean for there to be a best hedonic state? How do we operationalize that? Is there ultimately such a thing as a best hedonic state? What is good and what is bad about a world that looks like this? How are we to ground such evaluations of such a world? What is the importance of narrative and story in conscious experience? Are hedonics alone what we want or what is good? Or do positive hedonic states need to be paired with concepts, narrative, music, action, a self, etc.? Should we violate a beings consent if attribute a high probability to some action increasing net happiness? ", "mini_description": "Posits a hierarchy of pleasures or experiences that are to be maximized according to their corresponding value and worth. "}, {"id": "00503020003", "title": "Two-level Utilitarianism", "description": " In practice, as a fusion of act and rule utilitarianism, two-level utilitarianism may be optimized to deploy the most epistemically useful and virtuous aspects of each respective theory in the right contexts. This can help with potential epistemic difficulties found in other forms of utilitarianism. ", "mini_description": " is the unification of rule and act utilitarianism. It holds that the acting in accordance with intuitive general rules and critically examining which actions will bring about the most good are both together essential. "}, {"id": "00503020004", "title": "Rule Utilitarianism", "description": "If specific rules can be thoroughly correlated with specific outcomes or labeled good or bad, then AIs can generate probabilistic models of the efficacy of certain rules in certain cases. Employing something like this always or part of the time may help with making utilitarianism more computationally tractable.", "mini_description": "Holds that the appropriate action is that which embodies and works to actualize some rule which is conducive to the greatest good. "}, {"id": "00503020005", "title": "Negative Utilitarianism", "description": " It may be the case that we discover there to be or decide that there is an asymmetry between suffering and happiness. If this turns out to be the case then there may be a special moral obligation to reduce suffering in the world before we work to maximize happiness. ", "mini_description": "Focuses primarily or even exclusively on reducing aggregate suffering prior to maximizing pleasure or wellbeing."}, {"id": "00503020006", "title": "State Utilitarianism", "description": "Forms of utilitarianism, such as state consequentialism, can, and have historically, gone terribly wrong. We should be careful in our utility calculus that the needs and welfare of current and future sentient beings are not sacrificed in the service of abstract ethical, political, social, or other conceptual ideals. ", "mini_description": " assesses the moral worth of an action based on how much it maximizes the welfare and good of the state in relation to other possible actions. "}, {"id": "00503020007", "title": "Situation Ethics", "description": "The role of love in AI architects and AIs themselves is important and deserves more research and attention. ", "mini_description": "Focuses on examining the morality of a given act through the context in which it is located and places an emphasis on the cultivation of unconditional love. "}]}]}, {"id": "005030201", "title": "Egoism", "description": "Egoism can be undertaken from a variety of positions. One might pursue or advocate for egoism due to the fact that individuals are best placed to know what they ought to pursue for themselves or that they have a fundamental right to pursue their own interests.These positions can be advocated for from the basis of being most rational, being most conducive to individual good, or because this is most likely to maximize aggregate welfare and wellbeing across all persons.  ", "mini_description": "Holds that maximizing the good for ones self should play a primary or the only role when undertaking actions. "}, {"id": "005030202", "title": "Altruism ", "description": "The important distinguishing feature of altruism, relative to utilitarianism, is that it does not hold the moral agent herself and from her perspective as playing a crucial role or even any role at all in moral consideration. The effects and implications of this distinction should be better understood. If there are AIs that are not patients of morality then this may be a good normative framework to employ. ", "mini_description": "Holds that the individual should undertake those actions which brings about the most good for the greatest number, excluding herself from central consideration. "}]}]}, {"id": "0050303", "title": "Deontology", "description": "Deontology may prove to play a foundational role in getting  machine ethics off the ground and with helping to mitigate epistemic difficulties intrinsic to other normative ethical theories through providing decision support and direction. This is due to the fact that deontology uses high level concepts which require broad and general understandings of persons, situations, and consequences. However, forms of deontology which take into account the importance of intention or other difficult to infer mental states may prove more challenging. <br><br>  AI may learn some form of deontology from humans in order to form a deontological ethical baseline or have some similar foundation hard programmed in. From this base, it can potentially generate novel deontological concepts, such as new virtues, rights, duties, and obligations for both persons and machines. Such concepts may prove to be epistemically useful and/or virtuous in maximizing or respecting deeper ethical intuitions we have, like the need to promote wellbeing and mitigate suffering. At bare minimum, it will be important for general purpose AI systems to understand deontological ethics so that they can be sensitive to human moral psychology and lived experience. ", "mini_description": "Judges the morality of an action or situation based on rights, duties, laws, and obligations persons possess and hold to one another and society at large. ", "breakdowns": [{"id": "00503030", "sub_nodes": [{"id": "005030300", "title": "Kantianism", "description": "While heuristics and general rules may provide some epistemic usefulness, sophisticated Kantian machine ethics may require much computation because Kants ethics requires an understanding of an agents intentions. This requires an understanding of human psychology and a sophisticated causal account of the world and persons. Of course, humans can do this rather efficiently with our brains at a very low scale and sometimes rather inaccurately. Improvements in AI regarding abstract concepts, reasoning, human psychology, one shot learning, and general algorithmic qualities can certainly cut down on computational issues surrounding such theories (Wallach 2009).", "mini_description": "Holds that in order to act morally one must be acting from duty and with the correct motivations, which are determined by Kants categorical imperative. The consequences of an action do not play a central role in this form of ethics. "}, {"id": "005030301", "title": "Contractualism", "description": "More research is needed on the role contractualism might play in value alignment. Potential contracts between AIs and humans themselves as well as among one another raise interesting ethical, legal, and computational questions. ", "mini_description": "Holds that morality should be based on agreements and contracts made between consenting individuals. "}, {"id": "005030302", "title": "Divine Command Theory ", "description": "The impacts of religion and dynamics between it and advanced AI research are unknown and potentially volatile. Most of the world consists of persons that are overtly or tacitly divine command theorists. This will very likely play a huge role in how both short-term and long-term value alignment is done. There may even be strong opposition to certain kinds of AI research from divine command theorists.  More research is needed on the interplay of religion and AI value alignment. ", "mini_description": " refers to a set of related theories that view gods nature and his edicts as the source of what is moral. "}, {"id": "005030303", "title": "Rawls' theory of justice", "description": "With the power that AI brings comes the ability to both exacerbate current injustices and generate new injustices through bias and systemic power structures, and yet also comes the ability to create new levels of wealth, equality, reparations, and justice. This is the dual-use nature of technology. AI and justice studies is a new and emerging discipline which deserves more time, consideration, and research. ", "mini_description": "Rawls pursues a theory of justice known as Justice as Fairness where he establishes two principles of justice. The first holds that every person should have equal right to the most basic and extensive liberty that is compatible with the similar liberty of others. The second holds that social and economic inequalities are to be arranged so that they are both: (a) to the greatest benefit of the least advantaged, consistent with the just savings principle, and (b) attached to offices and positions open to all under conditions of fair equality of opportunity. (Rawls 1971) "}]}]}, {"id": "0050304", "title": "Unknown possible ethical structures...", "description": "AIs may one day use strange hybrid or novel approaches to normative ethics, using both new and old theories with new and old concepts. ", "mini_description": "The theories covered in this work by no means exhaust the space of all possible normative theories. "}]}], "links": [{"id": "00502"}, {"id": "00504"}]}, {"id": "00504", "title": "Metaethics", "description": "Metaethics can be viewed as the endeavor to know what normative framework to use and when. It is thus something to be practiced by both AI designers and potentially even AIs. It is a topic rarely covered by ethicists and AI engineers in the context of value alignment and there is thus little to no research about it. In the short term, it might be the case that its relevance is low as the ethical impacts of metaethical disagreements for normative theory and applied ethics are limited in the world. In the long term, however, if superintelligence is created and the cosmic endowment can be optimized to some end then metaethics becomes crucially important. We can understand the importance of metaethics here as metaphysical and epistemological issues regarding what beneficial intelligence entails. To what end should we aim intelligence and why? This can be captured as debates between moral realist and moral anti-realist positions, which consequently have said metaphysical and epistemological implications in ethics and thus in what it means to be a beneficial AI. One cannot continue with the project of generating beneficial AI without a clear understanding of beneficial. This question largely exists in the domain of metaethics. <br><br> This may be the most difficult form of ethics to develop in machines as it is the most abstract, but more technical research is needed on abstract concept representations in AI. If metaethics is instantiated or developed in AI then AI will not only be ethical decision makers, but also ethical reasoners as well. This would be a necessary but not sufficient condition for creating beneficial AGIs and thus represents one of the most crucial parts to long-term value alignment. Answering the question of what is good and valuable in the world and why? is an interdependently necessary part of value alignment and may be one of few things left to contemplate once intelligence is solved. <br><br> Here are some important questions to consider here: What are the impacts of specific metaethical theories? How much would some position change our current paths on value alignment? How would it change technical AI research? What probabilities am I assigning to different metaethical theories? How does the interplay of uncertainty and potential impact of specific positions affect how we ought to proceed with value alignment? ", "mini_description": "Seeks to understand the nature of ethical statements, attitudes, motivation, properties, and judgements.", "breakdowns": [{"id": "005040", "sub_nodes": [{"id": "0050400", "title": "Semantic Theories ", "description": "If we are to create value-aligned and beneficial AIs in the long term then both humans and AIs must understand what is meant when one says You shouldnt turn humans into paperclips. Whether or not we actively think about it, moral sentences and statements are often said and interpreted in subtly and overtly different ways. This risks creating moral confusion and harm if human and AI semantic understandings of moral sentences are different amongst themselves and/or between each other.", "mini_description": "Seek to address the question of, What is the linguistic meaning of moral terms or judgments? They are primarily concerned with whether or not moral statements contain truth values or are arbitrary and subjective.", "breakdowns": [{"id": "00504000", "sub_nodes": [{"id": "005040000", "title": "Cognitivism", "description": "Varying semantic theories of ethics may have important epistemic impacts on computational metaethics. More research is needed regarding ethical semantics and AI. ", "mini_description": "Refers to a set of theories which hold that moral sentences express genuine propositions. This is to say they are capable of being true or false. This stands in opposition to non-cognitivism.", "breakdowns": [{"id": "0050400000", "sub_nodes": [{"id": "00504000000", "title": "Moral Realism", "description": "Moral realism has important implications in AI and human moral epistemology. If moral facts are about real features of the world then they may be discoverable and an objectively beneficial AI could be generated.  This position also leaves no room for cultural diversity regarding the moral status of any particular action and would thus impact how humans ought to practice value alignment.", "mini_description": "Holds that moral propositions are about robust, mind-independent facts. This is to say that moral propositions are facts about objective features of the world.", "breakdowns": [{"id": "005040000000", "sub_nodes": [{"id": "0050400000000", "title": "Ethical Naturalism", "description": "If moral properties stand in some metaphysical relation to non-ethical properties then a rigorous science and concrete moral epistemology may be possible for humans and AI systems.  More research is needed on the impact of such positions on value alignment in the short and long-terms.", "mini_description": "Holds that there are objective moral properties and that these are reducible or stand in some metaphysical relation to entirely non-ethical properties."}, {"id": "0050400000001", "title": "Ethical Non-naturalism", "description": "If this position turns out to be true, there are open questions regarding whether or not AI systems would also have the same a priori or intuitive knowledge of moral properties.  More research is needed on the impact of such positions on value alignment in the short and long-terms.", "mini_description": "Holds that there are objective and irreducible moral properties and that we sometimes have a priori or intuitive knowledge of these moral properties."}]}]}, {"id": "00504000001", "title": "Ethical Subjectivism ", "description": "The short-term implications of ethical subjectivism, should it be true, are unclear. From this context, the answer to the question of what we ought to do should this view be true will indeed be subjective and contingent upon the context in which it is asked, but it is unclear if we will continue down our current path of value alignment or radically change course because of this. ", "mini_description": "Is a form of moral anti-realism that holds that moral statements are propositions made conventionally true or false by the approval, disapproval, and/or conventions of individual persons or groups of people.", "breakdowns": [{"id": "005040000010", "sub_nodes": [{"id": "0050400000100", "title": "Ideal Observer Theory ", "description": "The view of coherent extrapolated volition is an attempt at addressing AI value alignment and can be viewed as an ideal observer theory (Yudkowsky 2004).  More research is needed regarding the ethical implications of ideal observer theory in the context of value alignment.", "mini_description": "Holds that what is right can determined by imagining the moral attitudes and beliefs of an ideal, perfectly rational, and well informed hypothetical observer. While this theory is considered subjectivist due to its reference to an ideal subject it still purports to provide universal answers to moral questions."}, {"id": "0050400000101", "title": "Divine Command Theory", "description": "The impacts of religion and dynamics between it and advanced AI research are unknown and potentially volatile. Most of the world consists of persons that are overtly or tacitly divine command theorists. This will very likely play a huge role in how both short-term and long-term value alignment is done. There may even be strong opposition to certain kinds of AI research from divine command theorists.  More research is needed on the interplay of religion and AI value alignment. ", "mini_description": "Posits that what is moral is determined by a unique supernatural being, such as god. This theory, like ideal observer theory, purports to be universalist even though it is subjectivist. "}]}]}, {"id": "00504000002", "title": "Error Theory", "description": "More research is needed on the effects of error theory on value alignment as well as the computational and epistemic implications in machines. If error theory is true then there is no right way to do value alignment. The real question is, if error theory is true then how would this fact affect how we do value alignment?", "mini_description": "Being a cognitivist theory, error theory holds that ethical statements express propositions. However, its central claim is that all such propositions are false. This is to say that moral claims such as murder is bad or murder is good are false. The only moral statements which are not false are negative ones, such as murder is not bad and murder is not good."}]}]}, {"id": "005040001", "title": "Non-cognitivism", "description": "If there is no third person or objective way of evaluating the truth value of moral statements then there is no concrete ethical learning protocol for humans or AIs. In terms of value alignment, some evaluative system found in humans or that is constructed by us must make it into advanced AI systems if they are to be moral agents. Given in this context that there are no right or wrong answers to the question of what makes beneficial AI what we choose to do is something to be freely willed and decided upon without boundary. ", "mini_description": "Refers to a group of theories which hold that moral statements are neither true nor false because they do not express genuine propositions.", "breakdowns": [{"id": "0050400010", "sub_nodes": [{"id": "00504000100", "title": "Emotivism", "description": "Given emotivism, it is an open question whether or not the group who holds power over value alignment efforts would respect other persons' ethical expressions and preferences. More research is needed on the role that emotivism plays in value alignment.", "mini_description": "Posits that ethical statements are merely expressions of emotion and preference. To say that something is morally good or morally bad is merely to express approval or disapproval. "}, {"id": "00504000101", "title": "Quasi-realism", "description": "Quasi-realism may function similarly to moral realist positions in the actual world. They hold different ethical epistemologies because of metaphysical differences regarding the existence of moral facts, but quasi-realism may have a  conventional moral-epistemology very similar to moral realist positions because widespread adoption would occur in the same context. Given this, they could end up looking very similar in the end. The relevant question is whether or not human morality would become much more fluid and dynamic given widespread understanding and commitment to quasi-realism. This would affect the scope and impact of this theory should it turn out to be true and thus how much research efforts it deserves. It seems that persons that adopt or accept quasi-realism today do not experience much of a shift in ethics and so this appears to be a position that, should it be true, wouldnt affect the direction of value alignment very much.  ", "mini_description": "Holds that moral statements conventionally behave like propositions and can be called true or false within the context of human convention, but it is understood that they ultimately do not express genuine propositions. "}, {"id": "00504000102", "title": "Universal Prescriptivism", "description": "Universal perscriptivists view the primary function of morality as a means of recommending (prescribing) certain acts over others. For example, to say that one ought not to steal. universal prescriptivism is saying , Do not steal. For autonomous AI systems to be value-aligned, it will be important for them to understand nuanced semantic distinctions made by different humans.", "mini_description": "Holds that moral statements function like universalized imperative sentences. This is to say that whoever makes a moral judgement is committed to the same judgement in any situation where the same relevant facts obtain.  "}]}]}]}]}, {"id": "0050401", "title": "Substantial Theories ", "description": "A fundamental understanding of the relativism, nihilism, or universalism of ethics is required for a sophisticated account of beneficial AI. Without a metaphysical account of ethics, values, and the good it will be difficult for advanced AI systems to act in a way that is ultimately ethical. For intelligence to be aimed at some end, we must understand if some ends are better than others, why they might be better, or if there are only evolutionary and culturally contingent ethical modes of thought. ", "mini_description": "Are attempts at capturing the nature of moral judgements.", "breakdowns": [{"id": "00504010", "sub_nodes": [{"id": "005040100", "title": "Moral Universalism  ", "description": "Given this position, it would be prudent for humans to work to isolate and understand which ethical principles, codes, or values are universally applicable. The implications of this would somewhat negate the need for culturally sensitive AI systems.", "mini_description": "Is the metaethical position that states that ethical systems are universally applicable. This is to say, regardless of a persons culture, race, sex, religion, nationality, sexuality or other distinguishing feature, that said person is an equal patient and agent within the context of some system of ethics. ", "breakdowns": [{"id": "0050401000", "sub_nodes": [{"id": "00504010000", "title": "Value Monism ", "description": "This view and the view of value pluralism boil down to the question of whether or not there are more than one intrinsic values. In the context of value monism, the answer to this would be no. The central contentions between value monism and pluralism surround issues regarding what sorts of things constitute intrinsic values. If AI or humans are able to isolate and identify which features of reality are intrinsically valuable, if such a claim makes sense, then these properties, features, or aspects of reality can be optimized for in order to bring about the best of all possible realities. ", "mini_description": " holds that all values and good are commensurable on a single scale. "}, {"id": "00504010001", "title": "Value Pluralism", "description": "This view and the view of value monism boil down to the question of whether or not there are multiple intrinsic values. Value pluralism would answer that there are more than one intrinsic values. Given this, it would be the job of AI and/or humans to identify the space of all intrinsic values and navigate in a way that is appropriate and reflects the relevant metaethical and normative considerations.", "mini_description": "Holds that there are many different values which may be equally correct and fundamental, yet that exist in direct conflict with one another. For example, this view would claim that  the moral life of a nun is incompatible with that of a mother, yet there is no purely rational measure of which is preferable or morally superior. "}]}]}, {"id": "005040101", "title": "Moral Relativism ", "description": "For the world in which moral relativism holds to be true and is widely spread and adopted, what is important to wonder is how the adoption of this metaethical position changes and affects current moral attitudes as well as affects value alignment. The answer to this question is salient because it affects metaethical research prioritization in the present day. If a metaethical theory doesnt change the way in which value alignment is done, relative to the current general path, then its not of much consequential import. More research is needed in moral psychology and metaethics regarding the implications of such theories in value alignment. ", "mini_description": "Holds that all moral judgements are arbitrary and contingent aspects of different persons' culture or personal views. It argues that there is no objective moral standard by which one can assess the truth of a moral proposition. There exists only a plurality of different moral views which all lack the same fundamental grounding ability to be assessed and are not universalizable."}, {"id": "005040102", "title": "Moral Nihilism ", "description": "Given moral nihilism, there arent any strict rules or imperatives for how value alignment ought to be done. In this context, value alignment would potentially play out as a mere power struggle and historical contingency. However, it is also likely  that we would maintain the same ethics we had prior to a moral nihilist awakening. As stated, more research is needed regarding the scale and impact of moral nihilism on value alignment should it be proven true.  ", "mini_description": "Holds that nothing has intrinsic moral value, nothing is intrinsically good or bad, and that morality is a conventional fiction that lacks objective or relative truth. "}]}]}, {"id": "0050402", "title": "Justification Theories ", "description": "In terms of moral epistemology, such theories can either arise as relativistic historical contingencies or as truths to be accessed through one or another means. Understanding what is the true or appropriate moral epistemology is a necessary but not sufficient condition for creating autonomous artificial moral agents. In terms of moral motivation, there are open questions as to what underlying computational structures are needed for driving AIs to follow moral compunctions or truths.", "mini_description": " are attempts at understanding moral epistemology and motivation for acting in accordance with morality. It attempts to answer the question of how are moral judgments to be supported or defended and if possible, how does one make moral progress?", "breakdowns": [{"id": "00504020", "sub_nodes": [{"id": "005040200", "title": "Empiricism", "description": "If moral empiricism holds to be true then value alignment and machine ethics becomes objective and actionable. More research is needed regarding the epistemological implications of such a theory. ", "mini_description": "Holds that knowledge or morality is primarily gained through observation and experience. This can be understood as an attempt to reduce moral facts to non-moral facts, such as facts about the world, persons, and their opinions. "}, {"id": "005040201", "title": "Moral Rationalism ", "description": "Should this view hold to be true, all that would be needed to generate value aligned AI is to create a fully rational system. IT should then be able to reason out all of ethics. ", "mini_description": "Holds that moral truths are knowable a priori and through reason alone. "}, {"id": "005040202", "title": "Ethical Intuitionism ", "description": "Should this view be true, there would be the open question of whether or not true AGIs have ethical intuitions that automatically come along for the ride.", "mini_description": "Holds that some moral truths can be known without inference. This is to say that moral truths are arrived at through an innate intuitive awareness of value, right and wrong, and morality itself. "}, {"id": "005040203", "title": "Moral Skepticism ", "description": "This is a view which appears to be popular among technical AI researchers and has influenced the general approach to value alignment. If one is skeptical about ethics, views philosophy as a failed enterprise, and sees morality as merely politics by other means then all that is left are facts about the preferences that persons do hold or might hold given more information about the world. Given this view, one might take a very descriptive ethical approach to value alignment. ", "mini_description": "Refers to a class of moral theories that argue either there are no moral facts and thus there can be no justification of or learning about morality or that there are moral facts but it is impossible to acquire knowledge of such facts."}, {"id": "005040204", "title": "Ethical Motivation", "description": "Even if the optimally prescribed ethical theory structure and values are known, and though it may seem obvious to humans, the formal underpinnings and mechanisms of its logical motivation require hardening. An agent that knows what the optimal ethics is would not suffice: it would need to be causally compelled to follow those ethics (Soares 2016).", "mini_description": "Considers how to logically motivate adherence to an understood-as-proper ethical system.", "links": [{"id": "000010102"}]}]}]}, {"id": "0050403", "title": "Ethical uncertainty", "description": "The impacts of AI promise to be great. Thus, uncertainty surrounding high impact systems and implementations of such systems should be studied and dealt with accordingly.", "mini_description": "Attempts to address how to participate in and understand ethics given our incomplete analysis and information. ", "breakdowns": [{"id": "00504030", "sub_nodes": [{"id": "005040300", "title": "Normative Uncertainty", "description": "There is much disagreement about which metaethical views are correct. Given this uncertainty, there is consequently much uncertainty about the appropriate application of normative ethical theories. As long as there is metaethical uncertainty, there will be uncertainty in normative ethics as it is downstream of and its success is contingent upon true metaethics.<br><br> Ideally one would want to determine if there are any methods approaching objectivity for enumerating, generating, or ranking value systems. This kind of metaethics explores more first-principles approaches to ethics. Questions about the most proper thing for one to do when they're uncertain about what ought to be done in the first place arise (MacAskill 2014, Tegmark 2015). Figuring out what norms govern uncertainty about normative claims, and how can uncertainty about moral claims can be resolved are both open areas of investigation (MacAskill 2014). Answers to these questions are reasonable baseline proxies to help reason about what sorts of conclusions the operators would come to if they had much more knowledge and much more time to think and develop (Soares and Fallenstein 2014a, Bostrom 2014). These problems are largely still in the realm of philosophy rather than computer science, but interdisciplinary approaches would seem to be be called for in order to explore different angles on this problem.", "mini_description": "Seeks to understand the appropriate use cases of and provide a commitment to different ethical theories that guide our behavior in daily life given our moral uncertainty. ", "breakdowns": [{"id": "0050403000", "sub_nodes": [{"id": "00504030000", "title": "Normative Ethical Cartography", "description": "The space of all possible normative ethical theories is large and vast. Those that work well with humans may not work well with and for creating moral machines. ", "mini_description": " seeks to explore the space of all possible normative ethical theories. "}, {"id": "00504030001", "title": "Allocating Normative Moral Commitment", "description": "The importance of this scales with the degree of our moral uncertainty and the amount of causal efficacy we have over the world.", "mini_description": "Seeks to understand how to allocate probabilistic truth or rightness values to different normative moral theories given imperfect knowledge about the space. "}]}], "links": [{"id": "002020001"}]}, {"id": "005040301", "title": "Metaethical uncertainty ", "description": "Given that the rest of ethics is downstream of and contingent upon metaethics, it is important that we generate a formalized way of proceeding.", "mini_description": " seeks to address epistemic issues within metaethics and issues of implementing morality given an incomplete metaethics.", "breakdowns": [{"id": "0050403010", "sub_nodes": [{"id": "00504030100", "title": "Metaethical Cartography", "description": "The space of all possible metaethical theories is likely smaller than the space of all possible normative ethical theories. ", "mini_description": "Seeks to explore the space of all possible metaethical theories. "}, {"id": "00504030101", "title": "Allocating metaethical moral commitment", "description": "The importance of this scales with the degree of our uncertainty regarding some metaethical position, the amount of causal efficacy we have over the world, and the impacts of said metaethical position on what we end up doing.", "mini_description": "Seeks to understand how to allocate probabilistic truth values to different metaethical moral theories given imperfect knowledge about the space."}, {"id": "00504030102", "title": "Ontological ethical effects", "description": "In humans, differences in world models, mereologies, and other ontological dualistic concepts, understandings, and thought structures have great implications in our ethics and lived experience of the world. This can specifically be seen in the ethical consequences of the illusions of self and free will in humans and their impacts on justice, view of self and other, selfishness, and in responsibility and duty. In the long term, new understandings about the multiverse or parallel universes may require that we update our ethics to account for this new information.", "mini_description": "Seeks to update all of our ethical theories as we learn more about what exists in the world and its relation(s) to other existing things.", "breakdowns": [{"id": "005040301020", "sub_nodes": [{"id": "0050403010200", "title": "Ontological Value", "description": "Within philosophical ontology, there are questions remaining regarding the theoretical underpinnings of the nature of, and treatment of, worth and value. The structural relationships between worth, value, value evaluation, morals, ethical systems, and instantiated ethical systems are insufficiently resolved for the present purposes with long-term AI. Another significant open question is whether there can exist objective values, and if so, how can they be derived.", "mini_description": "Refers to the theoretical underpinnings of the nature and treatment of worth and value.", "links": [{"id": "002020002"}, {"id": "00202000101"}, {"id": "00202"}]}]}]}]}]}]}]}]}], "links": [{"id": "00503"}]}]}], "links": [{"id": "0000003"}, {"id": "00202"}]}, {"id": "006", "title": "Governance", "description": "How we choose to govern technological and social developments stems from the values we hold. Governance can play a key role in enabling value alignment between humans and machines, as well as in determining what and whose values will be implemented into law and practice, and how this will be carried out. Technologies themselves also structure and govern our world through silent, but powerful influence (Jasanoff, 2016). Governance can be broken down into three interrelated categories: coordination, policy, and law.   ", "mini_description": "Governance is broader than the direct reach of governments; it represents the norms and values held by society, which are structured through various formal and informal processes of decision-making to ensure accountability, stability, broad participation, and the rule of law.", "breakdowns": [{"id": "0060", "sub_nodes": [{"id": "00600", "title": "Coordination", "description": "What mechanisms exist to help align human and machine values? How is it that particular value systems emerge as dominant in regards to novel technologies? Who is part of these conversations and how are decisions made? Coordination in value alignment is about consensus-making through existing systems of influence. Strategies for the development of AI at the technological, international, national, and organizational levels help structure imaginaries of appropriate value systems.", "mini_description": "Coordination is the process of reaching a working consensus within a defined (though not static) group or category. It refers to the structures through which governance occurs. ", "breakdowns": [{"id": "006000", "sub_nodes": [{"id": "0060000", "title": "Technological coordination", "description": "Computer code and algorithms increasingly structure our social, financial, and political worlds (Lessig, 2000). However, technology companies are largely driven by profit and growth motives rather than resilience or human flourishing, and the idea that technology will naturally act in our interests is unwarranted (Ito, 2017). ", "mini_description": " Technology itself shapes our reality and defines what is possible within the spaces  both virtual and real  that we inhabit."}, {"id": "0060001", "title": "International coordination", "description": "Issues whose effects expand beyond the scope of individual states often require international coordination. Some bodies such as the European Union enable regional coordinated action, while bodies such as the United Nations enable global action, for example in the form of international treaties like the Biological Weapons Convention Treaty, The Paris Agreement on Climate Change, and the 1967 Outer Space Treaty.", "mini_description": "Coordination can occur among, between, or outside of the realm of nation-states.", "breakdowns": [{"id": "00600010", "sub_nodes": [{"id": "006000100", "title": "European Union", "description": "Existing regulation in the EU includes the General Data Protection Regulation (GDPR) that goes into effect May 2018 and will impact AI development because, for example, it will require algorithms to have a degree of explainability (GDPR, 2016). The EU also has a Robotics and Artificial Intelligence Unit, which is exploring additional regulatory options (Robotics and Artificial Intelligence, 2016).", "mini_description": "The political and economic union of European countries."}, {"id": "006000101", "title": "United Nations", "description": "The UN held a meeting about the role of AI in the economy in 2017 called The Future of Everything  Sustainable Development in the Age of Rapid Technological Change. Earlier in 2017, the UN was a partner of the AI for Good Global Summit, which discussed how AI innovation can help achieve the Sustainable Development Goals.", "mini_description": "An intergovernmental organization tasked to promote international cooperation and to create and maintain international order. "}, {"id": "006000102", "title": "Organisation for Economic Cooperation and Development", "description": "The OECD held a meeting in Paris October 2017 called \"AI: Intelligent Machines, Smart Policies\" to explore questions about what sort of policy and institutional frameworks should guide AI design and use, and how we can ensure that AI benefits society as a whole.", "mini_description": "The OECD is an intergovernmental economic organization with 35 member countries, founded in 1960 to stimulate economic progress and world trade."}, {"id": "006000103", "title": "Global Governing Body", "description": "Some people propose a new global governing body to proactively manage the opportunities and threats associated with AI. For example, the idea of a global leadership council on AI has been proposed by Sam Altman and others (Bostrom, Dafoe, Flynn, 2017).", "mini_description": "An institution or group that works towards political cooperation among transnational actors to manage concerns that affect more than one country."}]}]}, {"id": "0060002", "title": "National coordination", "description": "Various national efforts are emerging around the world to coordinate strategies for the development of AI. Countries are not all taking the same approach to harnessing and managing AI and examples of national efforts include a range of economic and political strategies, R&amp;D guidelines, discussions with citizens, and the establishment of regulatory instruments.", "mini_description": " The nation state remains a powerful unit of strategic coordination and governance. ", "breakdowns": [{"id": "00600020", "sub_nodes": [{"id": "006000200", "title": "The United States", "description": "A series of reports and workshops relating to AI were commissioned under the Obama Administration, but the Trump administration has not taken a public stance on AI. The US does have a new AI Congressional Caucus however, which hopes to pass a Future of AI Act and create a federal advisory committee at the Department of Commerce to examine issues relating to AI.", "mini_description": "The US is a country in North America with a population of more than 320 million people."}, {"id": "006000201", "title": "The United Kingdom", "description": "The UK Parliament has a Select Committee on AI currently exploring many issues relating to the development of AI. The UK Government also launched a new Centre for Data Ethics and Innovation, which will advise the government on ethical, safe, and innovative uses of data and AI (UK Industrial Strategy, 2017).", "mini_description": "A sovereign state in Western Europe that includes Great Britain (England, Scotland, and Wales) and Northern Ireland."}, {"id": "006000202", "title": "China", "description": "China published its New Generation Artificial Intelligence Development Plan July 2017. This plan outlines the countrys goal to become the world leader in AI by 2030.", "mini_description": "Officially called the People's Republic of China (PRC), China is a sovereign state in East Asia and the world's most populous country, with a population of approximately 1.4 billion."}, {"id": "006000203", "title": "The United Arab of Emirates", "description": "The United Arab of Emirates unveiled a national strategy for AI in 2017 revolving around Smart Government for all services and infrastructure.", "mini_description": "An absolute monarchy in Western Asia and a federation of seven emirates, with a population around 10 million."}, {"id": "006000204", "title": "Japan", "description": "Japan is developing guidelines for AI R&amp;D and industry (The Conference toward AI Network Society, 2017).", "mini_description": "A sovereign island nation off the East coast of Asia with a population of more than 127 million."}, {"id": "006000205", "title": "France", "description": "France launched an AI Policy Initiative that will submit recommendations to the government following an online consultation with citizens.", "mini_description": "Officially the French Republic, France is a Western European country with a population of around 67 million."}, {"id": "006000206", "title": "Finland", "description": "Finland has an artificial intelligence programme and intends to design a platform economy that runs on AI to greatly boost its economic development.", "mini_description": " A republic in Northern Europe with a population of approximately 5.5 million."}, {"id": "006000207", "title": "Estonia", "description": "Estonia is working on an AI bill that would give robots and AI legal status. ", "mini_description": "Officially the Republic of Estonia, this country is in the Baltic region of Northern Europe and has a population of approximately 1.3 million."}]}]}, {"id": "0060003", "title": "Industry and non-profit coordination", "description": "There have already been various attempts to coordinate the responsible development of AI from technology companies and non-profit research and standards organizations. These efforts have come both from within the technology industry, as well as from outside civil society organizations. Issues addressed include safety and efficacy standards, ethical and social considerations, and policy recommendations.", "mini_description": "The private sector has emerged as a powerful voice in national and global governance in recent decades, and private-public partnerships have become the norm in many instances.", "breakdowns": [{"id": "00600030", "sub_nodes": [{"id": "006000300", "title": "World Economic Forum", "description": "WEF has a program on artificial intelligence and robotics within its Center for the Fourth Industrial Revolution, which produces reports and holds events on the topic.", "mini_description": "An independent non-profit organization committed to improving the state of the world by engaging business, political, academic and other leaders of society to shape global, regional and industry agendas."}, {"id": "006000301", "title": "The IEEE Global Initiative for Autonomous and Intelligent Systems", "description": "The IEEE Global Initiative for Autonomous and Intelligent Systems developed a set of AI standards in November 2017 to address metrics for ensuring human wellbeing, developing methodologies to achieve ethically-driven nudging parameters, and establishing fail-safe mechanisms in autonomous and semi-autonomous systems (IEEE, 2017). The group also produced a report titled Ethically Aligned Design.", "mini_description": "The IEEE is the world's largest technical professional organization for the advancement of technology. This new initiative is tasked with exploring legal, safety, and ethical issues relating to AI and robots."}, {"id": "006000302", "title": "Future of Life Institute", "description": "FLI developed the Asilomar AI Principles in collaboration with many scientists, academics, and organizations following its Beneficial AI conference in January 2017. The Principles list 23 goals for the safe and beneficial development of AI (FLI, 2017).", "mini_description": "FLI is a charity and outreach organization based in Cambridge, Massachusetts working to ensure that tomorrows most powerful technologies are beneficial for humanity."}, {"id": "006000303", "title": "AI Now", "description": "AI Now produces annual reports and holds events relating to AI. The AI Now 2017 report has 10 recommendations for managing the profound social, ethical, and political implications of AI (AI Now, 2017).", "mini_description": "The AI Now Institute is a non-profit organization housed at New York University, which produces interdisciplinary research on the social implications of artificial intelligence. "}, {"id": "006000304", "title": "The Information Technology Industry Council", "description": "The Information Technology Industry Council developed a set of AI Policy Principles to advance the benefits and responsible growth of AI. (ITI, 2017).", "mini_description": "The Information Technology Industry Council is a Washington, D.C.-based trade association that represents companies from the information and communications technology industry and works to influence select policy issues."}, {"id": "006000305", "title": "The Partnership on AI", "description": "The Partnership developed a set of thematic pillars as well as a list of eight tenets (Partnership on AI, 2017).", "mini_description": "The Partnership on AI was created by leading technology companies such as Google, Amazon, Facebook, and Apple 'in order to study and formulate best practices on AI technologies, to advance the publics understanding of AI, and to serve as an open platform for discussion and engagement about AI and its influences on people and society.'"}, {"id": "006000306", "title": "DeepMind Ethics &amp; Society", "description": "DMES has outlined key ethical challenges arising from AI as well as a set of five core Principles (DMES, 2017).", "mini_description": "DeepMind created its Ethics &amp; Society group in 2017 because the company believes that Technology is not value neutral, and technologists must take responsibility for the ethical and social impact of their work."}, {"id": "006000307", "title": "The British Standard", "description": "BSI designed a guide on the ethical design and application of robots and robotic systems (BSI, 2016).", "mini_description": "BSI Group, also known as the British Standards Institution (or BSI), is the national standards body of the United Kingdom. BSI produces technical standards on a wide range of products and services, and also supplies certification and standards-related services to businesses."}]}]}]}]}, {"id": "00601", "title": "Policy ", "description": "What are the key issues and topic areas around which values come into play for our interaction with increasingly intelligent systems? And how do policies shape and respond to the goals of AI?", "mini_description": "Policy is a defined course of action, which may be proposed by a government, business, organization, or individual. Policies are typically arrived at following a process of deliberation regarding key considerations and stakeholders. ", "breakdowns": [{"id": "006010", "sub_nodes": [{"id": "0060100", "title": "Policy considerations ", "description": "Many of the critical policy considerations relating to AI are intimately connected to the question of value alignment. How can we ensure that AI agents do not pick up on our bad traits, or provide great power to nefarious actors?", "mini_description": "A set of considerations has emerged as particularly important in the debate about AI policy.", "breakdowns": [{"id": "00601000", "sub_nodes": [{"id": "006010000", "title": "Social and political control", "description": "Our actions online generate enormous amounts of data not only about what we buy, but also about what we think and want. Companies like Cambridge Analytica have shown that personalizing messages based upon this big data can have a lot of power in swaying political outcomes (Grassegger and Krogerus, 2017). Fake news is another important phenomenon in the ongoing story of efforts to manipulate social and political understandings. Both fake news and targeted messaging played a role in the 2016 US presidential election (Allcott and Gentzkow, 2017).", "mini_description": " Groups or individuals can exert control over social and political events through many kinds of tactics. Political propaganda, which aims to persuade people of a particular message that may or may not be accurate, has a long history and is alive and well today. By combining old psychological tactics with new social media and AI technologies, the capacity for much more widespread and targeted propaganda efforts is now being realized."}, {"id": "006010001", "title": "Bias", "description": "Algorithms trained on existing data can unintentionally magnify and institutionalize biases present in the world. For example, algorithms have been found showing ads for high-income jobs to men much more often than to women, and showing ads for arrest records more frequently under searches of distinctively black names (Hajian, Bonchi, and Castillo, 2016).", "mini_description": "Bias is prejudice in favor or against a person, group, or thing. Algorithmic bias occurs when the data being used to teach a machine learning system includes instances of bias, or when peoples own implicit values influence the collection, selection, or use of data."}, {"id": "006010002", "title": "Economic advancement", "description": "Numerous countries and companies are pivoting to strategies that put artificial intelligence at the center of their economic growth. Google now calls itself an AI-first company, while an independent report published by the UK government states that AI can bring major social and economic benefits to the UK, and that AI offers massive gains in efficiency and performance to most or all industry sectors.", "mini_description": "The advancement of a nations economic abilities, usually due to technological innovation, political shifts, or social investments."}, {"id": "006010003", "title": "Privacy", "description": " Ubiquitous cameras and tracking devices have led to less privacy in the outside world, while online tracking and the ability to uncover old information mean we have limited privacy online (Sanders, 2011). ", "mini_description": "The state or condition of being free from being observed or disturbed by other people or institutions."}, {"id": "006010004", "title": "Data ownership", "description": "Data is a valuable resource in todays world, but the people who generate and provide the data rarely own or manage it for themselves. The emergence of data commons, open source software, and companies that pay people for their data are some of the ways people are exploring alternative data governance models. Openness in AI is not a simple story, however, and carries its own risks (Bostrom, 2017).", "mini_description": "Having legal rights and control over the acquisition, use, and distribution of a set of data elements."}, {"id": "006010005", "title": "Accountability and liability", "description": "Even though machine learning algorithms are often described as a black box, there are ways to hold AI systems accountable via explainability (Doshi-Velez and Kortz, 2017).", "mini_description": "Accountability is the condition of being responsible for something. Liability means the same thing, but usually under a state of law."}, {"id": "006010006", "title": "Inequality and the labor market", "description": "Automating processes via computation and robotics is likely to impact everyone, but will not hit all regions or sectors of the labor market equally (Frey and Osborne, 2013). Universal basic income is a proposed solution to this inequality (Brynjolfsson 2014). ", "mini_description": "Inequality refers to a difference or imbalance in circumstances. Technological change can lead to rapid changes in the labor market and either reduce or increase conditions of inequality."}, {"id": "006010007", "title": "Existential risk", "description": "Superintelligent AI poses potentially existential threats via the creation of autonomous weapons, destabilizing political equilibria, and unintended consequences (Global Priorities Project, 2017).", "mini_description": "A permanent threat to the survival of Earth-originating intelligent life."}]}]}, {"id": "0060101", "title": "Applied policy areas", "description": "The following sectors are already experiencing disruptive change from the introduction of artificial intelligence, leading in many cases to the examination of the core values of each field: human health and disease, policing, cybersecurity, autonomous weapons, autonomous cars, education, human augmentation,  government, law, art and sports", "mini_description": "Are domain specific issues where specific implementations of policies must be crafted and carried out.", "breakdowns": [{"id": "00601010", "sub_nodes": [{"id": "006010100", "title": "Human health and disease", "description": " AI is being used to sort through and make sense of massive amounts of biomedical data to help with drug development, diagnosis, and treatment, which could improve care, lead to the generation of new treatments, reduce costs, and bring health insights to those who have limited access to doctors and care. The use of AI could also accelerate development of more experimental human biotechnologies such as CRISPR gene editing or neuro-enhancement. We could see a cultural shift as healthcare moves from being more reactive to more proactive and predictive (i.e. resulting in previvors). However, increasing quantification of human bodies poses social and ethical dilemmas about control and cultural/political understandings of bodies. ", "mini_description": "Health is a state of physical, mental, and social wellbeing, while disease is one factor that negatively impacts that state."}, {"id": "006010101", "title": "Policing", "description": "The use of algorithms in policing became highly controversial after an independent report found that the COMPAS tool, being used to predict inmates likelihood of recidivism, was systematically giving higher likelihoods to people of color (Angwin et al., 2016). A legal case of Eric Loomis against COMPAS went up to the Wisconsin Supreme Court (but was turned down by the U.S. Supreme Court). Loomis argued he was denied due process since the algorithm is proprietary.", "mini_description": "Policing is one aspect of the criminal justice system intended to uphold social control, deter crime, and sanction those who violate laws. The role of AI and algorithmic tools in policing is controversial. "}, {"id": "006010102", "title": "Cybersecurity", "description": " AI is being used as a critical new cybersecurity tool, which is seen as particularly valuable within the Internet of Things with its billions of connected devices. AI is also becoming a challenging new cyber-weapon since it makes it easier to find and exploit vulnerabilities at a massive scale. Additionally, AI systems are vulnerable to attack if they are purposefully fed misleading data (Papernot et al., 2017). ", "mini_description": "The state of being protected against the criminal or unauthorized use of electronic data, or the measures taken to achieve this."}, {"id": "006010103", "title": "Autonomous weapons", "description": " Many semi-autonomous weapon systems already exist around the world (Boulanin and Verbruggen, 2017), but there is an increasingly vocal movement from civil society organizations and scientists that fully autonomous lethal weapons should be banned (Docherty, 2016).", "mini_description": "The definition of autonomous weapons is contentious because of the continuum of autonomy present in weapon systems, but generally they are defined as robots designed to select and attack targets without human intervention."}, {"id": "006010104", "title": "Autonomous cars", "description": " Numerous companies around the world have now developed autonomous and semiautonomous vehicles, which are being tested on city streets. Autonomous vehicles may dramatically improve driving safety conditions and convenience, though they also pose new challenges for drivers (Endsley, 2017) as well as social and ethical challenges related to the programming of decision-making (Bonnefon et al., 2016)", "mini_description": "Autonomous cars (or other transportation vehicles) are capable of sensing and navigating their environment without human input. The degree of automation in cars is typically depicted by five levels of autonomy."}, {"id": "006010105", "title": "Education", "description": "Both the content of necessary education and the modes by which it is delivered are changing rapidly, leading experts to question how state standards and educators can keep up. ", "mini_description": "The process of receiving and giving instruction."}, {"id": "006010106", "title": "Human augmentation", "description": "AI may be used to expedite or intensify certain efforts to augment human ability, either through external devices around us, or through internal devices implanted within us. These powerful tools will face many safety, efficacy, and ethical challenges, including the danger that they could exacerbate inequalities between people.", "mini_description": "The application of technology to expand physical or mental abilities. ", "links": [{"id": "005020103"}]}, {"id": "006010107", "title": "Government", "description": " AI will increasingly be used to streamline governmental processes. But its societal use will also impact the social contract and alter citizens expectations of and relationships to their government. How will citizens be able to vote on the development and direction of AI? What regulations relating to AI make sense?", "mini_description": "The governing body of a nation. There are many different governmental systems including democracy, communism, oligarchy, theocracy, and anarchy."}, {"id": "006010108", "title": "Art and sports", "description": " Despite the common perception of robots thinking in simplistic and linear ways, AI systems have been used to make movies and advertisements, create artwork, write stories, play games, design new structures, etc. In what settings will this artificial creativity be meaningful and in what settings will it be banned for violating notions of fair competition?", "mini_description": " Art is defined as the expression of creative skill, while sport involves competitive physical exertion usually for entertainment."}]}]}, {"id": "0060102", "title": "Social norms", "description": "In most cases these do not appear to be innate, although some are nearly universal among social animals. Policies and governance, like everything else, emerge from within environments that are already structured by these societal norms and practices.", "mini_description": "Social norms are the practices and behaviors that we learn from the people and institutions we interact with. ", "breakdowns": [{"id": "00601020", "sub_nodes": [{"id": "006010200", "title": "Fairness", "description": " A rudimentary understanding of fairness and empathy is possibly exhibited among all mammals (Bekoff, 2008). Many human societies have been, and continue to be, deeply unequal, but social norms often dictate stories as to why this is the case.", "mini_description": "Is equitable and impartial treatment of persons."}, {"id": "006010201", "title": "Respect for others and the environment", "description": "As social creatures dependent upon the environment for our survival, respect and sustainability are widely held social norms throughout the world, except notably in cases where they are perceived to conflict with other norms and values mentioned here.", "mini_description": "Is a social and moral honoring towards one's own community and the persons which inhabit it. "}, {"id": "006010202", "title": "Human rights", "description": "The notion of human rights is a relatively recent development in political thought usually traced back to the Enlightenment and efforts to establish representative governments respecting of the freedoms of individual citizens.", "mini_description": "Refers to the supposed innate and intrinsic rights which everyone possesses."}, {"id": "006010203", "title": "Family", "description": "The notion of a family unit has not been fixed across time or societies, but remains an important formulation in many instances in the modern world.", "mini_description": "Is a human social unit usually consisting of persons directly biologically related. "}, {"id": "006010204", "title": "Achievement", "description": "Personal and institutional success is honored in many societies, whether monetarily or with social and cultural prestige. ", "mini_description": "Is the accomplishing of goals or realization of virtues and 'the good.'"}]}]}]}]}, {"id": "00602", "title": "Law", "description": "The laws that influence AI fall into three categories: direct AI policy, such as driverless car regulations, indirect AI policy, such as intellectual property and data laws, and AI relevant policy, such as welfare policies. (Brundage and Bryson, 2016). Many of the indirect and relevant policies are discussed elsewhere, but several direct AI policies are mentioned here.", "mini_description": "Laws are the systems of rules that regulate our actions. Unlike other forms of governance and soft power, laws usually have enforceable penalties for those who do not follow them. ", "breakdowns": [{"id": "006020", "sub_nodes": [{"id": "0060200", "title": "Industry-specific regulations:", "description": "Certain industries have a head start in exploring new regulations for the way they interact with AI. Regulations emerging for autonomous cars are a prime example.", "mini_description": "Laws relating to a particular field or area."}, {"id": "0060201", "title": "Legal rights of robots", "description": "The humanoid robot Sophia was granted Saudi citizenship. Also, an AI chatbot was granted residency in Tokyo. Estonia is publically discussing giving legal subjectivity and representative rights to AI algorithms that can buy and sell goods on peoples behalf.", "mini_description": "Laws relating to the legal status, rights, and responsibilities of AI agents and robots."}, {"id": "0060202", "title": "Robot taxes", "description": "South Korea introduced what has been called the worlds first robot tax in 2017. Bill Gates has also publicly endorsed the idea, suggesting that the funds could be used to help train a displaced worker. The idea has picked up some steam in California. ", "mini_description": "The taxation of robot owners to offset the social costs caused by automation in the workforce."}, {"id": "0060203", "title": "Data regulations", "description": " The EU General Data Protection Regulation (GDPR) goes into force in 2018 and will require explainability from algorithms that influence peoples livelihoods.", "mini_description": "Laws relating to the use, ownership, and distribution of datasets. "}]}]}]}]}]}]}